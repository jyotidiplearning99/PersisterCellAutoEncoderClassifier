{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf52d337",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sequence1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Load and preprocess biological sequences data\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Assuming sequences_data is a list of protein sequences corresponding to the nodes in the graph\u001b[39;00m\n\u001b[1;32m    136\u001b[0m sequences_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# Replace with actual sequences\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m X_bert \u001b[38;5;241m=\u001b[39m \u001b[43mextract_bert_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Placeholder data for training and testing\u001b[39;00m\n\u001b[1;32m    140\u001b[0m target_variable \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes()))  \u001b[38;5;66;03m# Placeholder for target variable\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 125\u001b[0m, in \u001b[0;36mextract_bert_features\u001b[0;34m(sequences)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Prepare data (here we assume sequences_data is a list of protein sequences)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mstr\u001b[39m(i), seq) \u001b[38;5;28;01mfor\u001b[39;00m i, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences)]\n\u001b[0;32m--> 125\u001b[0m batch_labels, batch_strs, batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    128\u001b[0m     results \u001b[38;5;241m=\u001b[39m model(batch_tokens, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m33\u001b[39m], return_contacts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/esm/data.py:266\u001b[0m, in \u001b[0;36mBatchConverter.__call__\u001b[0;34m(self, raw_batch)\u001b[0m\n\u001b[1;32m    264\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_batch)\n\u001b[1;32m    265\u001b[0m batch_labels, seq_str_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mraw_batch)\n\u001b[0;32m--> 266\u001b[0m seq_encoded_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet\u001b[38;5;241m.\u001b[39mencode(seq_str) \u001b[38;5;28;01mfor\u001b[39;00m seq_str \u001b[38;5;129;01min\u001b[39;00m seq_str_list]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncation_seq_length:\n\u001b[1;32m    268\u001b[0m     seq_encoded_list \u001b[38;5;241m=\u001b[39m [seq_str[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncation_seq_length] \u001b[38;5;28;01mfor\u001b[39;00m seq_str \u001b[38;5;129;01min\u001b[39;00m seq_encoded_list]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/esm/data.py:266\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_batch)\n\u001b[1;32m    265\u001b[0m batch_labels, seq_str_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mraw_batch)\n\u001b[0;32m--> 266\u001b[0m seq_encoded_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphabet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq_str \u001b[38;5;129;01min\u001b[39;00m seq_str_list]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncation_seq_length:\n\u001b[1;32m    268\u001b[0m     seq_encoded_list \u001b[38;5;241m=\u001b[39m [seq_str[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncation_seq_length] \u001b[38;5;28;01mfor\u001b[39;00m seq_str \u001b[38;5;129;01min\u001b[39;00m seq_encoded_list]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/esm/data.py:250\u001b[0m, in \u001b[0;36mAlphabet.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_to_idx[tok] \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/esm/data.py:250\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text)]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sequence1'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import random\n",
    "import esm  # Install ESM: pip install fair-esm\n",
    "\n",
    "# Load protein interaction network data\n",
    "data = pd.read_csv('/users/barmanjy/Desktop/Persister Cell/GSE1000_series_matrix.txt', delimiter='\\t', skiprows=67, header=None)\n",
    "\n",
    "# Preprocess your data and construct a graph\n",
    "def preprocess_data(data):\n",
    "    # Convert non-numeric values to NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    data = data.dropna(axis=1, how='all')  # Drop columns with all NaNs\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = data.corr()\n",
    "\n",
    "    # Threshold correlations (optional)\n",
    "    threshold = 0.5\n",
    "    adjacency_matrix = np.where(abs(correlation_matrix) > threshold, 1, 0)\n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = preprocess_data(data)\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "# Custom Transformer to extract network features\n",
    "class NetworkFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, G):\n",
    "        features = []\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G)\n",
    "        for node in G.nodes():\n",
    "            node_features = [\n",
    "                G.degree(node),\n",
    "                nx.clustering(G, node),\n",
    "                betweenness[node],\n",
    "                eigenvector[node]\n",
    "            ]\n",
    "            features.append(node_features)\n",
    "        return np.array(features)\n",
    "\n",
    "# Function to train deep learning model for feature extraction\n",
    "def train_deep_learning_model(X_train, y_train, param_grid):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=param_grid['units'], input_shape=(X_train_scaled.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=param_grid['optimizer'], loss='mean_squared_error')\n",
    "\n",
    "    X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "    model.fit(X_train_reshaped, y_train, epochs=param_grid['epochs'], batch_size=param_grid['batch_size'])\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Function to calculate PI scores using Ricci curvature\n",
    "def calculate_pi(G, d, X_network, X_bert, model, scaler):\n",
    "    X_combined = np.concatenate((X_network, X_bert), axis=1)\n",
    "    X_combined_scaled = scaler.transform(X_combined)\n",
    "    X_combined_reshaped = np.reshape(X_combined_scaled, (X_combined_scaled.shape[0], X_combined_scaled.shape[1], 1))\n",
    "    pi_scores = model.predict(X_combined_reshaped)\n",
    "    return pi_scores.flatten()\n",
    "\n",
    "# Function to calculate distances based on edge weights\n",
    "def calculate_distances(G, a, x):\n",
    "    distances = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        distances[edge] = a[i][j] * x[i] * x[j]\n",
    "    return distances\n",
    "\n",
    "# Function to calculate Ricci curvature\n",
    "def calculate_ricci_curvature(G, a, x):\n",
    "    ricci_curvature = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        ricci_curvature[edge] = 2 * (a[i][j] - (x[i] + x[j]))\n",
    "    return ricci_curvature\n",
    "\n",
    "# Function to update distances in discrete Ricci flow\n",
    "def update_distances(G, d, ricci_curvature, delta_t):\n",
    "    updated_distances = {edge: distance + ricci_curvature[edge] * delta_t for edge, distance in d.items()}\n",
    "    return updated_distances\n",
    "\n",
    "# Function to perform discrete Ricci flow iterations\n",
    "def discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations, convergence_threshold=1e-6):\n",
    "    d_stem = calculate_distances(G, a, x_stem)\n",
    "    d_diff = calculate_distances(G, a, x_diff)\n",
    "    ricci_curvature_stem = calculate_ricci_curvature(G, a, x_stem)\n",
    "    ricci_curvature_diff = calculate_ricci_curvature(G, a, x_diff)\n",
    "    prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    for _ in range(num_iterations):\n",
    "        d_stem = update_distances(G, d_stem, ricci_curvature_stem, delta_t)\n",
    "        d_diff = update_distances(G, d_diff, ricci_curvature_diff, delta_t)\n",
    "        # Convergence check\n",
    "        if np.allclose(np.array(list(d_stem.values())), np.array(list(prev_d_stem.values())), atol=convergence_threshold) and \\\n",
    "           np.allclose(np.array(list(d_diff.values())), np.array(list(prev_d_diff.values())), atol=convergence_threshold):\n",
    "            break\n",
    "        prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    return d_stem, d_diff\n",
    "\n",
    "# Function to extract BERT features using ESM model\n",
    "def extract_bert_features(sequences):\n",
    "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    # Prepare data (here we assume sequences_data is a list of protein sequences)\n",
    "    data = [(str(i), seq) for i, seq in enumerate(sequences)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    \n",
    "    token_representations = results[\"representations\"][33]\n",
    "    sequence_representations = token_representations.mean(1).numpy()\n",
    "    return sequence_representations\n",
    "\n",
    "# Load and preprocess biological sequences data\n",
    "# Assuming sequences_data is a list of protein sequences corresponding to the nodes in the graph\n",
    "sequences_data = [\"sequence1\", \"sequence2\", ...]  # Replace with actual sequences\n",
    "X_bert = extract_bert_features(sequences_data)\n",
    "\n",
    "# Placeholder data for training and testing\n",
    "target_variable = np.random.rand(len(G.nodes()))  # Placeholder for target variable\n",
    "\n",
    "# Define hyperparameters for the deep learning model\n",
    "param_grid = {\n",
    "    'units': 50,\n",
    "    'optimizer': 'adam',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "print(\"Shape of adjacency matrix:\", adjacency_matrix.shape)\n",
    "print(\"Shape of target variable:\", target_variable.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(adjacency_matrix, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train deep learning model for feature extraction\n",
    "model, scaler = train_deep_learning_model(X_train, y_train, param_grid)\n",
    "\n",
    "# Extract network features\n",
    "network_feature_extractor = NetworkFeatureExtractor()\n",
    "X_network = network_feature_extractor.transform(G)\n",
    "\n",
    "# Placeholder for edge weights based on gene expression data\n",
    "a = np.random.rand(len(G.nodes()), len(G.nodes()))\n",
    "\n",
    "# Define initial and final states for the Ricci flow\n",
    "x_stem = np.random.rand(len(G.nodes()))  # Placeholder for undifferentiated cell state\n",
    "x_diff = np.random.rand(len(G.nodes()))  # Placeholder for fully differentiated cell state\n",
    "\n",
    "# Perform discrete Ricci flow iterations\n",
    "delta_t = 0.1  # Placeholder for time step\n",
    "num_iterations = 10  # Placeholder for number of iterations\n",
    "d_stem, d_diff = discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations)\n",
    "\n",
    "# Calculate proliferation index\n",
    "pi_scores_stem = calculate_pi(G, d_stem, X_network, X_bert, model, scaler)\n",
    "pi_scores_diff = calculate_pi(G, d_diff, X_network, X_bert, model, scaler)\n",
    "\n",
    "# Convert pi_scores_stem and pi_scores_diff to dictionaries for easier manipulation\n",
    "pi_scores_stem = {node: score for node, score in enumerate(pi_scores_stem)}\n",
    "pi_scores_diff = {node: score for node, score in enumerate(pi_scores_diff)}\n",
    "\n",
    "# Categorize cells into groups based on proliferation index\n",
    "threshold1 = 0.5\n",
    "threshold2 = 0.3\n",
    "threshold3 = 0.1\n",
    "\n",
    "group1 = {node: score for node, score in pi_scores_stem.items() if score > threshold1}\n",
    "group2 = {node: score for node, score in pi_scores_stem.items() if threshold2 < score <= threshold1}\n",
    "group3 = {node: score for node, score in pi_scores_stem.items() if threshold3 < score <= threshold2}\n",
    "\n",
    "# Print the groups\n",
    "print(\"Group 1:\", group1)\n",
    "print(\"Group 2:\", group2)\n",
    "print(\"Group 3:\", group3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4b9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m161.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.3/401.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.1 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ef6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of adjacency matrix: (10, 10)\n",
      "Shape of target variable: (10,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/barmanjy/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950ms/step - loss: 0.3592\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3488\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3380\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3267\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3147\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3020\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2885\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2743\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2592\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2432\n",
      "Shape of X_train: (8, 10)\n",
      "Shape of X_combined_train: (10, 1284)\n",
      "Shape of X_combined_train_scaled: (8, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1284 features, but StandardScaler is expecting 10 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 220\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X_combined_train_scaled:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_combined_train_scaled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Calculate proliferation index\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m pi_scores_train \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_pi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_stem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_esm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m pi_scores_test \u001b[38;5;241m=\u001b[39m calculate_pi(G, d_diff, X_network, X_esm, model, scaler)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Placeholder for edge weights based on gene expression data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m, in \u001b[0;36mcalculate_pi\u001b[0;34m(G, d, X_network, X_esm, model, scaler)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_pi\u001b[39m(G, d, X_network, X_esm, model, scaler):\n\u001b[1;32m     73\u001b[0m     X_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_network, X_esm), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     X_combined_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     X_combined_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_combined_scaled, (X_combined_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_combined_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     76\u001b[0m     pi_scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_combined_reshaped)\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1284 features, but StandardScaler is expecting 10 features as input."
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import esm  # Install ESM: pip install fair-esm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# Load protein interaction network data\n",
    "data = pd.read_csv('/users/barmanjy/Desktop/Persister Cell/GSE1000_series_matrix.txt', delimiter='\\t', skiprows=67, header=None)\n",
    "\n",
    "# Preprocess your data and construct a graph\n",
    "def preprocess_data(data):\n",
    "    # Convert non-numeric values to NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    data = data.dropna(axis=1, how='all')  # Drop columns with all NaNs\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = data.corr()\n",
    "\n",
    "    # Threshold correlations (optional)\n",
    "    threshold = 0.5\n",
    "    adjacency_matrix = np.where(abs(correlation_matrix) > threshold, 1, 0)\n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = preprocess_data(data)\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "# Custom Transformer to extract network features\n",
    "class NetworkFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, G):\n",
    "        features = []\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G)\n",
    "        for node in G.nodes():\n",
    "            node_features = [\n",
    "                G.degree(node),\n",
    "                nx.clustering(G, node),\n",
    "                betweenness[node],\n",
    "                eigenvector[node]\n",
    "            ]\n",
    "            features.append(node_features)\n",
    "        return np.array(features)\n",
    "\n",
    "# Function to train deep learning model for feature extraction\n",
    "def train_deep_learning_model(X_train, y_train, param_grid):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=param_grid['units'], input_shape=(X_train_scaled.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=param_grid['optimizer'], loss='mean_squared_error')\n",
    "\n",
    "    X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "    model.fit(X_train_reshaped, y_train, epochs=param_grid['epochs'], batch_size=param_grid['batch_size'])\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Function to calculate PI scores using Ricci curvature\n",
    "def calculate_pi(G, d, X_network, X_esm, model, scaler):\n",
    "    X_combined = np.concatenate((X_network, X_esm), axis=1)\n",
    "    X_combined_scaled = scaler.transform(X_combined)\n",
    "    X_combined_reshaped = np.reshape(X_combined_scaled, (X_combined_scaled.shape[0], X_combined_scaled.shape[1], 1))\n",
    "    pi_scores = model.predict(X_combined_reshaped)\n",
    "    return pi_scores.flatten()\n",
    "\n",
    "# Function to calculate distances based on edge weights\n",
    "def calculate_distances(G, a, x):\n",
    "    distances = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        distances[edge] = a[i][j] * x[i] * x[j]\n",
    "    return distances\n",
    "\n",
    "# Function to calculate Ricci curvature\n",
    "def calculate_ricci_curvature(G, a, x):\n",
    "    ricci_curvature = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        ricci_curvature[edge] = 2 * (a[i][j] - (x[i] + x[j]))\n",
    "    return ricci_curvature\n",
    "\n",
    "# Function to update distances in discrete Ricci flow\n",
    "def update_distances(G, d, ricci_curvature, delta_t):\n",
    "    updated_distances = {edge: distance + ricci_curvature[edge] * delta_t for edge, distance in d.items()}\n",
    "    return updated_distances\n",
    "\n",
    "# Function to perform discrete Ricci flow iterations\n",
    "def discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations, convergence_threshold=1e-6):\n",
    "    d_stem = calculate_distances(G, a, x_stem)\n",
    "    d_diff = calculate_distances(G, a, x_diff)\n",
    "    ricci_curvature_stem = calculate_ricci_curvature(G, a, x_stem)\n",
    "    ricci_curvature_diff = calculate_ricci_curvature(G, a, x_diff)\n",
    "    prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    for _ in range(num_iterations):\n",
    "        d_stem = update_distances(G, d_stem, ricci_curvature_stem, delta_t)\n",
    "        d_diff = update_distances(G, d_diff, ricci_curvature_diff, delta_t)\n",
    "        # Convergence check\n",
    "        if np.allclose(np.array(list(d_stem.values())), np.array(list(prev_d_stem.values())), atol=convergence_threshold) and \\\n",
    "           np.allclose(np.array(list(d_diff.values())), np.array(list(prev_d_diff.values())), atol=convergence_threshold):\n",
    "            break\n",
    "        prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    return d_stem, d_diff\n",
    "\n",
    "# Function to extract ESM features\n",
    "def extract_esm_features(sequences):\n",
    "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    # Prepare data (here we assume sequences_data is a list of protein sequences)\n",
    "    data = [(str(i), seq) for i, seq in enumerate(sequences)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    \n",
    "    token_representations = results[\"representations\"][33]\n",
    "    sequence_representations = token_representations.mean(1).numpy()\n",
    "    return sequence_representations\n",
    "\n",
    "\n",
    "# Optional: Fine-tune ESM model on your data\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "def fine_tune_esm_model(sequences, labels, epochs=10, batch_size=32):\n",
    "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    dataset = ProteinDataset(sequences, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            sequences, labels = batch\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter([(str(i), seq) for i, seq in enumerate(sequences)])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "            sequence_representations = token_representations.mean(1)\n",
    "\n",
    "            loss = loss_fn(sequence_representations, torch.tensor(labels, dtype=torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load and preprocess biological sequences data\n",
    "# Assuming sequences_data is a list of protein sequences corresponding to the nodes in the graph\n",
    "sequences_data = [\"MAGVKILKKEKILDYLGELKMHLRGGGSLGGALQRLGEMAKKKMEKGLIHGRGILKLKTP\", \"MKALLGPWRAVGLLLCALVSCASADGDGALRPPLQGPPAHAARGPHGHPGAPGHPGPPPGLGQP\"]  # Replace with actual sequences\n",
    "X_esm = extract_esm_features(sequences_data)\n",
    "\n",
    "# Number of samples needed to match X_network\n",
    "num_samples_needed = len(G.nodes())\n",
    "\n",
    "# Generate random protein sequences\n",
    "random_sequences = [generate_random_sequence(20) for _ in range(num_samples_needed)]\n",
    "\n",
    "# Extract ESM features from random sequences\n",
    "X_esm = extract_esm_features(random_sequences)\n",
    "\n",
    "# Placeholder data for training and testing\n",
    "target_variable = np.random.rand(len(G.nodes()))  # Placeholder for target variable\n",
    "\n",
    "# Define hyperparameters for the deep learning model\n",
    "param_grid = {\n",
    "    'units': 50,\n",
    "    'optimizer': 'adam',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "print(\"Shape of adjacency matrix:\", adjacency_matrix.shape)\n",
    "print(\"Shape of target variable:\", target_variable.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(adjacency_matrix, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train deep learning model for feature extraction\n",
    "model, scaler = train_deep_learning_model(X_train, y_train, param_grid)\n",
    "\n",
    "# Transform both training and testing data using the fitted scaler\n",
    "X_combined_train_scaled = scaler.transform(X_train)\n",
    "X_combined_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_combined_train:\", X_combined_train.shape)\n",
    "print(\"Shape of X_combined_train_scaled:\", X_combined_train_scaled.shape)\n",
    "\n",
    "\n",
    "# Calculate proliferation index\n",
    "pi_scores_train = calculate_pi(G, d_stem, X_network, X_esm, model, scaler)\n",
    "pi_scores_test = calculate_pi(G, d_diff, X_network, X_esm, model, scaler)\n",
    "\n",
    "# Placeholder for edge weights based on gene expression data\n",
    "a = np.random.rand(len(G.nodes()), len(G.nodes()))\n",
    "\n",
    "# Define initial and final states for the Ricci flow\n",
    "x_stem = np.random.rand(len(G.nodes()))  # Placeholder for undifferentiated cell state\n",
    "x_diff = np.random.rand(len(G.nodes()))  # Placeholder for fully differentiated cell state\n",
    "\n",
    "# Perform discrete Ricci flow iterations\n",
    "delta_t = 0.1  # Placeholder for time step\n",
    "num_iterations = 10  # Placeholder for number of iterations\n",
    "d_stem, d_diff = discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations)\n",
    "\n",
    "# Calculate proliferation index\n",
    "pi_scores_stem = calculate_pi(G, d_stem, X_network, X_esm, model, scaler)\n",
    "pi_scores_diff = calculate_pi(G, d_diff, X_network, X_esm, model, scaler)\n",
    "\n",
    "# Convert pi_scores_stem and pi_scores_diff to dictionaries for easier manipulation\n",
    "pi_scores_stem = {node: score for node, score in enumerate(pi_scores_stem)}\n",
    "pi_scores_diff = {node: score for node, score in enumerate(pi_scores_diff)}\n",
    "\n",
    "# Categorize cells into groups based on proliferation index\n",
    "threshold1 = 0.5\n",
    "threshold2 = 0.3\n",
    "threshold3 = 0.1\n",
    "\n",
    "group1 = {node: score for node, score in pi_scores_stem.items() if score > threshold1}\n",
    "group2 = {node: score for node, score in pi_scores_stem.items() if threshold2 < score <= threshold1}\n",
    "group3 = {node: score for node, score in pi_scores_stem.items() if threshold3 < score <= threshold2}\n",
    "group4 = {node: score for node, score in pi_scores_stem.items() if score <= threshold3}\n",
    "\n",
    "# Print the groups\n",
    "print(\"Group 1:\", group1)\n",
    "print(\"Group 2:\", group2)\n",
    "print(\"Group 3:\", group3)\n",
    "print(\"Group 4:\", group4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77a4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /users/barmanjy/.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch)\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c0447a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_network before concatenation: (10, 4)\n",
      "Shape of X_bert before concatenation: (10, 1024)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of features in X_network and X_bert must match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X_bert before concatenation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_bert\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Ensure the number of features in X_network and X_bert match\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_network\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m X_bert\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features in X_network and X_bert must match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Combine network features and BERT features\u001b[39;00m\n\u001b[1;32m    149\u001b[0m X_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_network, X_bert), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of features in X_network and X_bert must match."
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load gene expression data\n",
    "data = pd.read_csv('/users/barmanjy/Desktop/Persister Cell/GSE1000_series_matrix.txt', delimiter='\\t', skiprows=67, header=None)\n",
    "\n",
    "# Preprocess the data and construct the graph\n",
    "def preprocess_data(data):\n",
    "    # Convert non-numeric values to NaN and drop columns with all NaNs\n",
    "    data = data.apply(pd.to_numeric, errors='coerce').dropna(axis=1, how='all')\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = data.corr()\n",
    "    # Threshold correlations\n",
    "    threshold = 0.5\n",
    "    adjacency_matrix = np.where(abs(correlation_matrix) > threshold, 1, 0)\n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = preprocess_data(data)\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "# Custom Transformer to extract network features\n",
    "class NetworkFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, G):\n",
    "        features = []\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G)\n",
    "        for node in G.nodes():\n",
    "            node_features = [\n",
    "                G.degree(node),\n",
    "                nx.clustering(G, node),\n",
    "                betweenness[node],\n",
    "                eigenvector[node]\n",
    "            ]\n",
    "            features.append(node_features)\n",
    "        return np.array(features)\n",
    "\n",
    "# Function to train deep learning model for feature extraction\n",
    "def train_deep_learning_model(X_train, y_train, param_grid):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=param_grid['units'], input_shape=(X_train_scaled.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=param_grid['optimizer'], loss='mean_squared_error')\n",
    "\n",
    "    X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "    model.fit(X_train_reshaped, y_train, epochs=param_grid['epochs'], batch_size=param_grid['batch_size'])\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Function to calculate PI scores using Ricci curvature\n",
    "def calculate_pi(G, d, X_network, X_bert, model, scaler):\n",
    "    X_combined = np.concatenate((X_network, X_bert), axis=1)\n",
    "    X_combined_scaled = scaler.transform(X_combined)\n",
    "    X_combined_reshaped = np.reshape(X_combined_scaled, (X_combined_scaled.shape[0], X_combined_scaled.shape[1], 1))\n",
    "    pi_scores = model.predict(X_combined_reshaped)\n",
    "    return pi_scores.flatten()\n",
    "\n",
    "# Function to calculate distances based on edge weights\n",
    "def calculate_distances(G, a, x):\n",
    "    distances = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        distances[edge] = a[i][j] * x[i] * x[j]\n",
    "    return distances\n",
    "\n",
    "# Function to calculate Ricci curvature\n",
    "def calculate_ricci_curvature(G, a, x):\n",
    "    ricci_curvature = {}\n",
    "    for edge in G.edges():\n",
    "        i, j = edge\n",
    "        ricci_curvature[edge] = 2 * (a[i][j] - (x[i] + x[j]))\n",
    "    return ricci_curvature\n",
    "\n",
    "# Function to update distances in discrete Ricci flow\n",
    "def update_distances(G, d, ricci_curvature, delta_t):\n",
    "    updated_distances = {edge: distance + ricci_curvature[edge] * delta_t for edge, distance in d.items()}\n",
    "    return updated_distances\n",
    "\n",
    "# Function to perform discrete Ricci flow iterations\n",
    "def discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations, convergence_threshold=1e-6):\n",
    "    d_stem = calculate_distances(G, a, x_stem)\n",
    "    d_diff = calculate_distances(G, a, x_diff)\n",
    "    ricci_curvature_stem = calculate_ricci_curvature(G, a, x_stem)\n",
    "    ricci_curvature_diff = calculate_ricci_curvature(G, a, x_diff)\n",
    "    prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    for _ in range(num_iterations):\n",
    "        d_stem = update_distances(G, d_stem, ricci_curvature_stem, delta_t)\n",
    "        d_diff = update_distances(G, d_diff, ricci_curvature_diff, delta_t)\n",
    "        if np.allclose(np.array(list(d_stem.values())), np.array(list(prev_d_stem.values())), atol=convergence_threshold) and \\\n",
    "           np.allclose(np.array(list(d_diff.values())), np.array(list(prev_d_diff.values())), atol=convergence_threshold):\n",
    "            break\n",
    "        prev_d_stem, prev_d_diff = d_stem.copy(), d_diff.copy()\n",
    "    return d_stem, d_diff\n",
    "\n",
    "# Function to extract BERT features using ProtBERT\n",
    "def extract_bert_features(sequences, model, tokenizer, max_length=512):\n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        inputs = tokenizer(seq, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        sequence_representation = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        features.append(sequence_representation)\n",
    "    return np.array(features)\n",
    "\n",
    "# Load ProtBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')\n",
    "model = BertModel.from_pretrained('Rostlab/prot_bert')\n",
    "\n",
    "# Generate random protein sequences (replace this with actual protein sequences)\n",
    "def generate_random_sequence(length):\n",
    "    return ''.join(random.choices('ACDEFGHIKLMNPQRSTVWY', k=length))\n",
    "\n",
    "# Ensure the number of random sequences matches the number of nodes in the graph\n",
    "num_samples_needed = len(G.nodes())\n",
    "random_sequences = [generate_random_sequence(20) for _ in range(num_samples_needed)]\n",
    "\n",
    "# Extract BERT features from protein sequences\n",
    "X_bert = extract_bert_features(random_sequences, model, tokenizer)\n",
    "\n",
    "# Reduce the dimensionality of BERT embeddings to match the number of features in X_network\n",
    "pca = PCA(n_components=4)\n",
    "X_bert_reduced = pca.fit_transform(X_bert)\n",
    "\n",
    "\n",
    "# Print shapes of X_network and X_bert before concatenation\n",
    "print(\"Shape of X_network before concatenation:\", X_network.shape)\n",
    "print(\"Shape of X_bert before concatenation:\", X_bert.shape)\n",
    "\n",
    "# Ensure the number of features in X_network and X_bert match\n",
    "assert X_network.shape[1] == X_bert.shape[1], \"Number of features in X_network and X_bert must match.\"\n",
    "\n",
    "# Combine network features and BERT features\n",
    "X_combined = np.concatenate((X_network, X_bert), axis=1)\n",
    "\n",
    "# Print shape of X_combined after concatenation\n",
    "print(\"Shape of X_combined after concatenation:\", X_combined.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_network, X_test_network, X_train_bert, X_test_bert, y_train, y_test = train_test_split(\n",
    "    X_network, X_bert_reduced, target_variable, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Combine network features and BERT features for training and testing sets\n",
    "X_train_combined = np.concatenate((X_train_network, X_train_bert), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_network, X_test_bert), axis=1)\n",
    "\n",
    "# Define hyperparameters for the deep learning model\n",
    "param_grid = {\n",
    "    'units': 50,\n",
    "    'optimizer': 'adam',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Train deep learning model for feature extraction\n",
    "model, scaler = train_deep_learning_model(X_train_combined, y_train, param_grid)\n",
    "\n",
    "# Calculate proliferation index for the training and testing sets\n",
    "pi_scores_train = calculate_pi(G, None, X_train_combined, X_train_bert, model, scaler)\n",
    "pi_scores_test = calculate_pi(G, None, X_test_combined, X_test_bert, model, scaler)\n",
    "\n",
    "# Placeholder for edge weights based on gene expression data\n",
    "a = np.random.rand(len(G.nodes()), len(G.nodes()))\n",
    "\n",
    "# Define initial and final states for the Ricci flow\n",
    "x_stem = np.random.rand(len(G.nodes()))\n",
    "x_diff = np.random.rand(len(G.nodes()))\n",
    "\n",
    "# Perform discrete Ricci flow iterations\n",
    "delta_t = 0.1\n",
    "num_iterations = 10\n",
    "d_stem, d_diff = discrete_ricci_flow(G, a, x_stem, x_diff, delta_t, num_iterations)\n",
    "\n",
    "# Calculate proliferation index for the stem and differentiated states\n",
    "pi_scores_stem = calculate_pi(G, d_stem, X_network, X_bert_reduced, model, scaler)\n",
    "pi_scores_diff = calculate_pi(G, d_diff, X_network, X_bert_reduced, model, scaler)\n",
    "\n",
    "# Categorize cells into groups based on proliferation index\n",
    "threshold1 = 0.5\n",
    "threshold2 = 0.3\n",
    "threshold3 = 0.1\n",
    "\n",
    "group1 = {node: score for node, score in enumerate(pi_scores_stem) if score > threshold1}\n",
    "group2 = {node: score for node, score in enumerate(pi_scores_stem) if threshold2 < score <= threshold1}\n",
    "group3 = {node: score for node, score in enumerate(pi_scores_stem) if threshold3 < score <= threshold2}\n",
    "group4 = {node: score for node, score in enumerate(pi_scores_stem) if score <= threshold3}\n",
    "\n",
    "# Print the groups\n",
    "print(\"Group 1:\", group1)\n",
    "print(\"Group 2:\", group2)\n",
    "print(\"Group 3:\", group3)\n",
    "print(\"Group 4:\", group4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39c65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
