{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73570c40-9bf8-48c7-8256-3cb527a81888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory to sys.path\n",
    "import sys\n",
    "sys.path.append('/scratch/project_2010376')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be953ef-7962-4dcc-ada2-25219e673cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'Non-Persister': 0, 'Persister': 1}\n",
      "Class 0 corresponds to Non-Persister, and class 1 corresponds to Persister.\n",
      "Features for non-persister (class 0) saved to 'cnn_rnn_non_persister_features.txt'\n",
      "Features for persister (class 1) saved to 'cnn_rnn_persister_features.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-21 11:40:18.748353: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 51ms/step - accuracy: 0.6703 - loss: 0.5853 - val_accuracy: 0.6196 - val_loss: 0.6895 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9719 - loss: 0.0862 - val_accuracy: 0.5017 - val_loss: 0.7952 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9890 - loss: 0.0331 - val_accuracy: 0.5017 - val_loss: 0.8576 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9924 - loss: 0.0238 - val_accuracy: 0.4983 - val_loss: 0.7162 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9953 - loss: 0.0170 - val_accuracy: 0.9143 - val_loss: 0.6139 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9954 - loss: 0.0165 - val_accuracy: 0.5017 - val_loss: 1.1774 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9956 - loss: 0.0159 - val_accuracy: 0.8028 - val_loss: 0.5775 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9961 - loss: 0.0135 - val_accuracy: 0.5614 - val_loss: 0.6833 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9955 - loss: 0.0138 - val_accuracy: 0.6027 - val_loss: 0.5406 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.5019 - val_loss: 0.6776 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m510/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9973 - loss: 0.0087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 245\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, file_path \u001b[38;5;129;01min\u001b[39;00m independent_datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 245\u001b[0m     counts \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_independent_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransfer Learning - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m counts\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Transfer Learning Predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 208\u001b[0m, in \u001b[0;36mpredict_independent_dataset\u001b[0;34m(model, file_path, train_columns, n_components)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_independent_dataset\u001b[39m(model, file_path, train_columns, n_components):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         X_independent \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_independent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m         X_independent \u001b[38;5;241m=\u001b[39m X_independent\u001b[38;5;241m.\u001b[39mreshape(X_independent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_independent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    210\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_independent)\n",
      "Cell \u001b[0;32mIn[6], line 201\u001b[0m, in \u001b[0;36mpreprocess_independent_data\u001b[0;34m(file_path, train_columns, n_components)\u001b[0m\n\u001b[1;32m    198\u001b[0m X_independent\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[0;32m--> 201\u001b[0m X_independent \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_independent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_independent\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:516\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_svd_solver\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:656\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    652\u001b[0m     U, Vt \u001b[38;5;241m=\u001b[39m svd_flip(U[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], Vt[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# sign flipping is done inside\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterated_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_ \u001b[38;5;241m=\u001b[39m n_samples\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m Vt\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/extmath.py:523\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 523\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[1;32m    532\u001b[0m B \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/extmath.py:340\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Perform power iterations with Q to further 'imprint' the top\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# singular vectors of A in Q\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m--> 340\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(A\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Q)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/scipy/linalg/_decomp_lu.py:186\u001b[0m, in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124millegal value in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mth argument of internal gesv|posv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    183\u001b[0m                      \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m-\u001b[39minfo)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlu\u001b[39m(a, permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m        p_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Compute LU decomposition of a matrix with partial pivoting.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a) \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Function to load data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(metadata_path, data_path, additional_datasets=None):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "\n",
    "    if additional_datasets:\n",
    "        for file_path, sample_type in additional_datasets:\n",
    "            additional_data = pd.concat(load_data_in_chunks(file_path)).transpose()\n",
    "            additional_data.reset_index(inplace=True)\n",
    "            additional_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "            additional_data['cell'] = additional_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "            additional_data['sample_type'] = sample_type\n",
    "            merged_data = pd.concat([merged_data, additional_data], ignore_index=True, sort=False)\n",
    "\n",
    "    gene_names = merged_data.columns[1:-2]  # Assuming the first column is 'cell', and last two are 'sample_name' and 'sample_type'\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'], errors='ignore')\n",
    "    X.columns = X.columns.astype(str)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    return X, y, merged_data, label_encoder, gene_names\n",
    "\n",
    "# CNN-RNN Hybrid Model\n",
    "def create_cnn_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        lr = lr * 0.85\n",
    "    return lr\n",
    "\n",
    "# Main Code Execution\n",
    "metadata_path = '/scratch/project_2010376/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/normalized_GSE150949_pc9_count.csv'\n",
    "additional_datasets = [\n",
    "    ('/scratch/project_2010751/GSM8118468_ob_treated.csv', 'Persister'),\n",
    "    ('/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv', 'Non-Persister')\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, merged_data, label_encoder, gene_names = preprocess_data(metadata_path, data_path, additional_datasets)\n",
    "\n",
    "# Confirm class 0 is non-persister and class 1 is persister\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoding Mapping:\", label_mapping)\n",
    "\n",
    "if label_mapping['Non-Persister'] == 0 and label_mapping['Persister'] == 1:\n",
    "    print(\"Class 0 corresponds to Non-Persister, and class 1 corresponds to Persister.\")\n",
    "else:\n",
    "    print(\"Class 0 corresponds to Persister, and class 1 corresponds to Non-Persister.\")\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=100)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE for balancing the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_reduced, y)\n",
    "\n",
    "# Extract features for non-persister (class 0) and persister (class 1)\n",
    "non_persister_features = X_resampled[y_resampled == 0]\n",
    "persister_features = X_resampled[y_resampled == 1]\n",
    "\n",
    "# Ensure all gene names are strings\n",
    "gene_names = [str(gene) for gene in gene_names]\n",
    "\n",
    "# Save the features to text files with gene names as headers\n",
    "np.savetxt('cnn_rnn_non_persister_features.txt', non_persister_features, delimiter=',', header=','.join(gene_names), fmt='%.6f', comments='')\n",
    "np.savetxt('cnn_rnn_persister_features.txt', persister_features, delimiter=',', header=','.join(gene_names), fmt='%.6f', comments='')\n",
    "\n",
    "print(\"Features for non-persister (class 0) saved to 'cnn_rnn_non_persister_features.txt'\")\n",
    "print(\"Features for persister (class 1) saved to 'cnn_rnn_persister_features.txt'\")\n",
    "\n",
    "# Train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Fix input shape by making it a tuple\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Train the CNN-RNN model\n",
    "model = create_cnn_rnn_model(input_shape)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_cnn_rnn_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Model training\n",
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation),\n",
    "                    epochs=100, batch_size=64, callbacks=[early_stopping, model_checkpoint, lr_schedule])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-Persister', 'Persister'], yticklabels=['Non-Persister', 'Persister'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the final model\n",
    "model.save('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Function to preprocess independent datasets\n",
    "def preprocess_independent_data(file_path, train_columns, n_components):\n",
    "    independent_data = pd.concat(load_data_in_chunks(file_path)).transpose()\n",
    "    independent_data.reset_index(inplace=True)\n",
    "    independent_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "\n",
    "    if independent_data.shape[0] > independent_data.shape[1]:\n",
    "        independent_data = independent_data.transpose()\n",
    "\n",
    "    X_independent = independent_data.drop(columns=['cell', 'sample_name', 'sample_type'], errors='ignore')\n",
    "    X_independent.columns = X_independent.columns.astype(str)\n",
    "    train_columns = train_columns.astype(str)\n",
    "\n",
    "    X_independent = X_independent.reindex(columns=train_columns, fill_value=0)\n",
    "    X_independent = X_independent.apply(pd.to_numeric, errors='coerce')\n",
    "    X_independent.fillna(0, inplace=True)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_independent = pca.fit_transform(X_independent)\n",
    "    \n",
    "    return X_independent\n",
    "\n",
    "# Function to predict on independent datasets\n",
    "def predict_independent_dataset(model, file_path, train_columns, n_components):\n",
    "    try:\n",
    "        X_independent = preprocess_independent_data(file_path, train_columns, n_components)\n",
    "        X_independent = X_independent.reshape(X_independent.shape[0], X_independent.shape[1], 1)\n",
    "        predictions = model.predict(X_independent)\n",
    "        predicted_labels = (predictions > 0.5).astype(int)\n",
    "        counts = Counter(predicted_labels.flatten())\n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return Counter()\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Get the feature columns from merged_data used in training\n",
    "train_columns = merged_data.drop(columns=['cell', 'sample_name', 'sample_type']).columns  # Exclude the target column\n",
    "\n",
    "# Use the same number of PCA components as used in the model\n",
    "n_components = model.input_shape[1]\n",
    "\n",
    "# Independent datasets\n",
    "independent_datasets = {\n",
    "    \"GSE134836_GSM3972651_PC9D0_untreated_filtered.csv\": \"/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv\",\n",
    "    \"GSM4869650_xCtrl\": \"/scratch/project_2010376/GSM4869650_xCtrl.dge.csv\",\n",
    "    \"new_GSM4869650_xCtrl.dge.csv\": \"/scratch/project_2010376/new_GSM4869650_xCtrl.dge.csv\",\n",
    "    \"GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\": \"/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\",\n",
    "    \"GSM4869653_xOsiCriz\":\"/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv\",\n",
    "    \"GSE149383_GSM3972669_D0_untreated.dge.csv\":\"/scratch/project_2010751/GSE149383_GSM3972669_D0_untreated.dge.csv\",\n",
    "    \"GSE160244_GSM4869650_day3_untreated.dge.csv\":\"/scratch/project_2010751/GSE160244_GSM4869650_day3_untreated.dge.csv\",\n",
    "    \"GSE160244_GSM4869652_xOsi_day3_dge.csv\":\"/scratch/project_2010751/GSE160244_GSM4869652_xOsi_day3_dge.csv\",\n",
    "    \"GSE260499_GSM8118463_Osi.RDS\":\"/scratch/project_2010751/GSE260499_GSM8118463_Osi.RDS\",\n",
    "    \"normalized_GSE150949_pc9_count.csv\":\"/scratch/project_2010751/normalized_GSE150949_pc9_count.csv\"\n",
    "}\n",
    "\n",
    "# Process each independent dataset and make predictions\n",
    "results = {}\n",
    "for dataset_name, file_path in independent_datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    counts = predict_independent_dataset(model, file_path, train_columns, n_components)\n",
    "    results[f\"Transfer Learning - {dataset_name}\"] = counts\n",
    "    print(f\"{dataset_name} - Transfer Learning Predictions: {counts}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Prediction Results:\")\n",
    "for model_dataset, counts in results.items():\n",
    "    print(f\"{model_dataset}: Predictions = {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8c21b0-3ce5-403d-b582-8b47e6e07843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene names for non-persisters saved to 'non_persister_gene_names.txt'\n",
      "Gene names for persisters saved to 'persister_gene_names.txt'\n"
     ]
    }
   ],
   "source": [
    "# Extract gene names from the merged data\n",
    "gene_names = merged_data.columns[1:]  # Assuming 'cell' is the first column and all others are gene names\n",
    "\n",
    "# Convert gene names to strings (in case some aren't strings already)\n",
    "gene_names = [str(gene) for gene in gene_names]\n",
    "\n",
    "# Save gene names for non-persisters (this will be all gene names)\n",
    "with open('non_persister_gene_names.txt', 'w') as file:\n",
    "    for gene in gene_names:\n",
    "        file.write(f\"{gene}\\n\")\n",
    "\n",
    "# Save gene names for persisters (this will be all gene names)\n",
    "with open('persister_gene_names.txt', 'w') as file:\n",
    "    for gene in gene_names:\n",
    "        file.write(f\"{gene}\\n\")\n",
    "\n",
    "print(\"Gene names for non-persisters saved to 'non_persister_gene_names.txt'\")\n",
    "print(\"Gene names for persisters saved to 'persister_gene_names.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8774e061-eec7-42fb-8cd1-f55c140c92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: GSM4869653_xOsiCriz\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
      "GSM4869653_xOsiCriz - Transfer Learning Predictions: Counter({0: 1274})\n",
      "\n",
      "Final Prediction Results:\n",
      "Transfer Learning - GSM4869653_xOsiCriz: Predictions = Counter({0: 1274})\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Get the feature columns from merged_data used in training\n",
    "train_columns = merged_data.drop(columns=['cell', 'sample_name', 'sample_type']).columns  # Exclude the target column\n",
    "\n",
    "# Use the same number of PCA components as used in the model\n",
    "n_components = model.input_shape[1]\n",
    "\n",
    "# Independent datasets\n",
    "independent_datasets = {\n",
    "    \"GSM4869653_xOsiCriz\":\"/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\",\n",
    "    \n",
    "}\n",
    "\n",
    "# Process each independent dataset and make predictions\n",
    "results = {}\n",
    "for dataset_name, file_path in independent_datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    counts = predict_independent_dataset(model, file_path, train_columns, n_components)\n",
    "    results[f\"Transfer Learning - {dataset_name}\"] = counts\n",
    "    print(f\"{dataset_name} - Transfer Learning Predictions: {counts}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Prediction Results:\")\n",
    "for model_dataset, counts in results.items():\n",
    "    print(f\"{model_dataset}: Predictions = {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671e7ceb-b42f-4449-a0e6-33e605a879ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:55:34.873214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 16:55:36.954816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 16:55:37.363403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 16:55:37.363441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 16:55:40.168024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 16:56:18.757450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 49.2 GiB for an array with shape (3298585886, 2) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m\n\u001b[1;32m    109\u001b[0m additional_datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    110\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/project_2010751/GSM8118468_ob_treated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPersister\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    111\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Persister\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    112\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Persister\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    113\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Persister\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m ]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m X, y, merged_data, gene_names \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_datasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Proceed only if both classes are present\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Apply PCA for dimensionality reduction\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(metadata_path, data_path, additional_datasets)\u001b[0m\n\u001b[1;32m     56\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Update labels for the merged_data based on specific sample names\u001b[39;00m\n\u001b[1;32m     62\u001b[0m persister_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m14_high\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m14_med\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m14_low\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/impute/_base.py:612\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m     n_missing \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(mask_valid_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    611\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(valid_statistics, n_missing)\n\u001b[0;32m--> 612\u001b[0m     coordinates \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_valid_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    614\u001b[0m     X[coordinates] \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m    616\u001b[0m X_indicator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_transform_indicator(missing_mask)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 49.2 GiB for an array with shape (3298585886, 2) and data type int64"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Function to load data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(metadata_path, data_path, additional_datasets=None):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    if len(common_cells) == 0:\n",
    "        raise ValueError(\"No common cells found between metadata and scRNA data.\")\n",
    "\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "\n",
    "    if additional_datasets:\n",
    "        for file_path, sample_type in additional_datasets:\n",
    "            additional_data = pd.concat(load_data_in_chunks(file_path)).transpose()\n",
    "            additional_data.reset_index(inplace=True)\n",
    "            additional_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "            additional_data['cell'] = additional_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "            additional_data['sample_type'] = sample_type\n",
    "            merged_data = pd.concat([merged_data, additional_data], ignore_index=True, sort=False)\n",
    "\n",
    "    gene_names = merged_data.columns[1:-2]  # Assuming the first column is 'cell', and last two are 'sample_name' and 'sample_type'\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'], errors='ignore')\n",
    "    X.columns = X.columns.astype(str)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    # Update labels for the merged_data based on specific sample names\n",
    "    persister_samples = [\"7\", \"3\", \"14_high\", \"14_med\", \"14_low\"]\n",
    "\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    return X, y_model1, merged_data, gene_names\n",
    "\n",
    "# CNN-RNN Hybrid Model\n",
    "def create_cnn_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        lr = lr * 0.85\n",
    "    return lr\n",
    "\n",
    "# Main Code Execution\n",
    "metadata_path = '/scratch/project_2010376/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/GSE150949_pc9_count_matrix.csv'\n",
    "additional_datasets = [\n",
    "    ('/scratch/project_2010751/GSM8118468_ob_treated.csv', 'Persister'),\n",
    "    ('/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv', 'Non-Persister')\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, merged_data, gene_names = preprocess_data(metadata_path, data_path, additional_datasets)\n",
    "\n",
    "# Proceed only if both classes are present\n",
    "if X is not None and y is not None:\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=100)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "\n",
    "    # Apply SMOTE for balancing the dataset\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y)\n",
    "\n",
    "    # Train-validation-test split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Reshape for CNN input\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Fix input shape by making it a tuple\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "    # Train the CNN-RNN model\n",
    "    model = create_cnn_rnn_model(input_shape)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_cnn_rnn_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    # Model training\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation),\n",
    "                        epochs=100, batch_size=64, callbacks=[early_stopping, model_checkpoint, lr_schedule])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-Persister', 'Persister'], yticklabels=['Non-Persister', 'Persister'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the final model\n",
    "    model.save('final_cnn_rnn_model.keras')\n",
    "else:\n",
    "    print(\"Skipping model training due to insufficient classes in the labeled data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1822a67a-e95c-40b1-8804-e8ce5e9ed5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 21:14:56.783916: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 21:14:58.972570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 21:14:59.364475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 21:14:59.364500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 21:15:03.024119: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 21:15:42.175590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'Non-Persister': 0, 'Persister': 1}\n",
      "Class 0 corresponds to Non-Persister, and class 1 corresponds to Persister.\n",
      "Features for non-persister (class 0) saved to 'cnn_rnn_non_persister_features.txt'\n",
      "Features for persister (class 1) saved to 'cnn_rnn_persister_features.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-21 22:05:11.414148: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 51ms/step - accuracy: 0.6585 - loss: 0.6191 - val_accuracy: 0.5427 - val_loss: 0.8375 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 51ms/step - accuracy: 0.9669 - loss: 0.1056 - val_accuracy: 0.5704 - val_loss: 0.6887 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9897 - loss: 0.0342 - val_accuracy: 0.5449 - val_loss: 0.6829 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9924 - loss: 0.0260 - val_accuracy: 0.5017 - val_loss: 0.9314 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9920 - loss: 0.0272 - val_accuracy: 0.5506 - val_loss: 0.6471 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9953 - loss: 0.0145 - val_accuracy: 0.5017 - val_loss: 0.7022 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9929 - loss: 0.0288 - val_accuracy: 0.5430 - val_loss: 0.6569 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9956 - loss: 0.0145 - val_accuracy: 0.5017 - val_loss: 0.7315 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 50ms/step - accuracy: 0.9964 - loss: 0.0115 - val_accuracy: 0.5167 - val_loss: 0.6726 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m524/524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 0.5017 - val_loss: 0.7241 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m325/524\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 0.9932 - loss: 0.0226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "GSM4869653_xOsiCriz - Transfer Learning Predictions: Counter({1: 2468, 0: 26})\n",
      "\n",
      "Processing dataset: GSE149383_GSM3972669_D0_untreated.dge.csv\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "GSE149383_GSM3972669_D0_untreated.dge.csv - Transfer Learning Predictions: Counter({0: 1365, 1: 239})\n",
      "\n",
      "Processing dataset: GSE160244_GSM4869650_day3_untreated.dge.csv\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "GSE160244_GSM4869650_day3_untreated.dge.csv - Transfer Learning Predictions: Counter({0: 4010, 1: 658})\n",
      "\n",
      "Processing dataset: GSE160244_GSM4869652_xOsi_day3_dge.csv\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "GSE160244_GSM4869652_xOsi_day3_dge.csv - Transfer Learning Predictions: Counter({0: 2835, 1: 837})\n",
      "\n",
      "Processing dataset: GSE260499_GSM8118463_Osi.RDS\n",
      "Error processing /scratch/project_2010751/GSE260499_GSM8118463_Osi.RDS: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n",
      "GSE260499_GSM8118463_Osi.RDS - Transfer Learning Predictions: Counter()\n",
      "\n",
      "Processing dataset: normalized_GSE150949_pc9_count.csv\n",
      "\u001b[1m693/693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step\n",
      "normalized_GSE150949_pc9_count.csv - Transfer Learning Predictions: Counter({1: 12267, 0: 9900})\n",
      "\n",
      "Final Prediction Results:\n",
      "Transfer Learning - GSE134836_GSM3972651_PC9D0_untreated_filtered.csv: Predictions = Counter({0: 12259, 1: 365})\n",
      "Transfer Learning - GSM4869650_xCtrl: Predictions = Counter({0: 3983, 1: 685})\n",
      "Transfer Learning - new_GSM4869650_xCtrl.dge.csv: Predictions = Counter({0: 3964, 1: 704})\n",
      "Transfer Learning - GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv: Predictions = Counter({0: 1274})\n",
      "Transfer Learning - GSM4869653_xOsiCriz: Predictions = Counter({1: 2468, 0: 26})\n",
      "Transfer Learning - GSE149383_GSM3972669_D0_untreated.dge.csv: Predictions = Counter({0: 1365, 1: 239})\n",
      "Transfer Learning - GSE160244_GSM4869650_day3_untreated.dge.csv: Predictions = Counter({0: 4010, 1: 658})\n",
      "Transfer Learning - GSE160244_GSM4869652_xOsi_day3_dge.csv: Predictions = Counter({0: 2835, 1: 837})\n",
      "Transfer Learning - GSE260499_GSM8118463_Osi.RDS: Predictions = Counter()\n",
      "Transfer Learning - normalized_GSE150949_pc9_count.csv: Predictions = Counter({1: 12267, 0: 9900})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Function to load data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(metadata_path, data_path, additional_datasets=None):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "\n",
    "    if additional_datasets:\n",
    "        for file_path, sample_type in additional_datasets:\n",
    "            additional_data = pd.concat(load_data_in_chunks(file_path)).transpose()\n",
    "            additional_data.reset_index(inplace=True)\n",
    "            additional_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "            additional_data['cell'] = additional_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "            additional_data['sample_type'] = sample_type\n",
    "            merged_data = pd.concat([merged_data, additional_data], ignore_index=True, sort=False)\n",
    "\n",
    "    gene_names = merged_data.columns[1:-2]  # Assuming the first column is 'cell', and last two are 'sample_name' and 'sample_type'\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'], errors='ignore')\n",
    "    X.columns = X.columns.astype(str)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    return X, y, merged_data, label_encoder, gene_names\n",
    "\n",
    "# CNN-RNN Hybrid Model\n",
    "def create_cnn_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        lr = lr * 0.85\n",
    "    return lr\n",
    "\n",
    "# Main Code Execution\n",
    "metadata_path = '/scratch/project_2010376/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/normalized_GSE150949_pc9_count.csv'\n",
    "additional_datasets = [\n",
    "    ('/scratch/project_2010751/GSM8118468_ob_treated.csv', 'Persister'),\n",
    "    ('/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv', 'Non-Persister'),\n",
    "    ('/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv', 'Non-Persister')\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, merged_data, label_encoder, gene_names = preprocess_data(metadata_path, data_path, additional_datasets)\n",
    "\n",
    "# Confirm class 0 is non-persister and class 1 is persister\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoding Mapping:\", label_mapping)\n",
    "\n",
    "if label_mapping['Non-Persister'] == 0 and label_mapping['Persister'] == 1:\n",
    "    print(\"Class 0 corresponds to Non-Persister, and class 1 corresponds to Persister.\")\n",
    "else:\n",
    "    print(\"Class 0 corresponds to Persister, and class 1 corresponds to Non-Persister.\")\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=100)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE for balancing the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_reduced, y)\n",
    "\n",
    "# Extract features for non-persister (class 0) and persister (class 1)\n",
    "non_persister_features = X_resampled[y_resampled == 0]\n",
    "persister_features = X_resampled[y_resampled == 1]\n",
    "\n",
    "# Ensure all gene names are strings\n",
    "gene_names = [str(gene) for gene in gene_names]\n",
    "\n",
    "# Save the features to text files with gene names as headers\n",
    "np.savetxt('cnn_rnn_non_persister_features.txt', non_persister_features, delimiter=',', header=','.join(gene_names), fmt='%.6f', comments='')\n",
    "np.savetxt('cnn_rnn_persister_features.txt', persister_features, delimiter=',', header=','.join(gene_names), fmt='%.6f', comments='')\n",
    "\n",
    "print(\"Features for non-persister (class 0) saved to 'cnn_rnn_non_persister_features.txt'\")\n",
    "print(\"Features for persister (class 1) saved to 'cnn_rnn_persister_features.txt'\")\n",
    "\n",
    "# Train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Fix input shape by making it a tuple\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Train the CNN-RNN model\n",
    "model = create_cnn_rnn_model(input_shape)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_cnn_rnn_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Model training\n",
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation),\n",
    "                    epochs=100, batch_size=64, callbacks=[early_stopping, model_checkpoint, lr_schedule])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-Persister', 'Persister'], yticklabels=['Non-Persister', 'Persister'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the final model\n",
    "model.save('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Function to preprocess independent datasets\n",
    "def preprocess_independent_data(file_path, train_columns, n_components):\n",
    "    independent_data = pd.concat(load_data_in_chunks(file_path)).transpose()\n",
    "    independent_data.reset_index(inplace=True)\n",
    "    independent_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "\n",
    "    if independent_data.shape[0] > independent_data.shape[1]:\n",
    "        independent_data = independent_data.transpose()\n",
    "\n",
    "    X_independent = independent_data.drop(columns=['cell', 'sample_name', 'sample_type'], errors='ignore')\n",
    "    X_independent.columns = X_independent.columns.astype(str)\n",
    "    train_columns = train_columns.astype(str)\n",
    "\n",
    "    X_independent = X_independent.reindex(columns=train_columns, fill_value=0)\n",
    "    X_independent = X_independent.apply(pd.to_numeric, errors='coerce')\n",
    "    X_independent.fillna(0, inplace=True)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_independent = pca.fit_transform(X_independent)\n",
    "    \n",
    "    return X_independent\n",
    "\n",
    "# Function to predict on independent datasets\n",
    "def predict_independent_dataset(model, file_path, train_columns, n_components):\n",
    "    try:\n",
    "        X_independent = preprocess_independent_data(file_path, train_columns, n_components)\n",
    "        X_independent = X_independent.reshape(X_independent.shape[0], X_independent.shape[1], 1)\n",
    "        predictions = model.predict(X_independent)\n",
    "        predicted_labels = (predictions > 0.5).astype(int)\n",
    "        counts = Counter(predicted_labels.flatten())\n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return Counter()\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Get the feature columns from merged_data used in training\n",
    "train_columns = merged_data.drop(columns=['cell', 'sample_name', 'sample_type']).columns  # Exclude the target column\n",
    "\n",
    "# Use the same number of PCA components as used in the model\n",
    "n_components = model.input_shape[1]\n",
    "\n",
    "# Independent datasets\n",
    "independent_datasets = {\n",
    "    \"GSE134836_GSM3972651_PC9D0_untreated_filtered.csv\": \"/scratch/project_2010751/GSE134836_GSM3972651_PC9D0_untreated_filtered.csv\",\n",
    "    \"GSM4869650_xCtrl\": \"/scratch/project_2010376/GSM4869650_xCtrl.dge.csv\",\n",
    "    \"new_GSM4869650_xCtrl.dge.csv\": \"/scratch/project_2010376/new_GSM4869650_xCtrl.dge.csv\",\n",
    "    \"GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\": \"/scratch/project_2010751/GSE138693_GSM4116265_PC9_1_invitro_normalized_untreated.csv\",\n",
    "    \"GSM4869653_xOsiCriz\":\"/scratch/project_2010751/GSE134839_GSM3972657_PC90D0_untreated.dge.csv\",\n",
    "    \"GSE149383_GSM3972669_D0_untreated.dge.csv\":\"/scratch/project_2010751/GSE149383_GSM3972669_D0_untreated.dge.csv\",\n",
    "    \"GSE160244_GSM4869650_day3_untreated.dge.csv\":\"/scratch/project_2010751/GSE160244_GSM4869650_day3_untreated.dge.csv\",\n",
    "    \"GSE160244_GSM4869652_xOsi_day3_dge.csv\":\"/scratch/project_2010751/GSE160244_GSM4869652_xOsi_day3_dge.csv\",\n",
    "    \"GSE260499_GSM8118463_Osi.RDS\":\"/scratch/project_2010751/GSE260499_GSM8118463_Osi.RDS\",\n",
    "    \"normalized_GSE150949_pc9_count.csv\":\"/scratch/project_2010751/normalized_GSE150949_pc9_count.csv\"\n",
    "}\n",
    "\n",
    "# Process each independent dataset and make predictions\n",
    "results = {}\n",
    "for dataset_name, file_path in independent_datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    counts = predict_independent_dataset(model, file_path, train_columns, n_components)\n",
    "    results[f\"Transfer Learning - {dataset_name}\"] = counts\n",
    "    print(f\"{dataset_name} - Transfer Learning Predictions: {counts}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Prediction Results:\")\n",
    "for model_dataset, counts in results.items():\n",
    "    print(f\"{model_dataset}: Predictions = {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3135456e-1528-4bce-ab82-171af2b629f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: normalized_GSE150949_pc9_count.csv\n",
      "\u001b[1m693/693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step\n",
      "normalized_GSE150949_pc9_count.csv - Transfer Learning Predictions: Counter({1: 20537, 0: 1630})\n",
      "\n",
      "Final Prediction Results:\n",
      "Transfer Learning - normalized_GSE150949_pc9_count.csv: Predictions = Counter({1: 20537, 0: 1630})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('final_cnn_rnn_model.keras')\n",
    "\n",
    "# Get the feature columns from merged_data used in training\n",
    "train_columns = merged_data.drop(columns=['cell', 'sample_name', 'sample_type']).columns  # Exclude the target column\n",
    "\n",
    "# Use the same number of PCA components as used in the model\n",
    "n_components = model.input_shape[1]\n",
    "\n",
    "# Independent datasets\n",
    "independent_datasets = {\n",
    "\n",
    "    \"normalized_GSE150949_pc9_count.csv\":\"/scratch/project_2010376/GSE150949_pc9_count_matrix.csv\"\n",
    "}\n",
    "\n",
    "# Process each independent dataset and make predictions\n",
    "results = {}\n",
    "for dataset_name, file_path in independent_datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    counts = predict_independent_dataset(model, file_path, train_columns, n_components)\n",
    "    results[f\"Transfer Learning - {dataset_name}\"] = counts\n",
    "    print(f\"{dataset_name} - Transfer Learning Predictions: {counts}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Prediction Results:\")\n",
    "for model_dataset, counts in results.items():\n",
    "    print(f\"{model_dataset}: Predictions = {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee683c2-5513-4b99-bc62-5a0c6879a5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
