{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:233: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path,engine='python',encoding='utf-8',nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_data)\n",
    "\n",
    "# Construct k-NN graph\n",
    "def construct_knn_graph(X, k):\n",
    "    knn_graph = kneighbors_graph(X, k, mode='connectivity', include_self=False)\n",
    "    coo_graph = knn_graph.tocoo()  # Convert to COO format\n",
    "    G = nx.from_edgelist(zip(coo_graph.row, coo_graph.col))  # Create graph from COO format\n",
    "    return G\n",
    "\n",
    "# Calculate Ollivier-Ricci curvature\n",
    "def compute_ricci_curvature(G, alpha=0.5):\n",
    "    orc = OllivierRicci(G, alpha=alpha, verbose=\"INFO\")\n",
    "    orc.compute_ricci_curvature()\n",
    "    return nx.get_edge_attributes(orc.G, \"ricciCurvature\")\n",
    "\n",
    "# Identify high proliferation index cells with alternative curvature aggregation methods\n",
    "def identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=None, top_percentile=None, centrality_measure=None):\n",
    "    if method == 'threshold':\n",
    "        high_proliferation_edges = [(u, v) for (u, v), curvature in ricci_curvatures.items() if curvature > threshold]\n",
    "    elif method == 'top_percentile':\n",
    "        sorted_curvatures = sorted(ricci_curvatures.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_curvatures) * top_percentile / 100)\n",
    "        high_proliferation_edges = [(u, v) for (u, v), _ in sorted_curvatures[:top_n]]\n",
    "    elif method == 'centrality':\n",
    "        centrality_scores = nx.betweenness_centrality(G, weight='ricciCurvature')\n",
    "        sorted_centrality = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_centrality) * top_percentile / 100)\n",
    "        high_proliferation_nodes = [node for node, _ in sorted_centrality[:top_n]]\n",
    "        high_proliferation_edges = [(u, v) for (u, v) in G.edges() if u in high_proliferation_nodes or v in high_proliferation_nodes]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'threshold', 'top_percentile', or 'centrality'.\")\n",
    "    \n",
    "    high_proliferation_nodes = set([node for edge in high_proliferation_edges for node in edge])\n",
    "    return high_proliferation_nodes\n",
    "\n",
    "# Differential Expression Analysis\n",
    "def differential_expression_analysis(data, high_proliferation_nodes):\n",
    "    if 'persister_label' in data.columns:\n",
    "        persister_labels = data['persister_label'].values\n",
    "        high_proliferation_labels = np.zeros_like(persister_labels)\n",
    "        high_proliferation_labels[list(high_proliferation_nodes)] = 1\n",
    "\n",
    "        # Perform differential expression analysis\n",
    "        differential_genes = []\n",
    "        for gene_index in range(data.shape[1]):\n",
    "            gene_expr_high_prolif = data[high_proliferation_labels == 1, gene_index]\n",
    "            gene_expr_non_high_prolif = data[high_proliferation_labels == 0, gene_index]\n",
    "            _, p_value = ttest_ind(gene_expr_high_prolif, gene_expr_non_high_prolif)\n",
    "            if p_value < 0.05:  # Adjust significance level as needed\n",
    "                differential_genes.append(gene_index)\n",
    "        return differential_genes\n",
    "    else:\n",
    "        print(\"Ground truth labels ('persister_label') not available. Skipping differential expression analysis.\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "k_values = [5, 10, 15, 20]\n",
    "threshold = 0.5  # Threshold for identifying high proliferation cells\n",
    "top_percentile = 5  # Top percentile for alternative aggregation methods\n",
    "centrality_percentile = 5  # Percentile of nodes for centrality-based aggregation\n",
    "\n",
    "# Experiment with different k values for kNN graph\n",
    "for k in k_values:\n",
    "    G = construct_knn_graph(X_pca, k)\n",
    "    ricci_curvatures = compute_ricci_curvature(G)\n",
    "    \n",
    "    # Identify high proliferation cells using thresholding method\n",
    "    high_proliferation_nodes_threshold = identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=threshold)\n",
    "    print(f\"High Proliferation Cells with threshold method and k={k}:\", high_proliferation_nodes_threshold)\n",
    "    \n",
    "    # Identify high proliferation cells using top percentile method\n",
    "    high_proliferation_nodes_percentile = identify_high_proliferation_nodes(ricci_curvatures, method='top_percentile', top_percentile=top_percentile)\n",
    "    print(f\"High Proliferation Cells with top percentile method and k={k}:\", high_proliferation_nodes_percentile)\n",
    "    \n",
    "    # Identify high proliferation cells using centrality method\n",
    "    high_proliferation_nodes_centrality = identify_high_proliferation_nodes(ricci_curvatures, method='centrality', top_percentile=centrality_percentile)\n",
    "    print(f\"High Proliferation Cells with centrality method and k={k}:\", high_proliferation_nodes_centrality)\n",
    "\n",
    "    # Differential expression analysis\n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_threshold)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with threshold method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_percentile)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with top percentile method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_centrality)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with centrality method:\", differential_genes)\n",
    "\n",
    "    # Visualize curvature distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(list(ricci_curvatures.values()), bins=50, kde=True)\n",
    "    plt.axvline(np.mean(list(ricci_curvatures.values())), color='r', linestyle='--')\n",
    "    plt.title(f'Distribution of Ricci Curvatures with k={k}')\n",
    "    plt.xlabel('Ricci Curvature')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize high proliferation nodes on the PCA plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='gray', alpha=0.5, label='All Cells')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_threshold), 0], X_pca[list(high_proliferation_nodes_threshold), 1], c='red', label='High Proliferation Cells (Threshold)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_percentile), 0], X_pca[list(high_proliferation_nodes_percentile), 1], c='blue', label='High Proliferation Cells (Percentile)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_centrality), 0], X_pca[list(high_proliferation_nodes_centrality), 1], c='green', label='High Proliferation Cells (Centrality)')\n",
    "    plt.title(f'PCA of scRNA-seq Data with k={k}')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce5b837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load scRNA data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Ensure 'persister_label' column exists\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersister_label\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)[0]\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size)\n",
    "\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for scBERT model\n",
    "for epoch in range(num_epochs):\n",
    "    scbert_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_bert:\n",
    "        optimizer_bert.zero_grad()\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_bert)}\")\n",
    "\n",
    "# Evaluation loop for scBERT model\n",
    "scbert_model.eval()\n",
    "correct_bert = 0\n",
    "total_bert = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_bert:\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_bert += labels.size(0)\n",
    "        correct_bert += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scBERT model on test set:', (correct_bert / total_bert))\n",
    "\n",
    "# Training loop for scGPT model\n",
    "for epoch in range(num_epochs):\n",
    "    scgpt_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_gpt:\n",
    "        optimizer_gpt.zero_grad()\n",
    "\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_gpt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_gpt)}\")\n",
    "\n",
    "# Evaluation loop for scGPT model\n",
    "scgpt_model.eval()\n",
    "correct_gpt = 0\n",
    "total_gpt = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_gpt:\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_gpt += labels.size(0)\n",
    "        correct_gpt += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scGPT model on test set:', (correct_gpt / total_gpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "29/05/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5318bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-05-31 19:44:39,948] A new study created in memory with name: no-name-0b0af047-c31c-4005-b676-f8ad60cf71b3\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "[I 2024-05-31 20:13:10,376] Trial 0 finished with value: 0.6778001487255096 and parameters: {'batch_size': 64, 'learning_rate': 0.0003695669422931764, 'dropout_rate': 0.1523855534373104}. Best is trial 0 with value: 0.6778001487255096.\n",
      "[I 2024-05-31 20:30:33,620] Trial 1 finished with value: 0.683274507522583 and parameters: {'batch_size': 64, 'learning_rate': 1.843960872354099e-05, 'dropout_rate': 0.3438750063315392}. Best is trial 0 with value: 0.6778001487255096.\n",
      "[I 2024-05-31 21:02:00,670] Trial 2 finished with value: 0.7191212475299835 and parameters: {'batch_size': 64, 'learning_rate': 0.0008053380662886468, 'dropout_rate': 0.49462457821963646}. Best is trial 0 with value: 0.6778001487255096.\n",
      "[I 2024-05-31 21:19:16,700] Trial 3 finished with value: 0.6739709377288818 and parameters: {'batch_size': 64, 'learning_rate': 1.4799746131625126e-05, 'dropout_rate': 0.25738271856885775}. Best is trial 3 with value: 0.6739709377288818.\n",
      "[I 2024-05-31 21:33:28,436] Trial 4 finished with value: 0.7078803976376852 and parameters: {'batch_size': 32, 'learning_rate': 0.0003780896432919425, 'dropout_rate': 0.47285602152954176}. Best is trial 3 with value: 0.6739709377288818.\n",
      "[I 2024-05-31 22:06:16,100] Trial 5 finished with value: 0.6800177454948425 and parameters: {'batch_size': 16, 'learning_rate': 3.857992692996665e-05, 'dropout_rate': 0.10135162398672058}. Best is trial 3 with value: 0.6739709377288818.\n",
      "[I 2024-05-31 22:34:04,063] Trial 6 finished with value: 0.6621859073638916 and parameters: {'batch_size': 64, 'learning_rate': 0.0007634347890859355, 'dropout_rate': 0.2628560277940932}. Best is trial 6 with value: 0.6621859073638916.\n",
      "[I 2024-05-31 23:07:40,791] Trial 7 finished with value: 0.6737666726112366 and parameters: {'batch_size': 64, 'learning_rate': 0.0007917984670573577, 'dropout_rate': 0.3986196438559877}. Best is trial 6 with value: 0.6621859073638916.\n",
      "[I 2024-05-31 23:27:42,860] Trial 8 finished with value: 0.6894221107165018 and parameters: {'batch_size': 32, 'learning_rate': 0.00020042859562035891, 'dropout_rate': 0.10162240422703017}. Best is trial 6 with value: 0.6621859073638916.\n",
      "[I 2024-05-31 23:44:22,116] Trial 9 finished with value: 0.679380198319753 and parameters: {'batch_size': 32, 'learning_rate': 0.00011739615354289177, 'dropout_rate': 0.28911870665575706}. Best is trial 6 with value: 0.6621859073638916.\n",
      "[I 2024-05-31 23:56:51,535] Trial 10 finished with value: 0.6747001528739929 and parameters: {'batch_size': 16, 'learning_rate': 6.019346970520772e-05, 'dropout_rate': 0.20452724527883995}. Best is trial 6 with value: 0.6621859073638916.\n",
      "[I 2024-06-01 00:10:03,036] Trial 11 finished with value: 0.7476276755332947 and parameters: {'batch_size': 64, 'learning_rate': 0.0008367685262568177, 'dropout_rate': 0.4012741526656045}. Best is trial 6 with value: 0.6621859073638916.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path,nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_dataloader)}')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "best_params = study.best_params\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "learning_rate = best_params['learning_rate']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "\n",
    "# Train final scBERT model with best hyperparameters\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scBERT model on test set\n",
    "test_accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "print('Accuracy of scBERT model on test set:', test_accuracy_bert)\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scGPT\n",
    "def objective_gpt(trial):\n",
    "    batch_size_gpt = trial.suggest_categorical('batch_size_gpt', [16, 32, 64])\n",
    "    learning_rate_gpt = trial.suggest_float('learning_rate_gpt', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate_gpt = trial.suggest_float('dropout_rate_gpt', 0.1, 0.5)\n",
    "\n",
    "    scgpt_model = SCGPTClassifier()\n",
    "    scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "    optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "    train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "\n",
    "    best_loss_gpt = float('inf')\n",
    "    early_stop_count_gpt = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scgpt_model.train()\n",
    "        running_loss_gpt = 0.0\n",
    "        for inputs, labels in train_dataloader_gpt:\n",
    "            optimizer_gpt.zero_grad()\n",
    "            input_ids = inputs\n",
    "            outputs = scgpt_model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_gpt.step()\n",
    "            running_loss_gpt += loss.item()\n",
    "        epoch_loss_gpt = running_loss_gpt / len(train_dataloader_gpt)\n",
    "        if epoch_loss_gpt < best_loss_gpt:\n",
    "            best_loss_gpt = epoch_loss_gpt\n",
    "            early_stop_count_gpt = 0\n",
    "        else:\n",
    "            early_stop_count_gpt += 1\n",
    "            if early_stop_count_gpt >= 3:\n",
    "                break\n",
    "    return best_loss_gpt\n",
    "\n",
    "study_gpt = optuna.create_study(direction='minimize')\n",
    "study_gpt.optimize(objective_gpt, n_trials=50)\n",
    "best_params_gpt = study_gpt.best_params\n",
    "\n",
    "batch_size_gpt = best_params_gpt['batch_size_gpt']\n",
    "learning_rate_gpt = best_params_gpt['learning_rate_gpt']\n",
    "dropout_rate_gpt = best_params_gpt['dropout_rate_gpt']\n",
    "\n",
    "# Train final scGPT model with best hyperparameters\n",
    "scgpt_model = SCGPTClassifier() \n",
    "scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scGPT model on test set\n",
    "test_accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "print('Accuracy of scGPT model on test set:', test_accuracy_gpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "01/06/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5afaf9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequences, labels\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Load your data\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m train_sequences, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m val_sequences, val_labels \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Initialize the BERT tokenizer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m---> 48\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     50\u001b[0m     labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16  # Reduced batch size\n",
    "num_epochs = 5  # Reduced number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Reduced number of trials\n",
    "best_params = study.best_params\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "learning_rate = best_params['learning_rate']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "\n",
    "# Train final scBERT model with best hyperparameters\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scBERT model on test set\n",
    "test_accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "print('Accuracy of scBERT model on test set:', test_accuracy_bert)\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scGPT\n",
    "def objective_gpt(trial):\n",
    "    batch_size_gpt = trial.suggest_categorical('batch_size_gpt', [16, 32])\n",
    "    learning_rate_gpt = trial.suggest_float('learning_rate_gpt', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate_gpt = trial.suggest_float('dropout_rate_gpt', 0.1, 0.5)\n",
    "\n",
    "    scgpt_model = SCGPTClassifier()\n",
    "    scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "    optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "    train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "\n",
    "    best_loss_gpt = float('inf')\n",
    "    early_stop_count_gpt = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scgpt_model.train()\n",
    "        running_loss_gpt = 0.0\n",
    "        for inputs, labels in train_dataloader_gpt:\n",
    "            optimizer_gpt.zero_grad()\n",
    "            input_ids = inputs\n",
    "            outputs = scgpt_model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_gpt.step()\n",
    "            running_loss_gpt += loss.item()\n",
    "        epoch_loss_gpt = running_loss_gpt / len(train_dataloader_gpt)\n",
    "        if epoch_loss_gpt < best_loss_gpt:\n",
    "            best_loss_gpt = epoch_loss_gpt\n",
    "            early_stop_count_gpt = 0\n",
    "        else:\n",
    "            early_stop_count_gpt += 1\n",
    "            if early_stop_count_gpt >= 3:\n",
    "                break\n",
    "    return best_loss_gpt\n",
    "\n",
    "study_gpt = optuna.create_study(direction='minimize')\n",
    "study_gpt.optimize(objective_gpt, n_trials=20)  # Reduced number of trials\n",
    "best_params_gpt = study_gpt.best_params\n",
    "\n",
    "batch_size_gpt = best_params_gpt['batch_size_gpt']\n",
    "learning_rate_gpt = best_params_gpt['learning_rate_gpt']\n",
    "dropout_rate_gpt = best_params_gpt['dropout_rate_gpt']\n",
    "\n",
    "# Train final scGPT model with best hyperparameters\n",
    "scgpt_model = SCGPTClassifier() \n",
    "scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scGPT model on test set\n",
    "test_accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "print('Accuracy of scGPT model on test set:', test_accuracy_gpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8061c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-01 11:16:29,609] A new study created in memory with name: no-name-5358ac86-67c1-4aeb-a511-4d5431ece3ef\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "[I 2024-06-01 11:31:35,849] Trial 0 finished with value: 0.6889887094497681 and parameters: {'batch_size': 16, 'learning_rate': 0.0007606855527820207, 'dropout_rate': 0.21249282974002137}. Best is trial 0 with value: 0.6889887094497681.\n",
      "[I 2024-06-01 11:46:40,141] Trial 1 finished with value: 0.6890106558799743 and parameters: {'batch_size': 16, 'learning_rate': 3.56352398770645e-05, 'dropout_rate': 0.23395271216020364}. Best is trial 0 with value: 0.6889887094497681.\n",
      "[I 2024-06-01 12:01:15,211] Trial 2 finished with value: 0.6768436312675477 and parameters: {'batch_size': 16, 'learning_rate': 5.567404829001719e-05, 'dropout_rate': 0.2123694068744817}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 12:16:57,781] Trial 3 finished with value: 0.737666130065918 and parameters: {'batch_size': 32, 'learning_rate': 0.0003565355324869913, 'dropout_rate': 0.25054834767341677}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 12:31:51,592] Trial 4 finished with value: 0.7082915663719177 and parameters: {'batch_size': 16, 'learning_rate': 0.0003491848647747935, 'dropout_rate': 0.3210866547493284}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 12:47:39,547] Trial 5 finished with value: 0.6928661863009135 and parameters: {'batch_size': 32, 'learning_rate': 0.00045379679944451473, 'dropout_rate': 0.10662493703888605}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 13:03:23,986] Trial 6 finished with value: 0.6895312269528707 and parameters: {'batch_size': 32, 'learning_rate': 0.00010947688226359782, 'dropout_rate': 0.26943842484718206}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 13:19:04,097] Trial 7 finished with value: 0.7106196284294128 and parameters: {'batch_size': 32, 'learning_rate': 1.8659099780747696e-05, 'dropout_rate': 0.3326498759839381}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 13:33:42,230] Trial 8 finished with value: 0.6789436101913452 and parameters: {'batch_size': 16, 'learning_rate': 0.00045270319901011816, 'dropout_rate': 0.29236045317596837}. Best is trial 2 with value: 0.6768436312675477.\n",
      "[I 2024-06-01 13:48:23,445] Trial 9 finished with value: 0.7546623468399047 and parameters: {'batch_size': 16, 'learning_rate': 0.0006271031527848417, 'dropout_rate': 0.3755110548529943}. Best is trial 2 with value: 0.6768436312675477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7756344079971313\n",
      "Epoch 2/5, Loss: 0.7250105857849121\n",
      "Epoch 3/5, Loss: 0.7645009517669678\n",
      "Epoch 4/5, Loss: 0.6893216490745544\n",
      "Epoch 5/5, Loss: 0.7204572916030884\n",
      "scBERT Test Accuracy: 0.6\n",
      "Epoch 1/5, Loss: 2.0250087141990663\n",
      "Epoch 2/5, Loss: 1.107263731956482\n",
      "Epoch 3/5, Loss: 0.8096065640449523\n",
      "Epoch 4/5, Loss: 0.7738061070442199\n",
      "Epoch 5/5, Loss: 0.6968650698661805\n",
      "scGPT Test Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16  # Reduced batch size\n",
    "num_epochs = 5  # Reduced number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Train and evaluate the scBERT model with optimized hyperparameters\n",
    "best_params = study.best_params\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "print(f'scBERT Test Accuracy: {accuracy_bert}')\n",
    "\n",
    "# Train and evaluate the scGPT model\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "print(f'scGPT Test Accuracy: {accuracy_gpt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bfec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-01 20:28:43,299] A new study created in memory with name: no-name-0ef83726-2d1b-4e55-9bb7-3291e85ed4e8\n",
      "[I 2024-06-01 20:43:53,611] Trial 0 finished with value: 0.7099897623062134 and parameters: {'batch_size': 16, 'learning_rate': 0.00013957952949142743, 'dropout_rate': 0.4683348289467878}. Best is trial 0 with value: 0.7099897623062134.\n",
      "[I 2024-06-01 20:59:58,904] Trial 1 finished with value: 0.6867982149124146 and parameters: {'batch_size': 32, 'learning_rate': 3.102497578731916e-05, 'dropout_rate': 0.14048672967982923}. Best is trial 1 with value: 0.6867982149124146.\n",
      "[I 2024-06-01 21:12:03,396] Trial 2 finished with value: 0.6781678080558777 and parameters: {'batch_size': 16, 'learning_rate': 0.0007219338176082561, 'dropout_rate': 0.4184725725831453}. Best is trial 2 with value: 0.6781678080558777.\n",
      "[I 2024-06-01 21:28:17,696] Trial 3 finished with value: 0.7071136236190796 and parameters: {'batch_size': 32, 'learning_rate': 1.2026154775285653e-05, 'dropout_rate': 0.4403860710280657}. Best is trial 2 with value: 0.6781678080558777.\n",
      "[I 2024-06-01 21:40:25,223] Trial 4 finished with value: 0.6973963975906372 and parameters: {'batch_size': 16, 'learning_rate': 1.7971540262265312e-05, 'dropout_rate': 0.34435009364208835}. Best is trial 2 with value: 0.6781678080558777.\n",
      "[I 2024-06-01 21:55:46,786] Trial 5 finished with value: 0.6926785826683044 and parameters: {'batch_size': 16, 'learning_rate': 1.0120523748178463e-05, 'dropout_rate': 0.2425052157443699}. Best is trial 2 with value: 0.6781678080558777.\n",
      "[I 2024-06-01 22:11:51,690] Trial 6 finished with value: 0.6778142054875692 and parameters: {'batch_size': 32, 'learning_rate': 6.165141593534278e-05, 'dropout_rate': 0.44658870243891013}. Best is trial 6 with value: 0.6778142054875692.\n",
      "[I 2024-06-01 22:28:03,377] Trial 7 finished with value: 0.6683056751887003 and parameters: {'batch_size': 32, 'learning_rate': 3.623832520426785e-05, 'dropout_rate': 0.37189785645414897}. Best is trial 7 with value: 0.6683056751887003.\n",
      "[I 2024-06-01 22:40:14,850] Trial 8 finished with value: 0.6882381796836853 and parameters: {'batch_size': 16, 'learning_rate': 1.1552152788120049e-05, 'dropout_rate': 0.32977869388603953}. Best is trial 7 with value: 0.6683056751887003.\n",
      "[I 2024-06-01 22:55:32,263] Trial 9 finished with value: 0.7065535664558411 and parameters: {'batch_size': 16, 'learning_rate': 0.00018257673765208882, 'dropout_rate': 0.28690673310107023}. Best is trial 7 with value: 0.6683056751887003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7949920694033304, Time: 189.37s\n",
      "Epoch 2/5, Loss: 0.7408809463183085, Time: 191.53s\n",
      "Epoch 3/5, Loss: 0.7009690006573995, Time: 188.27s\n",
      "Epoch 4/5, Loss: 0.7023180921872457, Time: 190.60s\n",
      "Epoch 5/5, Loss: 0.6998785336812338, Time: 191.97s\n",
      "Total Training Time: 951.74s\n",
      "Total Evaluation Time: 12.07s\n",
      "scBERT Test Accuracy: 0.6\n",
      "Training Time: 951.74s\n",
      "Test Time: 15.05s\n",
      "Epoch 1/5, Loss: 1.9480788826942443, Time: 217.04s\n",
      "Epoch 2/5, Loss: 0.8745065212249756, Time: 214.57s\n",
      "Epoch 3/5, Loss: 0.8125643372535706, Time: 215.01s\n",
      "Epoch 4/5, Loss: 0.7565965414047241, Time: 215.47s\n",
      "Epoch 5/5, Loss: 0.7439954400062561, Time: 216.46s\n",
      "Total Training Time: 1078.54s\n",
      "Total Evaluation Time: 14.78s\n",
      "scGPT Test Accuracy: 0.6\n",
      "Training Time: 1078.54s\n",
      "Test Time: 15.60s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16  # Reduced batch size\n",
    "num_epochs = 5  # Reduced number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    return accuracy\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Train and evaluate the scBERT model with optimized hyperparameters\n",
    "best_params = study.best_params\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "train_start_time = time.time()\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "train_end_time = time.time()\n",
    "\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "test_start_time = time.time()\n",
    "accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "test_end_time = time.time()\n",
    "\n",
    "print(f'scBERT Test Accuracy: {accuracy_bert}')\n",
    "print(f'Training Time: {train_end_time - train_start_time:.2f}s')\n",
    "print(f'Test Time: {test_end_time - test_start_time:.2f}s')\n",
    "\n",
    "# Train and evaluate the scGPT model\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_start_time = time.time()\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "train_end_time = time.time()\n",
    "\n",
    "test_start_time = time.time()\n",
    "accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "test_end_time = time.time()\n",
    "\n",
    "print(f'scGPT Test Accuracy: {accuracy_gpt}')\n",
    "print(f'Training Time: {train_end_time - train_start_time:.2f}s')\n",
    "print(f'Test Time: {test_end_time - test_start_time:.2f}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "02/10/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972ca52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-01 23:51:43,145] A new study created in memory with name: no-name-161f78c0-7cac-4b3e-8d44-9be182b5f44b\n",
      "[I 2024-06-02 00:06:52,297] Trial 0 finished with value: 0.6884732246398926 and parameters: {'batch_size': 16, 'learning_rate': 0.0005801227582949602, 'dropout_rate': 0.34200829077452943}. Best is trial 0 with value: 0.6884732246398926.\n",
      "[I 2024-06-02 00:21:55,879] Trial 1 finished with value: 0.7250295877456665 and parameters: {'batch_size': 16, 'learning_rate': 0.0008760008270346263, 'dropout_rate': 0.2576031911629467}. Best is trial 0 with value: 0.6884732246398926.\n",
      "[I 2024-06-02 00:38:06,246] Trial 2 finished with value: 0.6748929619789124 and parameters: {'batch_size': 32, 'learning_rate': 0.0003391808215470797, 'dropout_rate': 0.2904514869515771}. Best is trial 2 with value: 0.6748929619789124.\n",
      "[I 2024-06-02 00:53:16,047] Trial 3 finished with value: 0.6897659063339233 and parameters: {'batch_size': 16, 'learning_rate': 3.4021726012946205e-05, 'dropout_rate': 0.20544495781249178}. Best is trial 2 with value: 0.6748929619789124.\n",
      "[I 2024-06-02 01:08:21,126] Trial 4 finished with value: 0.6874918699264526 and parameters: {'batch_size': 16, 'learning_rate': 2.926241156451032e-05, 'dropout_rate': 0.20291539096081854}. Best is trial 2 with value: 0.6748929619789124.\n",
      "[I 2024-06-02 01:24:28,497] Trial 5 finished with value: 0.6819592118263245 and parameters: {'batch_size': 32, 'learning_rate': 1.2997022502478975e-05, 'dropout_rate': 0.46499983083537566}. Best is trial 2 with value: 0.6748929619789124.\n",
      "[I 2024-06-02 01:36:34,332] Trial 6 finished with value: 0.6826582431793213 and parameters: {'batch_size': 16, 'learning_rate': 0.0004906774912648829, 'dropout_rate': 0.45710712556993605}. Best is trial 2 with value: 0.6748929619789124.\n",
      "[I 2024-06-02 01:52:49,075] Trial 7 finished with value: 0.6667441328366598 and parameters: {'batch_size': 32, 'learning_rate': 0.0005981386566569035, 'dropout_rate': 0.23163208265748936}. Best is trial 7 with value: 0.6667441328366598.\n",
      "[I 2024-06-02 02:07:58,836] Trial 8 finished with value: 0.6969444751739502 and parameters: {'batch_size': 16, 'learning_rate': 3.084499305880444e-05, 'dropout_rate': 0.3963635419785049}. Best is trial 7 with value: 0.6667441328366598.\n",
      "[I 2024-06-02 02:23:13,506] Trial 9 finished with value: 0.6814158797264099 and parameters: {'batch_size': 16, 'learning_rate': 4.251850000551901e-05, 'dropout_rate': 0.20942819452599776}. Best is trial 7 with value: 0.6667441328366598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9131436944007874, Time: 192.25s\n",
      "Epoch 2/5, Loss: 0.8749574820200602, Time: 192.09s\n",
      "Epoch 3/5, Loss: 0.739409883817037, Time: 191.20s\n",
      "Epoch 4/5, Loss: 0.8021972974141439, Time: 193.72s\n",
      "Epoch 5/5, Loss: 0.7857325077056885, Time: 194.10s\n",
      "Total Training Time: 963.35s\n",
      "Total Evaluation Time: 11.42s\n",
      "scBERT Test Accuracy: 0.4\n",
      "Training Time: 963.35s\n",
      "Test Time: 14.42s\n",
      "Epoch 1/5, Loss: 1.1051663517951966, Time: 222.40s\n",
      "Epoch 2/5, Loss: 0.7276015043258667, Time: 223.12s\n",
      "Epoch 3/5, Loss: 0.7035327911376953, Time: 220.05s\n",
      "Epoch 4/5, Loss: 0.6972214102745056, Time: 216.57s\n",
      "Epoch 5/5, Loss: 0.7177702903747558, Time: 218.97s\n",
      "Total Training Time: 1101.12s\n",
      "Total Evaluation Time: 14.76s\n",
      "scGPT Test Accuracy: 0.6\n",
      "Training Time: 1101.12s\n",
      "Test Time: 15.59s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16  # Reduced batch size\n",
    "num_epochs = 5  # Reduced number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    return accuracy\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Train and evaluate the scBERT model with optimized hyperparameters\n",
    "best_params = study.best_params\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "train_start_time = time.time()\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "train_end_time = time.time()\n",
    "\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "test_start_time = time.time()\n",
    "accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "test_end_time = time.time()\n",
    "\n",
    "print(f'scBERT Test Accuracy: {accuracy_bert}')\n",
    "print(f'Training Time: {train_end_time - train_start_time:.2f}s')\n",
    "print(f'Test Time: {test_end_time - test_start_time:.2f}s')\n",
    "\n",
    "# Train and evaluate the scGPT model\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_start_time = time.time()\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "train_end_time = time.time()\n",
    "\n",
    "test_start_time = time.time()\n",
    "accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "test_end_time = time.time()\n",
    "\n",
    "print(f'scGPT Test Accuracy: {accuracy_gpt}')\n",
    "print(f'Training Time: {train_end_time - train_start_time:.2f}s')\n",
    "print(f'Test Time: {test_end_time - test_start_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28eb9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-02 10:26:09,472] A new study created in memory with name: no-name-aab75bb0-a22f-4637-b6aa-16e01287bb21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7172313531239828, Time: 218.94s\n",
      "Epoch 2/5, Loss: 0.6894198656082153, Time: 205.49s\n",
      "Epoch 3/5, Loss: 0.6763014793395996, Time: 206.48s\n",
      "Epoch 4/5, Loss: 0.6869503657023112, Time: 207.51s\n",
      "Epoch 5/5, Loss: 0.6914328535397848, Time: 205.59s\n",
      "Total Training Time: 1044.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 10:43:49,282] Trial 0 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.098253023730156e-05, 'dropout_rate': 0.19555086246733772}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.21s\n",
      "Epoch 1/5, Loss: 0.8275733947753906, Time: 197.33s\n",
      "Epoch 2/5, Loss: 1.1343012690544128, Time: 197.61s\n",
      "Epoch 3/5, Loss: 0.8109597086906433, Time: 199.33s\n",
      "Epoch 4/5, Loss: 0.7754833459854126, Time: 199.11s\n",
      "Epoch 5/5, Loss: 0.752349579334259, Time: 199.21s\n",
      "Total Training Time: 992.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 11:00:37,180] Trial 1 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.0008388677688757641, 'dropout_rate': 0.4327451740040592}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.72s\n",
      "Epoch 1/5, Loss: 0.6863109111785889, Time: 198.70s\n",
      "Epoch 2/5, Loss: 0.7264795780181885, Time: 198.94s\n",
      "Epoch 3/5, Loss: 0.6964643478393555, Time: 197.94s\n",
      "Epoch 4/5, Loss: 0.7082552313804626, Time: 198.87s\n",
      "Early stopping at epoch 4\n",
      "Total Training Time: 794.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 11:14:06,889] Trial 2 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 2.3286739200669534e-05, 'dropout_rate': 0.40626538961752445}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.68s\n",
      "Epoch 1/5, Loss: 0.6752307415008545, Time: 197.31s\n",
      "Epoch 2/5, Loss: 0.6860402822494507, Time: 197.78s\n",
      "Epoch 3/5, Loss: 0.6839542388916016, Time: 197.77s\n",
      "Epoch 4/5, Loss: 0.7701284766197205, Time: 199.22s\n",
      "Early stopping at epoch 4\n",
      "Total Training Time: 792.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 11:27:34,217] Trial 3 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 1.3507522438771335e-05, 'dropout_rate': 0.3720618926402155}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.68s\n",
      "Epoch 1/5, Loss: 0.8068356037139892, Time: 197.82s\n",
      "Epoch 2/5, Loss: 0.6772196292877197, Time: 204.28s\n",
      "Epoch 3/5, Loss: 0.7117452502250672, Time: 199.76s\n",
      "Epoch 4/5, Loss: 0.7425118684768677, Time: 200.01s\n",
      "Epoch 5/5, Loss: 0.7200944662094116, Time: 200.62s\n",
      "Early stopping at epoch 5\n",
      "Total Training Time: 1002.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 11:44:32,148] Trial 4 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 2.4009374827438337e-05, 'dropout_rate': 0.4657248275799887}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.81s\n",
      "Epoch 1/5, Loss: 0.7106072505315145, Time: 206.25s\n",
      "Epoch 2/5, Loss: 0.6830528577168783, Time: 205.46s\n",
      "Epoch 3/5, Loss: 0.6962872346242269, Time: 205.06s\n",
      "Epoch 4/5, Loss: 0.6830102602640787, Time: 205.52s\n",
      "Epoch 5/5, Loss: 0.6973384817441305, Time: 205.00s\n",
      "Total Training Time: 1027.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 12:01:58,968] Trial 5 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 5.345235488360063e-05, 'dropout_rate': 0.16607426917231832}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.69s\n",
      "Epoch 1/5, Loss: 0.8741228699684143, Time: 199.69s\n",
      "Epoch 2/5, Loss: 0.7158250570297241, Time: 198.81s\n",
      "Epoch 3/5, Loss: 0.7003739953041077, Time: 200.52s\n",
      "Epoch 4/5, Loss: 0.6879575967788696, Time: 198.80s\n",
      "Epoch 5/5, Loss: 0.7203323006629944, Time: 199.33s\n",
      "Total Training Time: 997.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 12:18:51,450] Trial 6 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 6.403628387660492e-05, 'dropout_rate': 0.1620680100232344}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.69s\n",
      "Epoch 1/5, Loss: 0.7190415461858114, Time: 205.26s\n",
      "Epoch 2/5, Loss: 0.7090791463851929, Time: 205.42s\n",
      "Epoch 3/5, Loss: 0.7228625814119974, Time: 205.11s\n",
      "Epoch 4/5, Loss: 0.6798882285753886, Time: 205.19s\n",
      "Epoch 5/5, Loss: 0.7156826257705688, Time: 204.64s\n",
      "Total Training Time: 1025.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 12:36:12,367] Trial 7 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.0002760435489246661, 'dropout_rate': 0.18976868622477316}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.68s\n",
      "Epoch 1/5, Loss: 0.7654791275660197, Time: 206.58s\n",
      "Epoch 2/5, Loss: 0.7141641775767008, Time: 205.31s\n",
      "Epoch 3/5, Loss: 0.6947280168533325, Time: 206.03s\n",
      "Epoch 4/5, Loss: 0.692451000213623, Time: 212.50s\n",
      "Epoch 5/5, Loss: 0.6840793490409851, Time: 209.10s\n",
      "Total Training Time: 1039.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 12:53:47,638] Trial 8 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.3895994627989614e-05, 'dropout_rate': 0.2335692743329812}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.12s\n",
      "Epoch 1/5, Loss: 0.7838709553082784, Time: 206.17s\n",
      "Epoch 2/5, Loss: 0.7584385474522909, Time: 205.07s\n",
      "Epoch 3/5, Loss: 0.6967947284380595, Time: 204.66s\n",
      "Epoch 4/5, Loss: 0.7047132651011149, Time: 205.08s\n",
      "Epoch 5/5, Loss: 0.676358699798584, Time: 206.14s\n",
      "Total Training Time: 1027.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 13:11:10,760] Trial 9 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.00014005290868161893, 'dropout_rate': 0.30871518847676394}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.69s\n",
      "Best Parameters: {'batch_size': 32, 'learning_rate': 2.098253023730156e-05, 'dropout_rate': 0.19555086246733772}\n",
      "Epoch 1/5, Loss: 0.7070582509040833, Time: 197.87s\n",
      "Epoch 2/5, Loss: 0.7088041543960572, Time: 200.04s\n",
      "Epoch 3/5, Loss: 0.7335042834281922, Time: 199.59s\n",
      "Epoch 4/5, Loss: 0.7085669636726379, Time: 199.79s\n",
      "Early stopping at epoch 4\n",
      "Total Training Time: 797.28s\n",
      "Total Evaluation Time: 11.67s\n",
      "scBERT Test Accuracy: 60.00%\n",
      "Epoch 1/5, Loss: 1.1985112190246583, Time: 253.46s\n",
      "Epoch 2/5, Loss: 0.7165852546691894, Time: 251.75s\n",
      "Epoch 3/5, Loss: 0.7542230725288391, Time: 248.90s\n",
      "Epoch 4/5, Loss: 0.7002256631851196, Time: 247.27s\n",
      "Epoch 5/5, Loss: 0.7284901022911072, Time: 248.42s\n",
      "Total Training Time: 1249.78s\n",
      "Total Evaluation Time: 17.04s\n",
      "scGPT Test Accuracy: 60.00%\n",
      "Classification completed and saved to GSE150949_scRNA_classified.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16  # Reduced batch size\n",
    "num_epochs = 5  # Reduced number of epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    return accuracy\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_model(scbert_model, optimizer, train_dataloader_bert, criterion, num_epochs=5)\n",
    "    accuracy = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Train and evaluate the models with the best hyperparameters\n",
    "scbert_optimizer = optim.Adam(scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "train_model(scbert_model, scbert_optimizer, train_dataloader_bert, criterion, num_epochs)\n",
    "scbert_accuracy = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "print(f'scBERT Test Accuracy: {scbert_accuracy * 100:.2f}%')\n",
    "\n",
    "scgpt_optimizer = optim.Adam(scgpt_model.parameters(), lr=learning_rate)\n",
    "train_model(scgpt_model, scgpt_optimizer, train_dataloader_gpt, criterion, num_epochs)\n",
    "scgpt_accuracy = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "print(f'scGPT Test Accuracy: {scgpt_accuracy * 100:.2f}%')\n",
    "\n",
    "# Based on the best accuracy, classify the entire dataset\n",
    "if scbert_accuracy > scgpt_accuracy:\n",
    "    best_model = scbert_model\n",
    "    best_tokenizer = bert_tokenizer\n",
    "else:\n",
    "    best_model = scgpt_model\n",
    "    best_tokenizer = gpt_tokenizer\n",
    "\n",
    "# Create a dataloader for the entire dataset\n",
    "full_dataset = RNASeqDataset(X_scaled, actual_labels, best_tokenizer, model_type='bert' if best_model == scbert_model else 'gpt')\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Classify the entire dataset\n",
    "def classify_dataset(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get predictions\n",
    "predictions = classify_dataset(best_model, full_dataloader)\n",
    "data['predicted_label'] = predictions\n",
    "\n",
    "# Save the classified dataset\n",
    "data['persister_label'] = actual_labels  # Add the actual labels back to the data\n",
    "data.to_csv('/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA_classified.csv', index=False)\n",
    "\n",
    "print('Classification completed and saved to GSE150949_scRNA_classified.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77040c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "´02/06/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b6197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-02 15:54:40,990] A new study created in memory with name: no-name-f544af6d-538b-4225-8605-442b94ae1e4d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.6780230601628622, Time: 218.71s\n",
      "Epoch 2/3, Loss: 0.6905593673388163, Time: 209.72s\n",
      "Epoch 3/3, Loss: 0.7033189733823141, Time: 211.26s\n",
      "Total Training Time: 639.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:05:36,822] Trial 0 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.8963127214339294e-05, 'dropout_rate': 0.1776864212297564}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.50s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6987577875455221, Time: 208.31s\n",
      "Epoch 2/3, Loss: 0.7386711835861206, Time: 209.64s\n",
      "Epoch 3/3, Loss: 0.7096268534660339, Time: 208.14s\n",
      "Total Training Time: 626.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:16:18,258] Trial 1 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 5.488470193938184e-05, 'dropout_rate': 0.42130184772980217}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.72s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.70217231909434, Time: 207.83s\n",
      "Epoch 2/3, Loss: 0.7106437484423319, Time: 207.39s\n",
      "Epoch 3/3, Loss: 0.6777363220850626, Time: 207.69s\n",
      "Total Training Time: 622.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:26:56,432] Trial 2 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.303218334756295e-05, 'dropout_rate': 0.15382115938372817}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.69s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7346393267313639, Time: 199.22s\n",
      "Epoch 2/3, Loss: 0.6964345375696818, Time: 201.69s\n",
      "Epoch 3/3, Loss: 0.7583057085673014, Time: 199.21s\n",
      "Total Training Time: 600.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:37:11,121] Trial 3 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.0127508697564435e-05, 'dropout_rate': 0.47708033618956713}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.94s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7293882369995117, Time: 199.37s\n",
      "Epoch 2/3, Loss: 0.735450804233551, Time: 198.52s\n",
      "Epoch 3/3, Loss: 0.7473359704017639, Time: 201.01s\n",
      "Total Training Time: 598.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:47:24,587] Trial 4 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.4583251996602115e-05, 'dropout_rate': 0.3442859817851834}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.96s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7789265712102255, Time: 199.23s\n",
      "Epoch 2/3, Loss: 0.7214029232660929, Time: 200.19s\n",
      "Epoch 3/3, Loss: 0.7364567518234253, Time: 200.18s\n",
      "Total Training Time: 599.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 16:57:41,496] Trial 5 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 5.841198893993964e-05, 'dropout_rate': 0.3291469630481474}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.01s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7202614188194275, Time: 187.00s\n",
      "Epoch 2/3, Loss: 0.8547983884811401, Time: 185.33s\n",
      "Epoch 3/3, Loss: 0.7221110701560974, Time: 184.73s\n",
      "Total Training Time: 557.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-02 17:07:13,249] Trial 6 finished with value: 0.4 and parameters: {'batch_size': 16, 'learning_rate': 0.000862713350175145, 'dropout_rate': 0.25593902467449203}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.02s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.6983048915863037, Time: 201.74s\n",
      "Epoch 2/3, Loss: 0.7184461951255798, Time: 198.30s\n",
      "Epoch 3/3, Loss: 0.7039341330528259, Time: 206.14s\n",
      "Total Training Time: 606.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 17:17:36,919] Trial 7 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 5.3402766642799504e-05, 'dropout_rate': 0.3932810915551864}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.37s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.8009879231452942, Time: 187.90s\n",
      "Epoch 2/3, Loss: 0.7198788166046143, Time: 185.66s\n",
      "Epoch 3/3, Loss: 0.6847321629524231, Time: 186.62s\n",
      "Total Training Time: 560.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 17:27:12,706] Trial 8 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 1.449005987349287e-05, 'dropout_rate': 0.4444108593916093}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.01s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7537136077880859, Time: 196.75s\n",
      "Epoch 2/3, Loss: 0.7983022729555765, Time: 198.78s\n",
      "Epoch 3/3, Loss: 0.7587332924207052, Time: 195.67s\n",
      "Total Training Time: 591.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 17:37:20,786] Trial 9 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.0002903787488948635, 'dropout_rate': 0.3443461500554773}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.99s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7988574504852295, Time: 186.80s\n",
      "Epoch 2/3, Loss: 0.7149532675743103, Time: 183.45s\n",
      "Epoch 3/3, Loss: 0.727476167678833, Time: 185.51s\n",
      "Total Training Time: 555.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 17:46:51,147] Trial 10 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.00018588779964394118, 'dropout_rate': 0.125892721738189}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.98s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7186281879742941, Time: 198.35s\n",
      "Epoch 2/3, Loss: 0.7152939438819885, Time: 196.71s\n",
      "Epoch 3/3, Loss: 0.7310625910758972, Time: 198.44s\n",
      "Total Training Time: 593.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 17:56:59,264] Trial 11 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.0001068435676095799, 'dropout_rate': 0.22081780856714772}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.01s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6890195608139038, Time: 199.41s\n",
      "Epoch 2/3, Loss: 0.7587713400522867, Time: 199.36s\n",
      "Epoch 3/3, Loss: 0.7292925914128622, Time: 198.52s\n",
      "Total Training Time: 597.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-02 18:07:11,303] Trial 12 finished with value: 0.4 and parameters: {'batch_size': 32, 'learning_rate': 2.2982433487900423e-05, 'dropout_rate': 0.20995601508782558}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.95s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.8025000691413879, Time: 199.02s\n",
      "Epoch 2/3, Loss: 0.7087345719337463, Time: 202.34s\n",
      "Epoch 3/3, Loss: 0.7082057396570841, Time: 200.38s\n",
      "Total Training Time: 601.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 18:17:27,576] Trial 13 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 6.021779112254828e-05, 'dropout_rate': 0.4083065663867058}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.92s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.8293628295262655, Time: 198.86s\n",
      "Epoch 2/3, Loss: 0.682129979133606, Time: 204.57s\n",
      "Epoch 3/3, Loss: 0.6906513770421346, Time: 201.14s\n",
      "Total Training Time: 604.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 18:27:46,873] Trial 14 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.42734524910798e-05, 'dropout_rate': 0.2759039831008053}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.12s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7036460041999817, Time: 186.43s\n",
      "Epoch 2/3, Loss: 0.7658825397491456, Time: 186.66s\n",
      "Epoch 3/3, Loss: 0.6866638422012329, Time: 188.30s\n",
      "Total Training Time: 561.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 18:37:23,292] Trial 15 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.0001205439860713912, 'dropout_rate': 0.17147119402704455}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.00s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6978557308514913, Time: 198.68s\n",
      "Epoch 2/3, Loss: 0.7352397044499716, Time: 201.14s\n",
      "Epoch 3/3, Loss: 0.7074801127115885, Time: 198.45s\n",
      "Total Training Time: 598.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 18:47:36,234] Trial 16 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.572766744104568e-05, 'dropout_rate': 0.10928264087808867}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.97s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7576595346132914, Time: 198.72s\n",
      "Epoch 2/3, Loss: 0.7842166423797607, Time: 201.07s\n",
      "Epoch 3/3, Loss: 0.7398517529169718, Time: 199.40s\n",
      "Total Training Time: 599.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 18:57:50,074] Trial 17 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.00038390197175170837, 'dropout_rate': 0.4978866575936191}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.06s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.765293276309967, Time: 188.14s\n",
      "Epoch 2/3, Loss: 0.7261101484298706, Time: 188.24s\n",
      "Epoch 3/3, Loss: 0.7108487010002136, Time: 186.90s\n",
      "Total Training Time: 563.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 19:07:27,919] Trial 18 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 4.269425696292457e-05, 'dropout_rate': 0.391581084816316}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.97s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7436188062032064, Time: 197.42s\n",
      "Epoch 2/3, Loss: 0.7019838492075602, Time: 196.13s\n",
      "Epoch 3/3, Loss: 0.7225064039230347, Time: 197.42s\n",
      "Total Training Time: 590.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 19:17:33,458] Trial 19 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 7.709929351599386e-05, 'dropout_rate': 0.21635759857365777}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.95s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7517266472180685, Time: 199.09s\n",
      "Epoch 2/3, Loss: 0.726882537206014, Time: 201.08s\n",
      "Epoch 3/3, Loss: 0.6961719989776611, Time: 199.33s\n",
      "Total Training Time: 599.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-02 19:27:47,557] Trial 20 finished with value: 0.4 and parameters: {'batch_size': 32, 'learning_rate': 2.1118254026193032e-05, 'dropout_rate': 0.2936073071180133}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.02s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.7186567783355713, Time: 199.84s\n",
      "Epoch 2/3, Loss: 0.7235923409461975, Time: 205.24s\n",
      "Epoch 3/3, Loss: 0.7187032500902811, Time: 201.13s\n",
      "Total Training Time: 606.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 19:38:08,467] Trial 21 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.289985233675352e-05, 'dropout_rate': 0.16106600322438408}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.06s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.735877255598704, Time: 199.18s\n",
      "Epoch 2/3, Loss: 0.6784356832504272, Time: 200.49s\n",
      "Epoch 3/3, Loss: 0.7201308409372965, Time: 197.76s\n",
      "Total Training Time: 597.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 19:48:20,845] Trial 22 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.0183695912242028e-05, 'dropout_rate': 0.1592722856637663}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.00s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7513393759727478, Time: 198.87s\n",
      "Epoch 2/3, Loss: 0.7036523421605428, Time: 198.70s\n",
      "Epoch 3/3, Loss: 0.7002967596054077, Time: 198.14s\n",
      "Total Training Time: 595.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 19:58:31,238] Trial 23 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.171776769193733e-05, 'dropout_rate': 0.2395270918202026}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.94s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7008519967397054, Time: 199.01s\n",
      "Epoch 2/3, Loss: 0.7074482043584188, Time: 199.92s\n",
      "Epoch 3/3, Loss: 0.6965252757072449, Time: 199.99s\n",
      "Total Training Time: 598.92s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 20:08:44,688] Trial 24 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.668552378794278e-05, 'dropout_rate': 0.18612218040262074}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.97s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7588207523028055, Time: 199.07s\n",
      "Epoch 2/3, Loss: 0.7170271476109823, Time: 199.80s\n",
      "Epoch 3/3, Loss: 0.7246663769086202, Time: 199.74s\n",
      "Total Training Time: 598.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 20:18:57,876] Trial 25 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 8.41783904384156e-05, 'dropout_rate': 0.13904083091842243}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 10.98s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7641647060712179, Time: 199.93s\n",
      "Epoch 2/3, Loss: 0.7293467322985331, Time: 200.51s\n",
      "Epoch 3/3, Loss: 0.7164691090583801, Time: 199.97s\n",
      "Total Training Time: 600.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 20:29:13,017] Trial 26 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.9474189339080366e-05, 'dropout_rate': 0.10365509147368349}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.13s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6896176218986512, Time: 186.91s\n",
      "Epoch 2/3, Loss: 0.791819941997528, Time: 185.85s\n",
      "Epoch 3/3, Loss: 0.7040169596672058, Time: 186.49s\n",
      "Total Training Time: 559.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 20:38:46,962] Trial 27 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.00018020541246474735, 'dropout_rate': 0.1877795398878549}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.03s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.8308999141057333, Time: 210.70s\n",
      "Epoch 2/3, Loss: 0.7734154860178629, Time: 211.70s\n",
      "Epoch 3/3, Loss: 0.7818954785664877, Time: 208.26s\n",
      "Total Training Time: 630.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 20:49:35,313] Trial 28 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.6739592273235555e-05, 'dropout_rate': 0.3107424928476582}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.96s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6449164748191833, Time: 208.26s\n",
      "Epoch 2/3, Loss: 0.7140778501828512, Time: 209.59s\n",
      "Epoch 3/3, Loss: 0.7631835540135702, Time: 208.62s\n",
      "Total Training Time: 626.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:00:20,004] Trial 29 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 4.5836778258557655e-05, 'dropout_rate': 0.4536538187616566}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.76s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.675674041112264, Time: 209.26s\n",
      "Epoch 2/3, Loss: 0.761301060517629, Time: 210.26s\n",
      "Epoch 3/3, Loss: 0.6997840404510498, Time: 208.64s\n",
      "Total Training Time: 628.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:11:03,454] Trial 30 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.256459225869341e-05, 'dropout_rate': 0.37402420279287}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.70s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7297466397285461, Time: 208.50s\n",
      "Epoch 2/3, Loss: 0.738485316435496, Time: 209.59s\n",
      "Epoch 3/3, Loss: 0.7557408809661865, Time: 208.33s\n",
      "Total Training Time: 626.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:21:45,199] Trial 31 finished with value: 0.55 and parameters: {'batch_size': 32, 'learning_rate': 1.0210040555535325e-05, 'dropout_rate': 0.4908921124835622}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.67s\n",
      "Accuracy: 0.5500, F1 Score: 0.7097, Precision: 0.5789, Recall: 0.9167\n",
      "Epoch 1/3, Loss: 0.7661988337834676, Time: 209.03s\n",
      "Epoch 2/3, Loss: 0.6905753016471863, Time: 208.75s\n",
      "Epoch 3/3, Loss: 0.7472078998883566, Time: 208.83s\n",
      "Total Training Time: 626.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:32:27,158] Trial 32 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.906418098743823e-05, 'dropout_rate': 0.4541572502965262}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.70s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.799682100613912, Time: 209.83s\n",
      "Epoch 2/3, Loss: 0.714357594648997, Time: 208.74s\n",
      "Epoch 3/3, Loss: 0.7020119825998942, Time: 209.06s\n",
      "Total Training Time: 627.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:43:10,147] Trial 33 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.578264651593899e-05, 'dropout_rate': 0.42589860157697174}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.76s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7158677379290262, Time: 209.25s\n",
      "Epoch 2/3, Loss: 0.7859827280044556, Time: 208.35s\n",
      "Epoch 3/3, Loss: 0.7356532017389933, Time: 209.35s\n",
      "Total Training Time: 626.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 21:53:52,400] Trial 34 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 6.17786859647773e-05, 'dropout_rate': 0.47557982078478045}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.70s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7221959829330444, Time: 210.65s\n",
      "Epoch 2/3, Loss: 0.7272068659464518, Time: 209.53s\n",
      "Epoch 3/3, Loss: 0.6980534195899963, Time: 208.05s\n",
      "Total Training Time: 628.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 22:04:35,932] Trial 35 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.3288226988707822e-05, 'dropout_rate': 0.3417334746214215}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.70s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7867297649383544, Time: 201.15s\n",
      "Epoch 2/3, Loss: 0.7513758897781372, Time: 201.20s\n",
      "Epoch 3/3, Loss: 0.764035415649414, Time: 199.27s\n",
      "Total Training Time: 601.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 22:14:53,063] Trial 36 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.0008996523847039539, 'dropout_rate': 0.36859550216733095}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.85s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7293193538983663, Time: 206.03s\n",
      "Epoch 2/3, Loss: 0.7136633992195129, Time: 206.98s\n",
      "Epoch 3/3, Loss: 0.7072166800498962, Time: 206.69s\n",
      "Total Training Time: 619.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 22:25:28,376] Trial 37 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.888986068305369e-05, 'dropout_rate': 0.4277482034006551}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.76s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7404240369796753, Time: 200.04s\n",
      "Epoch 2/3, Loss: 0.7131534099578858, Time: 197.82s\n",
      "Epoch 3/3, Loss: 0.7132641315460205, Time: 198.30s\n",
      "Total Training Time: 596.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 22:35:39,987] Trial 38 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 2.8682714745587966e-05, 'dropout_rate': 0.254383284708576}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.67s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.718638559182485, Time: 223.20s\n",
      "Epoch 2/3, Loss: 0.7038586735725403, Time: 213.10s\n",
      "Epoch 3/3, Loss: 0.7099942962328593, Time: 209.11s\n",
      "Total Training Time: 645.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 22:46:41,723] Trial 39 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.2482917885082042e-05, 'dropout_rate': 0.1327426035210855}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.68s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7806624372800192, Time: 208.21s\n",
      "Epoch 2/3, Loss: 0.9660837451616923, Time: 209.11s\n",
      "Epoch 3/3, Loss: 0.8069780071576437, Time: 207.60s\n",
      "Total Training Time: 624.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-02 22:57:25,512] Trial 40 finished with value: 0.4 and parameters: {'batch_size': 32, 'learning_rate': 0.0005721761462474053, 'dropout_rate': 0.46771592743627227}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.67s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.7251812815666199, Time: 208.62s\n",
      "Epoch 2/3, Loss: 0.6857437690099081, Time: 209.23s\n",
      "Epoch 3/3, Loss: 0.8009778062502543, Time: 209.61s\n",
      "Total Training Time: 627.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 23:08:08,306] Trial 41 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 5.257439327942294e-05, 'dropout_rate': 0.4280649311166451}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.73s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7740238308906555, Time: 208.49s\n",
      "Epoch 2/3, Loss: 0.7429904341697693, Time: 208.95s\n",
      "Epoch 3/3, Loss: 0.7024082541465759, Time: 207.12s\n",
      "Total Training Time: 624.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 23:18:48,583] Trial 42 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.112027877132872e-05, 'dropout_rate': 0.40105142740511385}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.69s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.717708170413971, Time: 208.56s\n",
      "Epoch 2/3, Loss: 0.6793968876202902, Time: 209.46s\n",
      "Epoch 3/3, Loss: 0.7118041117986044, Time: 208.43s\n",
      "Total Training Time: 626.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 23:29:30,355] Trial 43 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 1.5969697712454295e-05, 'dropout_rate': 0.3745184687076213}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.75s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7693594296773275, Time: 209.14s\n",
      "Epoch 2/3, Loss: 0.6955845952033997, Time: 211.13s\n",
      "Epoch 3/3, Loss: 0.7101577520370483, Time: 210.16s\n",
      "Total Training Time: 630.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 23:40:16,362] Trial 44 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 3.8813891732356314e-05, 'dropout_rate': 0.30533620544368517}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.74s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.8932833870251974, Time: 209.25s\n",
      "Epoch 2/3, Loss: 0.7134016752243042, Time: 208.78s\n",
      "Epoch 3/3, Loss: 0.7088051438331604, Time: 209.57s\n",
      "Total Training Time: 627.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-02 23:50:59,279] Trial 45 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.00013155192501685339, 'dropout_rate': 0.330144348159911}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.73s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7697397112846375, Time: 209.88s\n",
      "Epoch 2/3, Loss: 0.7378343939781189, Time: 195.23s\n",
      "Epoch 3/3, Loss: 0.7536140203475952, Time: 193.45s\n",
      "Total Training Time: 598.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-03 00:01:13,979] Trial 46 finished with value: 0.4 and parameters: {'batch_size': 16, 'learning_rate': 7.249077726730233e-05, 'dropout_rate': 0.35688882322312443}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.08s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.7973875602086385, Time: 209.04s\n",
      "Epoch 2/3, Loss: 0.7007066210110983, Time: 208.26s\n",
      "Epoch 3/3, Loss: 0.707445482412974, Time: 208.99s\n",
      "Total Training Time: 626.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 00:11:56,774] Trial 47 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 2.416623922900222e-05, 'dropout_rate': 0.2882603195640222}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.71s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7847066720326742, Time: 209.18s\n",
      "Epoch 2/3, Loss: 0.7196443875630697, Time: 208.49s\n",
      "Epoch 3/3, Loss: 0.7231016953786215, Time: 209.10s\n",
      "Total Training Time: 626.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 00:22:38,834] Trial 48 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 4.75848864219207e-05, 'dropout_rate': 0.19662721825039392}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.67s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7092327872912089, Time: 210.78s\n",
      "Epoch 2/3, Loss: 0.7518264452616373, Time: 208.16s\n",
      "Epoch 3/3, Loss: 0.6895715991655985, Time: 208.64s\n",
      "Total Training Time: 627.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 00:33:21,732] Trial 49 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 6.264783949620875e-05, 'dropout_rate': 0.4163041036611652}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.71s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Best hyperparameters: {'batch_size': 32, 'learning_rate': 1.8963127214339294e-05, 'dropout_rate': 0.1776864212297564}\n",
      "Epoch 1/5, Loss: 0.7181537946065267, Time: 207.93s\n",
      "Epoch 2/5, Loss: 0.734636922677358, Time: 209.49s\n",
      "Epoch 3/5, Loss: 0.7008355657259623, Time: 209.35s\n",
      "Epoch 4/5, Loss: 0.7021482586860657, Time: 209.70s\n",
      "Epoch 5/5, Loss: 0.6874314546585083, Time: 208.76s\n",
      "Total Training Time: 1045.23s\n",
      "Total Evaluation Time: 11.74s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EnsembleModel.forward() missing 1 required positional argument: 'inputs2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 270\u001b[0m\n\u001b[1;32m    267\u001b[0m ensemble_model \u001b[38;5;241m=\u001b[39m EnsembleModel(best_scbert_model, best_scgpt_model)\n\u001b[1;32m    268\u001b[0m ensemble_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(ensemble_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 270\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader_gpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m evaluate_model(ensemble_model, \u001b[38;5;28mzip\u001b[39m(test_dataloader_best, test_dataloader_gpt))\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Save the models\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 153\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_dataloader, criterion, num_epochs, patience)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# For GPT\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m--> 153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    155\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: EnsembleModel.forward() missing 1 required positional argument: 'inputs2'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs=3)  # Reduce epochs for tuning\n",
    "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Increase number of trials\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train and evaluate with best hyperparameters\n",
    "best_scbert_model = SCBERTClassifier()\n",
    "best_scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer = optim.Adam(best_scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_best = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_dataloader_best = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "train_model(best_scbert_model, best_optimizer, train_dataloader_best, criterion, num_epochs)\n",
    "evaluate_model(best_scbert_model, test_dataloader_best)\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.fc = nn.Linear(4, 2)  # Combining logits from two models\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        logits1 = self.model1(**inputs1)\n",
    "        logits2 = self.model2(inputs2)\n",
    "        combined_logits = torch.cat((logits1, logits2), dim=1)\n",
    "        return self.fc(combined_logits)\n",
    "\n",
    "# Instantiate and train ensemble model\n",
    "best_scgpt_model = SCGPTClassifier()\n",
    "best_scgpt_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer_gpt = optim.Adam(best_scgpt_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "ensemble_model = EnsembleModel(best_scbert_model, best_scgpt_model)\n",
    "ensemble_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_model(ensemble_model, ensemble_optimizer, zip(train_dataloader_best, train_dataloader_gpt), criterion, num_epochs)\n",
    "evaluate_model(ensemble_model, zip(test_dataloader_best, test_dataloader_gpt))\n",
    "\n",
    "# Save the models\n",
    "torch.save(best_scbert_model.state_dict(), 'best_scbert_model.pth')\n",
    "torch.save(best_scgpt_model.state_dict(), 'best_scgpt_model.pth')\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533de8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "03/06/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-04 13:47:08,493] A new study created in memory with name: no-name-b8030d7f-91c5-492b-b9f1-b0b3796a3313\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7010463873545328, Time: 225.95s\n",
      "Epoch 2/3, Loss: 0.7061854799588522, Time: 206.63s\n",
      "Epoch 3/3, Loss: 0.6527900894482931, Time: 202.96s\n",
      "Total Training Time: 635.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 13:58:08,812] Trial 0 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 8.782272962051413e-05, 'dropout_rate': 0.441188394919498}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.23s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7641678849856058, Time: 207.05s\n",
      "Epoch 2/3, Loss: 0.8038041392962137, Time: 199.93s\n",
      "Epoch 3/3, Loss: 0.7548534870147705, Time: 203.89s\n",
      "Total Training Time: 610.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:08:36,808] Trial 1 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.0005209071139540421, 'dropout_rate': 0.32882505099643244}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.05s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7437606851259867, Time: 217.27s\n",
      "Epoch 2/3, Loss: 0.7165540655454, Time: 206.84s\n",
      "Epoch 3/3, Loss: 0.7097341219584147, Time: 200.56s\n",
      "Total Training Time: 624.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:19:17,441] Trial 2 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 6.376323347192863e-05, 'dropout_rate': 0.11293828588690565}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.12s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.6624381343523661, Time: 207.61s\n",
      "Epoch 2/3, Loss: 0.7225381135940552, Time: 200.55s\n",
      "Epoch 3/3, Loss: 0.729839007059733, Time: 202.79s\n",
      "Total Training Time: 610.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:29:47,329] Trial 3 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 8.925534817242588e-05, 'dropout_rate': 0.370963610795752}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.05s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7582910299301148, Time: 186.95s\n",
      "Epoch 2/3, Loss: 0.7165068507194519, Time: 177.35s\n",
      "Epoch 3/3, Loss: 0.7481189370155334, Time: 178.78s\n",
      "Total Training Time: 543.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.19s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:39:05,334] Trial 4 finished with value: 0.4 and parameters: {'batch_size': 16, 'learning_rate': 0.0003400122231910321, 'dropout_rate': 0.14478625885724875}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7639226198196412, Time: 185.75s\n",
      "Epoch 2/3, Loss: 0.7270947337150574, Time: 178.50s\n",
      "Epoch 3/3, Loss: 0.701966392993927, Time: 178.66s\n",
      "Total Training Time: 542.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:48:23,362] Trial 5 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 1.5781728619168713e-05, 'dropout_rate': 0.3927669632104295}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 11.09s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.8313389619191488, Time: 203.81s\n",
      "Epoch 2/3, Loss: 0.7869373758633932, Time: 228.37s\n",
      "Epoch 3/3, Loss: 0.8699963092803955, Time: 206.80s\n",
      "Total Training Time: 638.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 14:59:20,439] Trial 6 finished with value: 0.6 and parameters: {'batch_size': 32, 'learning_rate': 0.0006036824085327791, 'dropout_rate': 0.4367715100594728}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.38s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.7481949329376221, Time: 199.69s\n",
      "Epoch 2/3, Loss: 0.7169508814811707, Time: 197.64s\n",
      "Epoch 3/3, Loss: 0.6947306275367737, Time: 200.74s\n",
      "Total Training Time: 598.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-04 15:09:35,265] Trial 7 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 2.57978758279113e-05, 'dropout_rate': 0.13874409649615696}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.32s\n",
      "Accuracy: 0.6000, F1 Score: 0.7500, Precision: 0.6000, Recall: 1.0000\n",
      "Epoch 1/3, Loss: 0.9380324482917786, Time: 221.56s\n",
      "Epoch 2/3, Loss: 0.6679794589678446, Time: 206.62s\n",
      "Epoch 3/3, Loss: 0.6652452150980631, Time: 205.09s\n",
      "Total Training Time: 633.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-04 15:20:24,772] Trial 8 finished with value: 0.4 and parameters: {'batch_size': 32, 'learning_rate': 0.00029522730851772353, 'dropout_rate': 0.36382286656284335}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Time: 12.62s\n",
      "Accuracy: 0.4000, F1 Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch 1/3, Loss: 0.7401591658592224, Time: 200.25s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 1#16\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            if isinstance(inputs, dict):  # For BERT\n",
    "                input_ids = inputs['input_ids']\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:  # For GPT\n",
    "                input_ids = inputs\n",
    "                outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs=3)  # Reduce epochs for tuning\n",
    "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Increase number of trials\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train and evaluate with best hyperparameters\n",
    "best_scbert_model = SCBERTClassifier()\n",
    "best_scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer = optim.Adam(best_scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_best = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_dataloader_best = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "train_model(best_scbert_model, best_optimizer, train_dataloader_best, criterion, num_epochs)\n",
    "evaluate_model(best_scbert_model, test_dataloader_best)\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.fc = nn.Linear(4, 2)  # Combining logits from two models\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        logits1 = self.model1(**inputs1)\n",
    "        logits2 = self.model2(inputs2)\n",
    "        combined_logits = torch.cat((logits1, logits2), dim=1)\n",
    "        return self.fc(combined_logits)\n",
    "\n",
    "# Instantiate and train ensemble model\n",
    "best_scgpt_model = SCGPTClassifier()\n",
    "best_scgpt_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer_gpt = optim.Adam(best_scgpt_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "ensemble_model = EnsembleModel(best_scbert_model, best_scgpt_model)\n",
    "ensemble_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_model(ensemble_model, ensemble_optimizer, zip(train_dataloader_best, train_dataloader_gpt), criterion, num_epochs)\n",
    "evaluate_model(ensemble_model, zip(test_dataloader_best, test_dataloader_gpt))\n",
    "\n",
    "# Save the models\n",
    "torch.save(best_scbert_model.state_dict(), 'best_scbert_model.pth')\n",
    "torch.save(best_scgpt_model.state_dict(), 'best_scgpt_model.pth')\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02176ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "04.06.2024 Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for (inputs1, labels1), (inputs2, labels2) in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ensure labels are the same\n",
    "            assert torch.equal(labels1, labels2), \"Labels must be the same for both models\"\n",
    "            \n",
    "            outputs = model(inputs1, inputs2)\n",
    "            loss = criterion(outputs, labels1)  # or labels2 since they are the same\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs1, labels1), (inputs2, labels2) in test_dataloader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Ensure labels are the same\n",
    "            assert torch.equal(labels1, labels2), \"Labels must be the same for both models\"\n",
    "            \n",
    "            outputs = model(inputs1, inputs2)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels1.size(0)\n",
    "            correct += (predicted == labels1).sum().item()\n",
    "            end_time = time.time()\n",
    "            total_evaluation_time += (end_time - start_time)\n",
    "\n",
    "            all_labels.extend(labels1.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f}s')\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs=3)  # Reduce epochs for tuning\n",
    "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Increase number of trials\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train and evaluate with best hyperparameters\n",
    "best_scbert_model = SCBERTClassifier()\n",
    "best_scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer = optim.Adam(best_scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_best = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_dataloader_best = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "train_model(best_scbert_model, best_optimizer, train_dataloader_best, criterion, num_epochs)\n",
    "evaluate_model(best_scbert_model, test_dataloader_best)\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.fc = nn.Linear(model1.fc.out_features + model2.fc.out_features, 2)  # Adjusted the input size\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        logits1 = self.model1(**inputs1)\n",
    "        logits2 = self.model2(inputs2)\n",
    "        combined_logits = torch.cat((logits1, logits2), dim=1)\n",
    "        return self.fc(combined_logits)\n",
    "\n",
    "# Instantiate and train ensemble model\n",
    "best_scgpt_model = SCGPTClassifier()\n",
    "best_scgpt_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer_gpt = optim.Adam(best_scgpt_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "ensemble_model = EnsembleModel(best_scbert_model, best_scgpt_model)\n",
    "ensemble_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_model(ensemble_model, ensemble_optimizer, zip(train_dataloader_best, train_dataloader_gpt), criterion, num_epochs)\n",
    "evaluate_model(ensemble_model, zip(test_dataloader_best, test_dataloader_gpt))\n",
    "\n",
    "# Save the models\n",
    "torch.save(best_scbert_model.state_dict(), 'best_scbert_model.pth')\n",
    "torch.save(best_scgpt_model.state_dict(), 'best_scgpt_model.pth')\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c14c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "05-06-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc0792e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-05 00:49:18,935] A new study created in memory with name: no-name-668d32a6-c86c-40ad-9e0f-98718631555c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.7687811454137167, Time: 218.52s\n",
      "Total Training Time: 218.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-05 00:53:01,649] Trial 0 failed with parameters: {'batch_size': 32, 'learning_rate': 1.0114738546174063e-05, 'dropout_rate': 0.3057514067490589} because of the following error: TypeError(\"unhashable type: 'slice'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/run/nvme/job_21849910/tmp/ipykernel_3973949/552733189.py\", line 248, in objective\n",
      "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader)\n",
      "  File \"/run/nvme/job_21849910/tmp/ipykernel_3973949/552733189.py\", line 204, in evaluate_model\n",
      "    outputs = model(inputs1, inputs2) if isinstance(inputs, tuple) else model(inputs1)  # Model prediction\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/run/nvme/job_21849910/tmp/ipykernel_3973949/552733189.py\", line 86, in forward\n",
      "    outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1052, in forward\n",
      "    self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "  File \"/users/barmanjy/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4457, in warn_if_padding_and_no_attention_mask\n",
      "    if self.config.pad_token_id in input_ids[:, [-1, 0]]:\n",
      "TypeError: unhashable type: 'slice'\n",
      "[W 2024-06-05 00:53:01,650] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 253\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m    252\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduce number of trials for quick testing\u001b[39;00m\n\u001b[1;32m    255\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[27], line 248\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    243\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset_bert, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    245\u001b[0m train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Pass device here\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m accuracy, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[0;32mIn[27], line 204\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dataloader)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     inputs1 \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m--> 204\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs1, inputs2) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Model prediction\u001b[39;00m\n\u001b[1;32m    205\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get predicted labels\u001b[39;00m\n\u001b[1;32m    207\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 86\u001b[0m, in \u001b[0;36mSCBERTClassifier.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 86\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m     88\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1052\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4457\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   4456\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 4457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m   4458\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4462\u001b[0m     )\n\u001b[1;32m   4464\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   4465\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label  # Return a single tuple\n",
    "\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, device, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack batch and ensure inputs are properly formatted\n",
    "            inputs, labels = batch\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_evaluation_time = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            if not batch or not batch[0]:  # Check if batch or inputs are empty\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch  # Unpack batch\n",
    "\n",
    "            if isinstance(inputs, tuple):\n",
    "                inputs1 = inputs[0]  # Get inputs for first model\n",
    "                inputs2 = inputs[1]  # Get inputs for second model\n",
    "            else:\n",
    "                inputs1 = inputs\n",
    "\n",
    "            outputs = model(inputs1, inputs2) if isinstance(inputs, tuple) else model(inputs1)  # Model prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted labels\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    if total == 0:\n",
    "        print(\"No samples in the dataset. Skipping evaluation.\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # Define device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs=1, device='cpu')  # Pass device here\n",
    "\n",
    "    \n",
    "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)  # Reduce number of trials for quick testing\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train and evaluate with best hyperparameters\n",
    "best_scbert_model = SCBERTClassifier()\n",
    "best_scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer = optim.Adam(best_scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_best = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_dataloader_best = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "train_model(best_scbert_model, best_optimizer, train_dataloader_best, criterion, num_epochs)\n",
    "evaluate_model(best_scbert_model, test_dataloader_best)\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.fc = nn.Linear(model1.fc.out_features + model2.fc.out_features, 2)  # Adjusted the input size\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        logits1 = self.model1(**inputs1)\n",
    "        logits2 = self.model2(inputs2)\n",
    "        combined_logits = torch.cat((logits1, logits2), dim=1)\n",
    "        return self.fc(combined_logits)\n",
    "\n",
    "# Instantiate and train ensemble model\n",
    "best_scgpt_model = SCGPTClassifier()\n",
    "best_scgpt_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer_gpt = optim.Adam(best_scgpt_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "ensemble_model = EnsembleModel(best_scbert_model, best_scgpt_model)\n",
    "ensemble_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_model(ensemble_model, ensemble_optimizer, zip(train_dataloader_best, train_dataloader_gpt), criterion, num_epochs)\n",
    "evaluate_model(ensemble_model, zip(test_dataloader_best, test_dataloader_gpt))\n",
    "\n",
    "# Save the models\n",
    "torch.save(best_scbert_model.state_dict(), 'best_scbert_model.pth')\n",
    "torch.save(best_scgpt_model.state_dict(), 'best_scgpt_model.pth')\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29942024",
   "metadata": {},
   "outputs": [],
   "source": [
    "05/06/2023 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec931e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/patsy/util.py:672: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return _pandas_is_categorical_dtype(dt)\n",
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_combat.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n",
      "[I 2024-06-05 06:25:09,811] A new study created in memory with name: no-name-cdae0e37-26b2-4cf1-9aa4-3d768ce5c99a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.7439374446868896, Time: 211.56s\n",
      "Total Training Time: 211.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-06-05 06:28:58,038] Trial 0 finished with value: 0.6 and parameters: {'batch_size': 16, 'learning_rate': 0.0004757005425973431, 'dropout_rate': 0.4097687427988205}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6000, F1 Score: 0.3750, Precision: 0.3000, Recall: 0.5000\n",
      "Best hyperparameters: {'batch_size': 16, 'learning_rate': 0.0004757005425973431, 'dropout_rate': 0.4097687427988205}\n",
      "Epoch 1/5, Loss: 0.7737460374832154, Time: 213.84s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path, nrows=100)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "# Extract actual labels\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label  # Return a single tuple\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Add a padding token to the GPT2 tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs, device, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack batch and ensure inputs are properly formatted\n",
    "            inputs, labels = batch\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            if not batch or not batch[0]:  # Check if batch or inputs are empty\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch  # Unpack batch\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)  # Model prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted labels\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    if total == 0:\n",
    "        print(\"No samples in the dataset. Skipping evaluation.\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # Define device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer = optim.Adam(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    train_model(scbert_model, optimizer, train_dataloader, criterion, num_epochs=1, device=device)  # Pass device here\n",
    "\n",
    "    \n",
    "    accuracy, _, _, _ = evaluate_model(scbert_model, test_dataloader, device)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)  # Reduce number of trials for quick testing\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train and evaluate with best hyperparameters\n",
    "best_scbert_model = SCBERTClassifier()\n",
    "best_scbert_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer = optim.Adam(best_scbert_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataloader_best = DataLoader(train_dataset_bert, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_dataloader_best = DataLoader(test_dataset_bert, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "train_model(best_scbert_model, best_optimizer, train_dataloader_best, criterion, num_epochs, device='cpu')\n",
    "evaluate_model(best_scbert_model, test_dataloader_best, device='cpu')\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.fc = nn.Linear(model1.fc.out_features + model2.fc.out_features, 2)  # Adjusted the input size\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        logits1 = self.model1(inputs1['input_ids'].to(device), attention_mask=inputs1['attention_mask'].to(device))\n",
    "        logits2 = self.model2(inputs2.to(device))\n",
    "        combined_logits = torch.cat((logits1, logits2), dim=1)\n",
    "        return self.fc(combined_logits)\n",
    "\n",
    "# Instantiate and train ensemble model\n",
    "best_scgpt_model = SCGPTClassifier()\n",
    "best_scgpt_model.dropout = nn.Dropout(best_params['dropout_rate'])\n",
    "best_optimizer_gpt = optim.Adam(best_scgpt_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "ensemble_model = EnsembleModel(best_scbert_model, best_scgpt_model)\n",
    "ensemble_optimizer = optim.Adam(ensemble_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_ensemble_dataloader = list(zip(train_dataloader_best, train_dataloader_gpt))\n",
    "test_ensemble_dataloader = list(zip(test_dataloader_best, test_dataloader_gpt))\n",
    "\n",
    "def train_ensemble_model(model, optimizer, train_dataloader, criterion, num_epochs, device, patience=3):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    total_training_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for batch_bert, batch_gpt in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack batches and ensure inputs are properly formatted\n",
    "            inputs_bert, labels_bert = batch_bert\n",
    "            inputs_gpt, labels_gpt = batch_gpt\n",
    "            input_ids_bert = inputs_bert['input_ids'].to(device)\n",
    "            attention_mask_bert = inputs_bert['attention_mask'].to(device)\n",
    "            input_ids_gpt = inputs_gpt.to(device)\n",
    "            labels = labels_bert.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model({'input_ids': input_ids_bert, 'attention_mask': attention_mask_bert}, input_ids_gpt)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_training_time += epoch_time\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "train_ensemble_model(ensemble_model, ensemble_optimizer, train_ensemble_dataloader, criterion, num_epochs, device='cpu')\n",
    "evaluate_model(ensemble_model, test_ensemble_dataloader, device='cpu')\n",
    "\n",
    "# Save the models\n",
    "torch.save(best_scbert_model.state_dict(), 'best_scbert_model.pth')\n",
    "torch.save(best_scgpt_model.state_dict(), 'best_scgpt_model.pth')\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bfd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
