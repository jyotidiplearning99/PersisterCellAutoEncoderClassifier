{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:233: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path,engine='python',encoding='utf-8')\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_data)\n",
    "\n",
    "# Construct k-NN graph\n",
    "def construct_knn_graph(X, k):\n",
    "    knn_graph = kneighbors_graph(X, k, mode='connectivity', include_self=False)\n",
    "    coo_graph = knn_graph.tocoo()  # Convert to COO format\n",
    "    G = nx.from_edgelist(zip(coo_graph.row, coo_graph.col))  # Create graph from COO format\n",
    "    return G\n",
    "\n",
    "# Calculate Ollivier-Ricci curvature\n",
    "def compute_ricci_curvature(G, alpha=0.5):\n",
    "    orc = OllivierRicci(G, alpha=alpha, verbose=\"INFO\")\n",
    "    orc.compute_ricci_curvature()\n",
    "    return nx.get_edge_attributes(orc.G, \"ricciCurvature\")\n",
    "\n",
    "# Identify high proliferation index cells with alternative curvature aggregation methods\n",
    "def identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=None, top_percentile=None, centrality_measure=None):\n",
    "    if method == 'threshold':\n",
    "        high_proliferation_edges = [(u, v) for (u, v), curvature in ricci_curvatures.items() if curvature > threshold]\n",
    "    elif method == 'top_percentile':\n",
    "        sorted_curvatures = sorted(ricci_curvatures.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_curvatures) * top_percentile / 100)\n",
    "        high_proliferation_edges = [(u, v) for (u, v), _ in sorted_curvatures[:top_n]]\n",
    "    elif method == 'centrality':\n",
    "        centrality_scores = nx.betweenness_centrality(G, weight='ricciCurvature')\n",
    "        sorted_centrality = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_centrality) * top_percentile / 100)\n",
    "        high_proliferation_nodes = [node for node, _ in sorted_centrality[:top_n]]\n",
    "        high_proliferation_edges = [(u, v) for (u, v) in G.edges() if u in high_proliferation_nodes or v in high_proliferation_nodes]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'threshold', 'top_percentile', or 'centrality'.\")\n",
    "    \n",
    "    high_proliferation_nodes = set([node for edge in high_proliferation_edges for node in edge])\n",
    "    return high_proliferation_nodes\n",
    "\n",
    "# Differential Expression Analysis\n",
    "def differential_expression_analysis(data, high_proliferation_nodes):\n",
    "    if 'persister_label' in data.columns:\n",
    "        persister_labels = data['persister_label'].values\n",
    "        high_proliferation_labels = np.zeros_like(persister_labels)\n",
    "        high_proliferation_labels[list(high_proliferation_nodes)] = 1\n",
    "\n",
    "        # Perform differential expression analysis\n",
    "        differential_genes = []\n",
    "        for gene_index in range(data.shape[1]):\n",
    "            gene_expr_high_prolif = data[high_proliferation_labels == 1, gene_index]\n",
    "            gene_expr_non_high_prolif = data[high_proliferation_labels == 0, gene_index]\n",
    "            _, p_value = ttest_ind(gene_expr_high_prolif, gene_expr_non_high_prolif)\n",
    "            if p_value < 0.05:  # Adjust significance level as needed\n",
    "                differential_genes.append(gene_index)\n",
    "        return differential_genes\n",
    "    else:\n",
    "        print(\"Ground truth labels ('persister_label') not available. Skipping differential expression analysis.\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "k_values = [5, 10, 15, 20]\n",
    "threshold = 0.5  # Threshold for identifying high proliferation cells\n",
    "top_percentile = 5  # Top percentile for alternative aggregation methods\n",
    "centrality_percentile = 5  # Percentile of nodes for centrality-based aggregation\n",
    "\n",
    "# Experiment with different k values for kNN graph\n",
    "for k in k_values:\n",
    "    G = construct_knn_graph(X_pca, k)\n",
    "    ricci_curvatures = compute_ricci_curvature(G)\n",
    "    \n",
    "    # Identify high proliferation cells using thresholding method\n",
    "    high_proliferation_nodes_threshold = identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=threshold)\n",
    "    print(f\"High Proliferation Cells with threshold method and k={k}:\", high_proliferation_nodes_threshold)\n",
    "    \n",
    "    # Identify high proliferation cells using top percentile method\n",
    "    high_proliferation_nodes_percentile = identify_high_proliferation_nodes(ricci_curvatures, method='top_percentile', top_percentile=top_percentile)\n",
    "    print(f\"High Proliferation Cells with top percentile method and k={k}:\", high_proliferation_nodes_percentile)\n",
    "    \n",
    "    # Identify high proliferation cells using centrality method\n",
    "    high_proliferation_nodes_centrality = identify_high_proliferation_nodes(ricci_curvatures, method='centrality', top_percentile=centrality_percentile)\n",
    "    print(f\"High Proliferation Cells with centrality method and k={k}:\", high_proliferation_nodes_centrality)\n",
    "\n",
    "    # Differential expression analysis\n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_threshold)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with threshold method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_percentile)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with top percentile method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_centrality)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with centrality method:\", differential_genes)\n",
    "\n",
    "    # Visualize curvature distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(list(ricci_curvatures.values()), bins=50, kde=True)\n",
    "    plt.axvline(np.mean(list(ricci_curvatures.values())), color='r', linestyle='--')\n",
    "    plt.title(f'Distribution of Ricci Curvatures with k={k}')\n",
    "    plt.xlabel('Ricci Curvature')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize high proliferation nodes on the PCA plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='gray', alpha=0.5, label='All Cells')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_threshold), 0], X_pca[list(high_proliferation_nodes_threshold), 1], c='red', label='High Proliferation Cells (Threshold)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_percentile), 0], X_pca[list(high_proliferation_nodes_percentile), 1], c='blue', label='High Proliferation Cells (Percentile)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_centrality), 0], X_pca[list(high_proliferation_nodes_centrality), 1], c='green', label='High Proliferation Cells (Centrality)')\n",
    "    plt.title(f'PCA of scRNA-seq Data with k={k}')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)[0]\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size)\n",
    "\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for scBERT model\n",
    "for epoch in range(num_epochs):\n",
    "    scbert_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_bert:\n",
    "        optimizer_bert.zero_grad()\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_bert)}\")\n",
    "\n",
    "# Evaluation loop for scBERT model\n",
    "scbert_model.eval()\n",
    "correct_bert = 0\n",
    "total_bert = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_bert:\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_bert += labels.size(0)\n",
    "        correct_bert += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scBERT model on test set:', (correct_bert / total_bert))\n",
    "\n",
    "# Training loop for scGPT model\n",
    "for epoch in range(num_epochs):\n",
    "    scgpt_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_gpt:\n",
    "        optimizer_gpt.zero_grad()\n",
    "\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_gpt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_gpt)}\")\n",
    "\n",
    "# Evaluation loop for scGPT model\n",
    "scgpt_model.eval()\n",
    "correct_gpt = 0\n",
    "total_gpt = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_gpt:\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_gpt += labels.size(0)\n",
    "        correct_gpt += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scGPT model on test set:', (correct_gpt / total_gpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cd69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5318bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
