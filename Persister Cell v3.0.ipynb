{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/barmanjy/.local/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:233: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path,engine='python',encoding='utf-8')\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_data)\n",
    "\n",
    "# Construct k-NN graph\n",
    "def construct_knn_graph(X, k):\n",
    "    knn_graph = kneighbors_graph(X, k, mode='connectivity', include_self=False)\n",
    "    coo_graph = knn_graph.tocoo()  # Convert to COO format\n",
    "    G = nx.from_edgelist(zip(coo_graph.row, coo_graph.col))  # Create graph from COO format\n",
    "    return G\n",
    "\n",
    "# Calculate Ollivier-Ricci curvature\n",
    "def compute_ricci_curvature(G, alpha=0.5):\n",
    "    orc = OllivierRicci(G, alpha=alpha, verbose=\"INFO\")\n",
    "    orc.compute_ricci_curvature()\n",
    "    return nx.get_edge_attributes(orc.G, \"ricciCurvature\")\n",
    "\n",
    "# Identify high proliferation index cells with alternative curvature aggregation methods\n",
    "def identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=None, top_percentile=None, centrality_measure=None):\n",
    "    if method == 'threshold':\n",
    "        high_proliferation_edges = [(u, v) for (u, v), curvature in ricci_curvatures.items() if curvature > threshold]\n",
    "    elif method == 'top_percentile':\n",
    "        sorted_curvatures = sorted(ricci_curvatures.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_curvatures) * top_percentile / 100)\n",
    "        high_proliferation_edges = [(u, v) for (u, v), _ in sorted_curvatures[:top_n]]\n",
    "    elif method == 'centrality':\n",
    "        centrality_scores = nx.betweenness_centrality(G, weight='ricciCurvature')\n",
    "        sorted_centrality = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_n = int(len(sorted_centrality) * top_percentile / 100)\n",
    "        high_proliferation_nodes = [node for node, _ in sorted_centrality[:top_n]]\n",
    "        high_proliferation_edges = [(u, v) for (u, v) in G.edges() if u in high_proliferation_nodes or v in high_proliferation_nodes]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'threshold', 'top_percentile', or 'centrality'.\")\n",
    "    \n",
    "    high_proliferation_nodes = set([node for edge in high_proliferation_edges for node in edge])\n",
    "    return high_proliferation_nodes\n",
    "\n",
    "# Differential Expression Analysis\n",
    "def differential_expression_analysis(data, high_proliferation_nodes):\n",
    "    if 'persister_label' in data.columns:\n",
    "        persister_labels = data['persister_label'].values\n",
    "        high_proliferation_labels = np.zeros_like(persister_labels)\n",
    "        high_proliferation_labels[list(high_proliferation_nodes)] = 1\n",
    "\n",
    "        # Perform differential expression analysis\n",
    "        differential_genes = []\n",
    "        for gene_index in range(data.shape[1]):\n",
    "            gene_expr_high_prolif = data[high_proliferation_labels == 1, gene_index]\n",
    "            gene_expr_non_high_prolif = data[high_proliferation_labels == 0, gene_index]\n",
    "            _, p_value = ttest_ind(gene_expr_high_prolif, gene_expr_non_high_prolif)\n",
    "            if p_value < 0.05:  # Adjust significance level as needed\n",
    "                differential_genes.append(gene_index)\n",
    "        return differential_genes\n",
    "    else:\n",
    "        print(\"Ground truth labels ('persister_label') not available. Skipping differential expression analysis.\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "k_values = [5, 10, 15, 20]\n",
    "threshold = 0.5  # Threshold for identifying high proliferation cells\n",
    "top_percentile = 5  # Top percentile for alternative aggregation methods\n",
    "centrality_percentile = 5  # Percentile of nodes for centrality-based aggregation\n",
    "\n",
    "# Experiment with different k values for kNN graph\n",
    "for k in k_values:\n",
    "    G = construct_knn_graph(X_pca, k)\n",
    "    ricci_curvatures = compute_ricci_curvature(G)\n",
    "    \n",
    "    # Identify high proliferation cells using thresholding method\n",
    "    high_proliferation_nodes_threshold = identify_high_proliferation_nodes(ricci_curvatures, method='threshold', threshold=threshold)\n",
    "    print(f\"High Proliferation Cells with threshold method and k={k}:\", high_proliferation_nodes_threshold)\n",
    "    \n",
    "    # Identify high proliferation cells using top percentile method\n",
    "    high_proliferation_nodes_percentile = identify_high_proliferation_nodes(ricci_curvatures, method='top_percentile', top_percentile=top_percentile)\n",
    "    print(f\"High Proliferation Cells with top percentile method and k={k}:\", high_proliferation_nodes_percentile)\n",
    "    \n",
    "    # Identify high proliferation cells using centrality method\n",
    "    high_proliferation_nodes_centrality = identify_high_proliferation_nodes(ricci_curvatures, method='centrality', top_percentile=centrality_percentile)\n",
    "    print(f\"High Proliferation Cells with centrality method and k={k}:\", high_proliferation_nodes_centrality)\n",
    "\n",
    "    # Differential expression analysis\n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_threshold)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with threshold method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_percentile)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with top percentile method:\", differential_genes)\n",
    "    \n",
    "    differential_genes = differential_expression_analysis(X_data, high_proliferation_nodes_centrality)\n",
    "    if differential_genes is not None:\n",
    "        print(\"Differential Genes with centrality method:\", differential_genes)\n",
    "\n",
    "    # Visualize curvature distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(list(ricci_curvatures.values()), bins=50, kde=True)\n",
    "    plt.axvline(np.mean(list(ricci_curvatures.values())), color='r', linestyle='--')\n",
    "    plt.title(f'Distribution of Ricci Curvatures with k={k}')\n",
    "    plt.xlabel('Ricci Curvature')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize high proliferation nodes on the PCA plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='gray', alpha=0.5, label='All Cells')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_threshold), 0], X_pca[list(high_proliferation_nodes_threshold), 1], c='red', label='High Proliferation Cells (Threshold)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_percentile), 0], X_pca[list(high_proliferation_nodes_percentile), 1], c='blue', label='High Proliferation Cells (Percentile)')\n",
    "    plt.scatter(X_pca[list(high_proliferation_nodes_centrality), 0], X_pca[list(high_proliferation_nodes_centrality), 1], c='green', label='High Proliferation Cells (Centrality)')\n",
    "    plt.title(f'PCA of scRNA-seq Data with k={k}')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce5b837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load scRNA data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Ensure 'persister_label' column exists\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersister_label\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/PUHTI_TYKKY_qLyOyDD/miniconda/envs/env1/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(data)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)[0]\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size)\n",
    "\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for scBERT model\n",
    "for epoch in range(num_epochs):\n",
    "    scbert_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_bert:\n",
    "        optimizer_bert.zero_grad()\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_bert)}\")\n",
    "\n",
    "# Evaluation loop for scBERT model\n",
    "scbert_model.eval()\n",
    "correct_bert = 0\n",
    "total_bert = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_bert:\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = scbert_model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_bert += labels.size(0)\n",
    "        correct_bert += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scBERT model on test set:', (correct_bert / total_bert))\n",
    "\n",
    "# Training loop for scGPT model\n",
    "for epoch in range(num_epochs):\n",
    "    scgpt_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_gpt:\n",
    "        optimizer_gpt.zero_grad()\n",
    "\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_gpt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_dataloader_gpt)}\")\n",
    "\n",
    "# Evaluation loop for scGPT model\n",
    "scgpt_model.eval()\n",
    "correct_gpt = 0\n",
    "total_gpt = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_gpt:\n",
    "        input_ids = inputs\n",
    "        outputs = scgpt_model(input_ids)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_gpt += labels.size(0)\n",
    "        correct_gpt += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of scGPT model on test set:', (correct_gpt / total_gpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "29/05/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5318bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "# Load scRNA data\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell/GSE150949_scRNA.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'persister_label' column exists\n",
    "if 'persister_label' not in data.columns:\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    data['persister_label'] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "actual_labels = data['persister_label'].values\n",
    "\n",
    "# Drop the label column from the data\n",
    "data = data.drop(columns=['persister_label'])\n",
    "\n",
    "# Preprocess the scRNA data\n",
    "def preprocess_scRNA_data(data):\n",
    "    adata = sc.AnnData(sparse.csr_matrix(data.values))  # Convert to sparse matrix\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize\n",
    "    sc.pp.log1p(adata)  # Logarithmic transformation\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, subset=True)  # Select highly variable genes\n",
    "    adata.obs['batch'] = np.random.randint(0, 2, size=adata.shape[0])  # Dummy batch column for batch correction\n",
    "    sc.pp.combat(adata, key='batch')  # Batch correction\n",
    "    return adata.X\n",
    "\n",
    "# Preprocess scRNA data\n",
    "X_data = preprocess_scRNA_data(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, actual_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom dataset for DataLoader\n",
    "class RNASeqDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, model_type='bert', max_length=512):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = model_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ' '.join(map(str, self.X[idx]))\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.model_type == 'bert':\n",
    "            inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "            inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}\n",
    "        elif self.model_type == 'gpt':\n",
    "            inputs = self.tokenizer.encode(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).squeeze(0)\n",
    "\n",
    "        return inputs, label\n",
    "\n",
    "# Define scBERT model\n",
    "class SCBERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased', num_classes=2):\n",
    "        super(SCBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Define scGPT model\n",
    "class SCGPTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='gpt2', num_classes=2):\n",
    "        super(SCGPTClassifier, self).__init__()\n",
    "        self.gpt = GPT2Model.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.gpt.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate tokenizer and models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "scbert_model = SCBERTClassifier()\n",
    "scgpt_model = SCGPTClassifier()\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset_bert = RNASeqDataset(X_train, y_train, bert_tokenizer, model_type='bert')\n",
    "train_dataset_gpt = RNASeqDataset(X_train, y_train, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "test_dataset_bert = RNASeqDataset(X_test, y_test, bert_tokenizer, model_type='bert')\n",
    "test_dataset_gpt = RNASeqDataset(X_test, y_test, gpt_tokenizer, model_type='gpt')\n",
    "\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_gpt = DataLoader(test_dataset_gpt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, optimizer, train_dataloader, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_dataloader)}')\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scBERT\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    scbert_model = SCBERTClassifier()\n",
    "    scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "    optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scbert_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader_bert:\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            outputs = scbert_model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_dataloader_bert)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "best_params = study.best_params\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "learning_rate = best_params['learning_rate']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "\n",
    "# Train final scBERT model with best hyperparameters\n",
    "scbert_model = SCBERTClassifier()\n",
    "scbert_model.dropout = nn.Dropout(dropout_rate)\n",
    "optimizer_bert = optim.AdamW(scbert_model.parameters(), lr=learning_rate)\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "train_model(scbert_model, optimizer_bert, train_dataloader_bert, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scBERT model on test set\n",
    "test_accuracy_bert = evaluate_model(scbert_model, test_dataloader_bert)\n",
    "print('Accuracy of scBERT model on test set:', test_accuracy_bert)\n",
    "\n",
    "# Hyperparameter tuning with Optuna for scGPT\n",
    "def objective_gpt(trial):\n",
    "    batch_size_gpt = trial.suggest_categorical('batch_size_gpt', [16, 32, 64])\n",
    "    learning_rate_gpt = trial.suggest_float('learning_rate_gpt', 1e-5, 1e-3, log=True)\n",
    "    dropout_rate_gpt = trial.suggest_float('dropout_rate_gpt', 0.1, 0.5)\n",
    "\n",
    "    scgpt_model = SCGPTClassifier()\n",
    "    scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "    optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "    train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "\n",
    "    best_loss_gpt = float('inf')\n",
    "    early_stop_count_gpt = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        scgpt_model.train()\n",
    "        running_loss_gpt = 0.0\n",
    "        for inputs, labels in train_dataloader_gpt:\n",
    "            optimizer_gpt.zero_grad()\n",
    "            input_ids = inputs\n",
    "            outputs = scgpt_model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_gpt.step()\n",
    "            running_loss_gpt += loss.item()\n",
    "        epoch_loss_gpt = running_loss_gpt / len(train_dataloader_gpt)\n",
    "        if epoch_loss_gpt < best_loss_gpt:\n",
    "            best_loss_gpt = epoch_loss_gpt\n",
    "            early_stop_count_gpt = 0\n",
    "        else:\n",
    "            early_stop_count_gpt += 1\n",
    "            if early_stop_count_gpt >= 3:\n",
    "                break\n",
    "    return best_loss_gpt\n",
    "\n",
    "study_gpt = optuna.create_study(direction='minimize')\n",
    "study_gpt.optimize(objective_gpt, n_trials=50)\n",
    "best_params_gpt = study_gpt.best_params\n",
    "\n",
    "batch_size_gpt = best_params_gpt['batch_size_gpt']\n",
    "learning_rate_gpt = best_params_gpt['learning_rate_gpt']\n",
    "dropout_rate_gpt = best_params_gpt['dropout_rate_gpt']\n",
    "\n",
    "# Train final scGPT model with best hyperparameters\n",
    "scgpt_model = SCGPTClassifier() \n",
    "scgpt_model.dropout = nn.Dropout(dropout_rate_gpt)\n",
    "optimizer_gpt = optim.AdamW(scgpt_model.parameters(), lr=learning_rate_gpt)\n",
    "train_dataloader_gpt = DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
    "train_model(scgpt_model, optimizer_gpt, train_dataloader_gpt, criterion, num_epochs)\n",
    "\n",
    "# Evaluate scGPT model on test set\n",
    "test_accuracy_gpt = evaluate_model(scgpt_model, test_dataloader_gpt)\n",
    "print('Accuracy of scGPT model on test set:', test_accuracy_gpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afaf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ca52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb9a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
