{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5788bb93-b301-45e4-95b7-ad7242b3d26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    # Clean and prepare metadata\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Prepare scRNA data\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Find common cells\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    # Filter metadata and scRNA data based on common cells\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    # Merge metadata with scRNA data\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/scratch/project_2010376/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010751/normalized_GSE150949_pc9_count.csv'\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_path, data_path)\n",
    "\n",
    "# Ensure y is of integer type\n",
    "y = y.astype(int)\n",
    "\n",
    "# Debugging prints\n",
    "print(\"y dtype:\", y.dtype)\n",
    "print(\"y unique values:\", np.unique(y))\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Debugging prints\n",
    "classes = np.unique(y)\n",
    "print(\"Classes dtype:\", classes.dtype)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Debugging prints\n",
    "print(\"Class weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249098e1-4c8e-48a6-8a8d-b7ef5b6c23d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a9e7b-2f22-407a-a8f3-25f2f6eca892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9dbf945-3db6-469a-ab75-e296b9203765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Distribution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 12\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m class_weight_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(class_weights))\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/sklearn/utils/class_weight.py:54\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses should have valid labels that are in y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     recip_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(le\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_ind)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[0;32m---> 54\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mrecip_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# user-defined dictionary\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(classes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Ensure y is of integer type\n",
    "y = y.astype(int)\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60274398-b85d-4cb9-b6a4-ce91559f404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=500)  # n_components based on available memory and performance\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d672cd-d319-4da2-b712-8e808d61c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 16:42:57.804809: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 70ms/step - loss: 1.1495 - val_loss: 1.0619 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - loss: 1.0118 - val_loss: 1.0290 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - loss: 1.0078 - val_loss: 1.0200 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 1.0029 - val_loss: 1.0151 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 114ms/step - loss: 1.0042 - val_loss: 1.0145 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9979 - val_loss: 1.0141 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 1.0015 - val_loss: 1.0139 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 1.0006 - val_loss: 1.0137 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - loss: 1.0008 - val_loss: 1.0135 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9996 - val_loss: 1.0135 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9965 - val_loss: 1.0134 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - loss: 0.9962 - val_loss: 1.0130 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - loss: 0.9974 - val_loss: 1.0130 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 1.0021 - val_loss: 1.0127 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 1.0003 - val_loss: 1.0126 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.9973 - val_loss: 1.0120 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 97ms/step - loss: 0.9977 - val_loss: 1.0113 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - loss: 0.9935 - val_loss: 1.0116 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9980 - val_loss: 1.0100 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 70ms/step - loss: 0.9987 - val_loss: 1.0090 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 0.9901 - val_loss: 1.0085 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 97ms/step - loss: 0.9884 - val_loss: 1.0073 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 93ms/step - loss: 0.9916 - val_loss: 1.0062 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 84ms/step - loss: 0.9948 - val_loss: 1.0057 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.9899 - val_loss: 1.0041 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9945 - val_loss: 1.0026 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - loss: 0.9895 - val_loss: 1.0020 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - loss: 0.9880 - val_loss: 1.0011 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9876 - val_loss: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 49ms/step - loss: 0.9849 - val_loss: 0.9989 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - loss: 0.9858 - val_loss: 0.9972 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 61ms/step - loss: 0.9838 - val_loss: 0.9966 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - loss: 0.9784 - val_loss: 0.9955 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 68ms/step - loss: 0.9824 - val_loss: 0.9944 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 82ms/step - loss: 0.9799 - val_loss: 0.9935 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9803 - val_loss: 0.9927 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 111ms/step - loss: 0.9806 - val_loss: 0.9916 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.9839 - val_loss: 0.9909 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 102ms/step - loss: 0.9813 - val_loss: 0.9904 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9821 - val_loss: 0.9892 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 0.9774 - val_loss: 0.9890 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9841 - val_loss: 0.9876 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - loss: 0.9707 - val_loss: 0.9867 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - loss: 0.9750 - val_loss: 0.9860 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.9779 - val_loss: 0.9861 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - loss: 0.9734 - val_loss: 0.9847 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9711 - val_loss: 0.9843 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 60ms/step - loss: 0.9702 - val_loss: 0.9840 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - loss: 0.9714 - val_loss: 0.9833 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9721 - val_loss: 0.9822 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 0.9732 - val_loss: 0.9816 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - loss: 0.9696 - val_loss: 0.9810 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 0.9673 - val_loss: 0.9807 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - loss: 0.9685 - val_loss: 0.9799 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - loss: 0.9750 - val_loss: 0.9799 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 65ms/step - loss: 0.9676 - val_loss: 0.9794 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9661 - val_loss: 0.9789 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9690 - val_loss: 0.9790 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 97ms/step - loss: 0.9688 - val_loss: 0.9780 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9653 - val_loss: 0.9776 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 75ms/step - loss: 0.9705 - val_loss: 0.9799 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - loss: 0.9736 - val_loss: 0.9770 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 67ms/step - loss: 0.9647 - val_loss: 0.9768 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9714 - val_loss: 0.9773 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 91ms/step - loss: 0.9679 - val_loss: 0.9767 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.9705 - val_loss: 0.9765 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9658 - val_loss: 0.9763 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9660 - val_loss: 0.9760 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9640 - val_loss: 0.9754 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.9632 - val_loss: 0.9748 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9621 - val_loss: 0.9747 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - loss: 0.9665 - val_loss: 0.9762 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - loss: 0.9657 - val_loss: 0.9741 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - loss: 0.9602 - val_loss: 0.9750 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - loss: 0.9627 - val_loss: 0.9742 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9615 - val_loss: 0.9744 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 0.9648 - val_loss: 0.9740 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9668 - val_loss: 0.9740 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - loss: 0.9660 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - loss: 0.9638 - val_loss: 0.9732 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9678 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - loss: 0.9652 - val_loss: 0.9724 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - loss: 0.9643 - val_loss: 0.9726 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9626 - val_loss: 0.9727 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 107ms/step - loss: 0.9638 - val_loss: 0.9721 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 97ms/step - loss: 0.9634 - val_loss: 0.9714 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9645 - val_loss: 0.9713 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 150ms/step - loss: 0.9587 - val_loss: 0.9713 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - loss: 0.9604 - val_loss: 0.9713 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - loss: 0.9628 - val_loss: 0.9716 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 107ms/step - loss: 0.9654 - val_loss: 0.9732 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 106ms/step - loss: 0.9665 - val_loss: 0.9715 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 89ms/step - loss: 0.9611 - val_loss: 0.9714 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - loss: 0.9627 - val_loss: 0.9704 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 90ms/step - loss: 0.9604 - val_loss: 0.9704 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - loss: 0.9622 - val_loss: 0.9701 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 100ms/step - loss: 0.9579 - val_loss: 0.9701 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - loss: 0.9608 - val_loss: 0.9700 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 96ms/step - loss: 0.9584 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 138ms/step - loss: 0.9608 - val_loss: 0.9701 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 0.9571 - val_loss: 0.9696 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - loss: 0.9579 - val_loss: 0.9691 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9587 - val_loss: 0.9692 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - loss: 0.9605 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9579 - val_loss: 0.9687 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9636 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 0.9579 - val_loss: 0.9689 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9645 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 76ms/step - loss: 0.9593 - val_loss: 0.9684 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - loss: 0.9595 - val_loss: 0.9687 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9605 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 0.9590 - val_loss: 0.9684 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - loss: 0.9569 - val_loss: 0.9674 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - loss: 0.9580 - val_loss: 0.9678 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9635 - val_loss: 0.9674 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 0.9619 - val_loss: 0.9678 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9547 - val_loss: 0.9681 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9587 - val_loss: 0.9680 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9566 - val_loss: 0.9675 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9598 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - loss: 0.9546 - val_loss: 0.9671 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - loss: 0.9594 - val_loss: 0.9674 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9541 - val_loss: 0.9669 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9622 - val_loss: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9580 - val_loss: 0.9674 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - loss: 0.9655 - val_loss: 0.9682 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - loss: 0.9547 - val_loss: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 0.9564 - val_loss: 0.9678 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - loss: 0.9592 - val_loss: 0.9680 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - loss: 0.9542 - val_loss: 0.9667 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 130ms/step - loss: 0.9556 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9592 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 100ms/step - loss: 0.9592 - val_loss: 0.9667 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9611 - val_loss: 0.9668 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9616 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 142ms/step - loss: 0.9552 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 104ms/step - loss: 0.9594 - val_loss: 0.9666 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9576 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - loss: 0.9562 - val_loss: 0.9660 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - loss: 0.9613 - val_loss: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - loss: 0.9597 - val_loss: 0.9659 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - loss: 0.9580 - val_loss: 0.9662 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 82ms/step - loss: 0.9587 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9550 - val_loss: 0.9656 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9533 - val_loss: 0.9660 - learning_rate: 0.0010\n",
      "Epoch 146/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9564 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 147/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - loss: 0.9556 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 148/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - loss: 0.9536 - val_loss: 0.9658 - learning_rate: 0.0010\n",
      "Epoch 149/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9537 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 150/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 123ms/step - loss: 0.9557 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 151/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9546 - val_loss: 0.9658 - learning_rate: 0.0010\n",
      "Epoch 152/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 112ms/step - loss: 0.9542 - val_loss: 0.9654 - learning_rate: 0.0010\n",
      "Epoch 153/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - loss: 0.9548 - val_loss: 0.9652 - learning_rate: 0.0010\n",
      "Epoch 154/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9563 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 155/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 60ms/step - loss: 0.9553 - val_loss: 0.9654 - learning_rate: 0.0010\n",
      "Epoch 156/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9520 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 157/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - loss: 0.9553 - val_loss: 0.9646 - learning_rate: 0.0010\n",
      "Epoch 158/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - loss: 0.9576 - val_loss: 0.9654 - learning_rate: 0.0010\n",
      "Epoch 159/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 127ms/step - loss: 0.9555 - val_loss: 0.9644 - learning_rate: 0.0010\n",
      "Epoch 160/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9586 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 161/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 83ms/step - loss: 0.9590 - val_loss: 0.9650 - learning_rate: 0.0010\n",
      "Epoch 162/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9577 - val_loss: 0.9643 - learning_rate: 0.0010\n",
      "Epoch 163/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 51ms/step - loss: 0.9544 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 164/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 0.9538 - val_loss: 0.9644 - learning_rate: 0.0010\n",
      "Epoch 165/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - loss: 0.9566 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 166/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 61ms/step - loss: 0.9560 - val_loss: 0.9637 - learning_rate: 0.0010\n",
      "Epoch 167/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9532 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 168/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - loss: 0.9535 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 169/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9540 - val_loss: 0.9637 - learning_rate: 0.0010\n",
      "Epoch 170/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 83ms/step - loss: 0.9582 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 171/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9543 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 172/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9545 - val_loss: 0.9635 - learning_rate: 0.0010\n",
      "Epoch 173/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 0.9554 - val_loss: 0.9630 - learning_rate: 0.0010\n",
      "Epoch 174/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - loss: 0.9521 - val_loss: 0.9642 - learning_rate: 0.0010\n",
      "Epoch 175/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 0.9552 - val_loss: 0.9630 - learning_rate: 0.0010\n",
      "Epoch 176/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 56ms/step - loss: 0.9514 - val_loss: 0.9640 - learning_rate: 0.0010\n",
      "Epoch 177/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9533 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 178/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 119ms/step - loss: 0.9553 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 179/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 89ms/step - loss: 0.9505 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 180/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 0.9555 - val_loss: 0.9637 - learning_rate: 0.0010\n",
      "Epoch 181/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - loss: 0.9543 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 182/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 92ms/step - loss: 0.9559 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 183/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9511 - val_loss: 0.9633 - learning_rate: 0.0010\n",
      "Epoch 184/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 89ms/step - loss: 0.9547 - val_loss: 0.9626 - learning_rate: 5.0000e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9557 - val_loss: 0.9627 - learning_rate: 5.0000e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 126ms/step - loss: 0.9490 - val_loss: 0.9628 - learning_rate: 5.0000e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9547 - val_loss: 0.9628 - learning_rate: 5.0000e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 132ms/step - loss: 0.9555 - val_loss: 0.9624 - learning_rate: 5.0000e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9511 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 126ms/step - loss: 0.9487 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9560 - val_loss: 0.9623 - learning_rate: 5.0000e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 137ms/step - loss: 0.9491 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9535 - val_loss: 0.9625 - learning_rate: 5.0000e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 124ms/step - loss: 0.9520 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - loss: 0.9478 - val_loss: 0.9620 - learning_rate: 5.0000e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.9503 - val_loss: 0.9621 - learning_rate: 5.0000e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - loss: 0.9505 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - loss: 0.9493 - val_loss: 0.9619 - learning_rate: 5.0000e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 0.9477 - val_loss: 0.9620 - learning_rate: 5.0000e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 0.9514 - val_loss: 0.9617 - learning_rate: 5.0000e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 88ms/step - loss: 0.9501 - val_loss: 0.9623 - learning_rate: 5.0000e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9511 - val_loss: 0.9618 - learning_rate: 5.0000e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 92ms/step - loss: 0.9508 - val_loss: 0.9618 - learning_rate: 5.0000e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9543 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - loss: 0.9511 - val_loss: 0.9622 - learning_rate: 5.0000e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - loss: 0.9464 - val_loss: 0.9619 - learning_rate: 5.0000e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - loss: 0.9541 - val_loss: 0.9620 - learning_rate: 5.0000e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9496 - val_loss: 0.9619 - learning_rate: 5.0000e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9506 - val_loss: 0.9617 - learning_rate: 5.0000e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 62ms/step - loss: 0.9493 - val_loss: 0.9621 - learning_rate: 5.0000e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - loss: 0.9555 - val_loss: 0.9621 - learning_rate: 2.5000e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 81ms/step - loss: 0.9510 - val_loss: 0.9616 - learning_rate: 2.5000e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - loss: 0.9517 - val_loss: 0.9618 - learning_rate: 2.5000e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 90ms/step - loss: 0.9502 - val_loss: 0.9617 - learning_rate: 2.5000e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 92ms/step - loss: 0.9523 - val_loss: 0.9617 - learning_rate: 2.5000e-04\n",
      "Epoch 216/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 0.9470 - val_loss: 0.9615 - learning_rate: 2.5000e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 98ms/step - loss: 0.9471 - val_loss: 0.9616 - learning_rate: 2.5000e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9511 - val_loss: 0.9613 - learning_rate: 2.5000e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9549 - val_loss: 0.9613 - learning_rate: 2.5000e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - loss: 0.9467 - val_loss: 0.9614 - learning_rate: 2.5000e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9470 - val_loss: 0.9615 - learning_rate: 2.5000e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - loss: 0.9500 - val_loss: 0.9614 - learning_rate: 2.5000e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - loss: 0.9498 - val_loss: 0.9614 - learning_rate: 2.5000e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 61ms/step - loss: 0.9485 - val_loss: 0.9613 - learning_rate: 2.5000e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9520 - val_loss: 0.9613 - learning_rate: 2.5000e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - loss: 0.9462 - val_loss: 0.9616 - learning_rate: 2.5000e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9542 - val_loss: 0.9615 - learning_rate: 2.5000e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 118ms/step - loss: 0.9535 - val_loss: 0.9617 - learning_rate: 2.5000e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - loss: 0.9478 - val_loss: 0.9612 - learning_rate: 1.2500e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - loss: 0.9485 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - loss: 0.9500 - val_loss: 0.9612 - learning_rate: 1.2500e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9493 - val_loss: 0.9612 - learning_rate: 1.2500e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - loss: 0.9502 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9519 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - loss: 0.9569 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 69ms/step - loss: 0.9482 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9451 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9528 - val_loss: 0.9612 - learning_rate: 1.2500e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - loss: 0.9513 - val_loss: 0.9610 - learning_rate: 1.2500e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9555 - val_loss: 0.9610 - learning_rate: 1.2500e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - loss: 0.9500 - val_loss: 0.9610 - learning_rate: 1.2500e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - loss: 0.9569 - val_loss: 0.9612 - learning_rate: 1.2500e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9535 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - loss: 0.9482 - val_loss: 0.9609 - learning_rate: 1.2500e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.9545 - val_loss: 0.9609 - learning_rate: 1.2500e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9508 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9548 - val_loss: 0.9609 - learning_rate: 1.2500e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 117ms/step - loss: 0.9510 - val_loss: 0.9608 - learning_rate: 1.2500e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.9479 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 250/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 125ms/step - loss: 0.9500 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 251/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9519 - val_loss: 0.9610 - learning_rate: 1.2500e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9519 - val_loss: 0.9609 - learning_rate: 1.2500e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - loss: 0.9475 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9455 - val_loss: 0.9609 - learning_rate: 1.2500e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - loss: 0.9554 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 106ms/step - loss: 0.9501 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - loss: 0.9451 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9509 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9542 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9484 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - loss: 0.9484 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9513 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9528 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9513 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - loss: 0.9513 - val_loss: 0.9609 - learning_rate: 6.2500e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9504 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9525 - val_loss: 0.9610 - learning_rate: 6.2500e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 0.9510 - val_loss: 0.9607 - learning_rate: 6.2500e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - loss: 0.9508 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 270/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9523 - val_loss: 0.9608 - learning_rate: 6.2500e-05\n",
      "Epoch 271/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - loss: 0.9471 - val_loss: 0.9608 - learning_rate: 3.1250e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9519 - val_loss: 0.9608 - learning_rate: 3.1250e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 0.9529 - val_loss: 0.9608 - learning_rate: 3.1250e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9468 - val_loss: 0.9607 - learning_rate: 3.1250e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - loss: 0.9514 - val_loss: 0.9608 - learning_rate: 3.1250e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 0.9501 - val_loss: 0.9607 - learning_rate: 3.1250e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - loss: 0.9493 - val_loss: 0.9607 - learning_rate: 3.1250e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - loss: 0.9504 - val_loss: 0.9607 - learning_rate: 3.1250e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9511 - val_loss: 0.9607 - learning_rate: 3.1250e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 102ms/step - loss: 0.9501 - val_loss: 0.9608 - learning_rate: 3.1250e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9535 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - loss: 0.9529 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9519 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - loss: 0.9527 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 0.9515 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - loss: 0.9471 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 51ms/step - loss: 0.9454 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9530 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 289/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - loss: 0.9492 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - loss: 0.9471 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 291/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - loss: 0.9496 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 292/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - loss: 0.9526 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 293/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - loss: 0.9511 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 294/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.9501 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 295/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 65ms/step - loss: 0.9514 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 296/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - loss: 0.9540 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 297/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - loss: 0.9525 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 298/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.9481 - val_loss: 0.9608 - learning_rate: 1.5625e-05\n",
      "Epoch 299/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.9546 - val_loss: 0.9607 - learning_rate: 1.5625e-05\n",
      "Epoch 300/300\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - loss: 0.9514 - val_loss: 0.9607 - learning_rate: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "def create_complex_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    x = Dense(2048, activation='relu')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(512, activation='relu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 256  # Adjusted encoding dimension for detailed encoding\n",
    "\n",
    "autoencoder, encoder = create_complex_autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_complex_autoencoder.keras', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=300,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_validation, X_validation),\n",
    "                callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "autoencoder.save('complex_autoencoder_model.keras')\n",
    "encoder.save('complex_encoder_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3fd2e2-f2e1-44cd-8474-fff5d6e01d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_validation_encoded = encoder.predict(X_validation)\n",
    "X_test_encoded = encoder.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4b93b6-ddf6-44ee-a583-1714b9e4b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3836 - loss: 1.9653 - val_accuracy: 0.8098 - val_loss: 0.7175 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7058 - loss: 1.0188 - val_accuracy: 0.8231 - val_loss: 0.6904 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7506 - loss: 0.9078 - val_accuracy: 0.8300 - val_loss: 0.6733 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7720 - loss: 0.8389 - val_accuracy: 0.8330 - val_loss: 0.6561 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7813 - loss: 0.7861 - val_accuracy: 0.8317 - val_loss: 0.6433 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7915 - loss: 0.7558 - val_accuracy: 0.8357 - val_loss: 0.6263 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7994 - loss: 0.7360 - val_accuracy: 0.8357 - val_loss: 0.6171 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8094 - loss: 0.6951 - val_accuracy: 0.8387 - val_loss: 0.6087 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8132 - loss: 0.6881 - val_accuracy: 0.8377 - val_loss: 0.6012 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.8209 - loss: 0.6559 - val_accuracy: 0.8382 - val_loss: 0.5929 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8213 - loss: 0.6601 - val_accuracy: 0.8378 - val_loss: 0.5874 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8258 - loss: 0.6441 - val_accuracy: 0.8395 - val_loss: 0.5815 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8307 - loss: 0.6383 - val_accuracy: 0.8417 - val_loss: 0.5789 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8347 - loss: 0.6208 - val_accuracy: 0.8420 - val_loss: 0.5749 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8344 - loss: 0.6246 - val_accuracy: 0.8425 - val_loss: 0.5699 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8406 - loss: 0.6062 - val_accuracy: 0.8402 - val_loss: 0.5688 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8383 - loss: 0.6062 - val_accuracy: 0.8412 - val_loss: 0.5651 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8397 - loss: 0.6064 - val_accuracy: 0.8417 - val_loss: 0.5648 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8430 - loss: 0.5936 - val_accuracy: 0.8418 - val_loss: 0.5616 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8420 - loss: 0.5958 - val_accuracy: 0.8435 - val_loss: 0.5610 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8467 - loss: 0.5886 - val_accuracy: 0.8435 - val_loss: 0.5600 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8477 - loss: 0.5757 - val_accuracy: 0.8436 - val_loss: 0.5586 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8500 - loss: 0.5721 - val_accuracy: 0.8435 - val_loss: 0.5571 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8493 - loss: 0.5736 - val_accuracy: 0.8455 - val_loss: 0.5548 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.8502 - loss: 0.5674 - val_accuracy: 0.8448 - val_loss: 0.5568 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8503 - loss: 0.5643 - val_accuracy: 0.8455 - val_loss: 0.5548 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8464 - loss: 0.5752 - val_accuracy: 0.8460 - val_loss: 0.5535 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8512 - loss: 0.5658 - val_accuracy: 0.8458 - val_loss: 0.5522 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8523 - loss: 0.5585 - val_accuracy: 0.8451 - val_loss: 0.5535 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8541 - loss: 0.5627 - val_accuracy: 0.8445 - val_loss: 0.5530 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8524 - loss: 0.5592 - val_accuracy: 0.8463 - val_loss: 0.5525 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8519 - loss: 0.5580 - val_accuracy: 0.8465 - val_loss: 0.5518 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8550 - loss: 0.5553 - val_accuracy: 0.8460 - val_loss: 0.5524 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8548 - loss: 0.5481 - val_accuracy: 0.8471 - val_loss: 0.5517 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8557 - loss: 0.5445 - val_accuracy: 0.8448 - val_loss: 0.5528 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8578 - loss: 0.5433 - val_accuracy: 0.8458 - val_loss: 0.5517 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8575 - loss: 0.5395 - val_accuracy: 0.8468 - val_loss: 0.5525 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8578 - loss: 0.5422 - val_accuracy: 0.8458 - val_loss: 0.5508 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.8593 - loss: 0.5355 - val_accuracy: 0.8460 - val_loss: 0.5510 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8576 - loss: 0.5397 - val_accuracy: 0.8448 - val_loss: 0.5525 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8573 - loss: 0.5403 - val_accuracy: 0.8465 - val_loss: 0.5528 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8634 - loss: 0.5297 - val_accuracy: 0.8475 - val_loss: 0.5510 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8608 - loss: 0.5273 - val_accuracy: 0.8473 - val_loss: 0.5514 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.8604 - loss: 0.5284 - val_accuracy: 0.8476 - val_loss: 0.5504 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8633 - loss: 0.5195 - val_accuracy: 0.8445 - val_loss: 0.5531 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8608 - loss: 0.5292 - val_accuracy: 0.8468 - val_loss: 0.5523 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8640 - loss: 0.5183 - val_accuracy: 0.8465 - val_loss: 0.5545 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8613 - loss: 0.5252 - val_accuracy: 0.8476 - val_loss: 0.5529 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8611 - loss: 0.5272 - val_accuracy: 0.8460 - val_loss: 0.5543 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8631 - loss: 0.5197 - val_accuracy: 0.8484 - val_loss: 0.5529 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8666 - loss: 0.5089 - val_accuracy: 0.8461 - val_loss: 0.5528 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8630 - loss: 0.5199 - val_accuracy: 0.8465 - val_loss: 0.5534 - learning_rate: 5.0000e-05\n",
      "Epoch 53/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8654 - loss: 0.5109 - val_accuracy: 0.8480 - val_loss: 0.5528 - learning_rate: 5.0000e-05\n",
      "Epoch 54/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8648 - loss: 0.5174 - val_accuracy: 0.8468 - val_loss: 0.5545 - learning_rate: 5.0000e-05\n",
      "Epoch 55/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8665 - loss: 0.5151 - val_accuracy: 0.8455 - val_loss: 0.5538 - learning_rate: 5.0000e-05\n",
      "Epoch 56/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.5116 - val_accuracy: 0.8455 - val_loss: 0.5543 - learning_rate: 5.0000e-05\n",
      "Epoch 57/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.5193 - val_accuracy: 0.8480 - val_loss: 0.5539 - learning_rate: 5.0000e-05\n",
      "Epoch 58/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.8677 - loss: 0.5040 - val_accuracy: 0.8486 - val_loss: 0.5552 - learning_rate: 5.0000e-05\n",
      "Epoch 59/150\n",
      "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8674 - loss: 0.5104 - val_accuracy: 0.8470 - val_loss: 0.5555 - learning_rate: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_classifier(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (encoding_dim,)\n",
    "\n",
    "classifier = create_ensemble_classifier(input_shape)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "classifier.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_classifier.keras', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "classifier.fit(X_train_encoded, y_train,\n",
    "               epochs=150,\n",
    "               batch_size=128,\n",
    "               validation_data=(X_validation_encoded, y_validation),\n",
    "               class_weight=class_weight_dict,\n",
    "               callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "classifier.save('classifier_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd91ddf-657c-4652-ab6d-196e7490d426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.8572 - loss: 0.5054\n",
      "Test Accuracy: 0.8549402952194214\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       722\n",
      "     14_high       0.86      0.88      0.87      1146\n",
      "      14_low       0.71      0.73      0.72       726\n",
      "      14_med       0.75      0.71      0.73      1020\n",
      "           3       0.94      0.93      0.94      1543\n",
      "           7       0.89      0.90      0.89       875\n",
      "\n",
      "    accuracy                           0.85      6032\n",
      "   macro avg       0.85      0.85      0.85      6032\n",
      "weighted avg       0.85      0.85      0.85      6032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict the test set\n",
    "y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2edefd7-a79c-4779-8667-41d791bfea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 18:48:53.331533: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 18:48:56.354048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 18:49:21.635090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    # Clean and prepare metadata\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Prepare scRNA data\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Find common cells\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    # Filter metadata and scRNA data based on common cells\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    # Merge metadata with scRNA data\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_pc9_count_matrix.csv'\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_path, data_path)\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b860f5d-9502-4f09-9951-28193aeb4d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample types: ['0' '7' '3' '14_high' '14_med' '14_low']\n",
      "Sample type distribution:\n",
      " sample_type\n",
      "3          15338\n",
      "14_high    11571\n",
      "14_med      9931\n",
      "7           8891\n",
      "14_low      7358\n",
      "0           7226\n",
      "Name: count, dtype: int64\n",
      "Merged data shape: (60315, 22169)\n",
      "First few rows of merged data:\n",
      "                cell  0  1  2  3  4  5  6  7  8  ... 22158 22159 22160 22161  \\\n",
      "0  AAACCTGAGACAAGCC  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "1  AAACCTGAGCAGACTG  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "2  AAACCTGAGCGAGAAA  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "3  AAACCTGAGGACAGAA  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "4  AAACCTGAGGCCGAAT  0  0  0  1  0  0  0  0  0  ...     0     0     0     0   \n",
      "\n",
      "  22162 22163 22164 22165 sample_name sample_type  \n",
      "0     0     0     0     0           0           0  \n",
      "1     0     0     0     0           0           0  \n",
      "2     0     0     0     0           0           0  \n",
      "3     0     0     0     0           0           0  \n",
      "4     0     0     0     0           0           0  \n",
      "\n",
      "[5 rows x 22169 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model1: (array([0, 1]), array([ 7226, 53089]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 21:18:41.934635: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 39ms/step - loss: 1.1086 - val_loss: 1.0346 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 1.0069 - val_loss: 1.0130 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 1.0008 - val_loss: 1.0095 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 1.0017 - val_loss: 1.0064 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9926 - val_loss: 1.0037 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9936 - val_loss: 1.0013 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9884 - val_loss: 0.9989 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9865 - val_loss: 0.9965 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9879 - val_loss: 0.9942 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9814 - val_loss: 0.9918 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9779 - val_loss: 0.9896 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9768 - val_loss: 0.9878 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9807 - val_loss: 0.9856 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9742 - val_loss: 0.9845 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9784 - val_loss: 0.9829 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9729 - val_loss: 0.9812 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9758 - val_loss: 0.9794 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9702 - val_loss: 0.9784 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9712 - val_loss: 0.9772 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9729 - val_loss: 0.9754 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9733 - val_loss: 0.9750 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9708 - val_loss: 0.9733 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9654 - val_loss: 0.9723 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9612 - val_loss: 0.9717 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9611 - val_loss: 0.9713 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9676 - val_loss: 0.9701 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9616 - val_loss: 0.9689 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9645 - val_loss: 0.9689 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9679 - val_loss: 0.9681 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9612 - val_loss: 0.9686 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9657 - val_loss: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9621 - val_loss: 0.9659 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9621 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9601 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9578 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9571 - val_loss: 0.9630 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9595 - val_loss: 0.9627 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9610 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9594 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9584 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9619 - val_loss: 0.9609 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9646 - val_loss: 0.9607 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9560 - val_loss: 0.9604 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9564 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9577 - val_loss: 0.9614 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9579 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9522 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9547 - val_loss: 0.9582 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9570 - val_loss: 0.9583 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9588 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9518 - val_loss: 0.9582 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9551 - val_loss: 0.9576 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9534 - val_loss: 0.9568 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9568 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9514 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9532 - val_loss: 0.9568 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9545 - val_loss: 0.9560 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9522 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9544 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9489 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9537 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9520 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9544 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9515 - val_loss: 0.9549 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9547 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9488 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9533 - val_loss: 0.9537 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9524 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9491 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9521 - val_loss: 0.9528 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9569 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9481 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9475 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9570 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9493 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9540 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9519 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9484 - val_loss: 0.9509 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9511 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9529 - val_loss: 0.9504 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 40ms/step - loss: 0.9500 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9466 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9484 - val_loss: 0.9507 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9532 - val_loss: 0.9507 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9474 - val_loss: 0.9510 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9529 - val_loss: 0.9501 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9474 - val_loss: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9486 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9493 - val_loss: 0.9492 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9463 - val_loss: 0.9488 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9460 - val_loss: 0.9494 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9470 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9489 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9457 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9454 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9478 - val_loss: 0.9488 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9488 - val_loss: 0.9484 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9501 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9487 - val_loss: 0.9482 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9503 - val_loss: 0.9483 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9498 - val_loss: 0.9476 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9501 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9513 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9478 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9472 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9474 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9427 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9521 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9466 - val_loss: 0.9470 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9491 - val_loss: 0.9463 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9464 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9440 - val_loss: 0.9461 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9524 - val_loss: 0.9476 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9497 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9491 - val_loss: 0.9460 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9457 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9457 - val_loss: 0.9456 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9490 - val_loss: 0.9459 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9490 - val_loss: 0.9456 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9442 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9487 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9424 - val_loss: 0.9463 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9451 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9477 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9458 - val_loss: 0.9452 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9450 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9481 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9458 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9463 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9446 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9438 - val_loss: 0.9446 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9478 - val_loss: 0.9439 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9452 - val_loss: 0.9443 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9438 - val_loss: 0.9452 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9448 - val_loss: 0.9436 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9442 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9456 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9427 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 146/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9436 - learning_rate: 0.0010\n",
      "Epoch 147/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9456 - val_loss: 0.9447 - learning_rate: 0.0010\n",
      "Epoch 148/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 149/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9448 - val_loss: 0.9443 - learning_rate: 0.0010\n",
      "Epoch 150/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9445 - val_loss: 0.9435 - learning_rate: 0.0010\n",
      "Epoch 151/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9466 - val_loss: 0.9447 - learning_rate: 0.0010\n",
      "Epoch 152/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9403 - val_loss: 0.9436 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9431 - val_loss: 0.9432 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9447 - val_loss: 0.9434 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9435 - val_loss: 0.9431 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9455 - val_loss: 0.9433 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9427 - val_loss: 0.9430 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 40ms/step - loss: 0.9436 - val_loss: 0.9423 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9443 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9421 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9458 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9445 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9420 - val_loss: 0.9420 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9416 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9406 - val_loss: 0.9427 - learning_rate: 5.0000e-04\n",
      "Epoch 167/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 168/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9423 - val_loss: 0.9420 - learning_rate: 5.0000e-04\n",
      "Epoch 169/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9427 - learning_rate: 5.0000e-04\n",
      "Epoch 170/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9471 - val_loss: 0.9426 - learning_rate: 5.0000e-04\n",
      "Epoch 171/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9423 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 172/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9420 - learning_rate: 5.0000e-04\n",
      "Epoch 173/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9431 - val_loss: 0.9416 - learning_rate: 5.0000e-04\n",
      "Epoch 174/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 175/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9422 - learning_rate: 5.0000e-04\n",
      "Epoch 176/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9402 - val_loss: 0.9413 - learning_rate: 2.5000e-04\n",
      "Epoch 177/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9464 - val_loss: 0.9415 - learning_rate: 2.5000e-04\n",
      "Epoch 178/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9421 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 179/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9414 - learning_rate: 2.5000e-04\n",
      "Epoch 180/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9410 - val_loss: 0.9414 - learning_rate: 2.5000e-04\n",
      "Epoch 181/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 182/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 183/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9413 - learning_rate: 2.5000e-04\n",
      "Epoch 184/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9464 - val_loss: 0.9414 - learning_rate: 2.5000e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9431 - val_loss: 0.9407 - learning_rate: 2.5000e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9467 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9430 - val_loss: 0.9410 - learning_rate: 2.5000e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9412 - learning_rate: 2.5000e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9408 - val_loss: 0.9411 - learning_rate: 2.5000e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9409 - learning_rate: 2.5000e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9443 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9426 - val_loss: 0.9413 - learning_rate: 2.5000e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9410 - learning_rate: 2.5000e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9370 - val_loss: 0.9410 - learning_rate: 2.5000e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9405 - learning_rate: 2.5000e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9412 - val_loss: 0.9404 - learning_rate: 2.5000e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9403 - learning_rate: 2.5000e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9407 - learning_rate: 2.5000e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9364 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9435 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9385 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9379 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9435 - val_loss: 0.9406 - learning_rate: 2.5000e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9401 - val_loss: 0.9408 - learning_rate: 2.5000e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9402 - learning_rate: 2.5000e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9399 - learning_rate: 2.5000e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9422 - val_loss: 0.9399 - learning_rate: 1.2500e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9410 - val_loss: 0.9402 - learning_rate: 1.2500e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9398 - learning_rate: 1.2500e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9392 - val_loss: 0.9399 - learning_rate: 1.2500e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9398 - learning_rate: 1.2500e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9443 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9428 - val_loss: 0.9398 - learning_rate: 1.2500e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9368 - val_loss: 0.9397 - learning_rate: 1.2500e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9395 - learning_rate: 1.2500e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9375 - val_loss: 0.9397 - learning_rate: 1.2500e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9418 - val_loss: 0.9395 - learning_rate: 1.2500e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9370 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9409 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9390 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9361 - val_loss: 0.9395 - learning_rate: 1.2500e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9372 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9387 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9395 - learning_rate: 1.2500e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9391 - val_loss: 0.9393 - learning_rate: 6.2500e-05\n",
      "Epoch 244/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9406 - val_loss: 0.9395 - learning_rate: 6.2500e-05\n",
      "Epoch 245/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9396 - learning_rate: 6.2500e-05\n",
      "Epoch 246/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9366 - val_loss: 0.9396 - learning_rate: 6.2500e-05\n",
      "Epoch 247/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9394 - learning_rate: 6.2500e-05\n",
      "Epoch 248/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9392 - learning_rate: 6.2500e-05\n",
      "Epoch 249/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9391 - learning_rate: 6.2500e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9397 - learning_rate: 6.2500e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9394 - learning_rate: 6.2500e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9395 - learning_rate: 6.2500e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9393 - learning_rate: 6.2500e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9366 - val_loss: 0.9392 - learning_rate: 6.2500e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9392 - learning_rate: 6.2500e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9396 - learning_rate: 6.2500e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9394 - learning_rate: 6.2500e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9391 - learning_rate: 6.2500e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9393 - learning_rate: 3.1250e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9415 - val_loss: 0.9392 - learning_rate: 3.1250e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9391 - learning_rate: 3.1250e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.9405 - val_loss: 0.9394 - learning_rate: 3.1250e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9396 - val_loss: 0.9392 - learning_rate: 3.1250e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9393 - learning_rate: 3.1250e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9392 - learning_rate: 3.1250e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9430 - val_loss: 0.9392 - learning_rate: 3.1250e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9391 - learning_rate: 3.1250e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9392 - learning_rate: 3.1250e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9365 - val_loss: 0.9391 - learning_rate: 1.5625e-05\n",
      "\u001b[1m2655/2655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8466 - loss: 0.4779 - val_accuracy: 0.9610 - val_loss: 0.1647 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9405 - loss: 0.2456 - val_accuracy: 0.9617 - val_loss: 0.1472 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9488 - loss: 0.2081 - val_accuracy: 0.9620 - val_loss: 0.1391 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1885 - val_accuracy: 0.9622 - val_loss: 0.1362 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1816 - val_accuracy: 0.9625 - val_loss: 0.1312 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9572 - loss: 0.1745 - val_accuracy: 0.9623 - val_loss: 0.1293 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1549 - val_accuracy: 0.9625 - val_loss: 0.1257 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9607 - loss: 0.1557 - val_accuracy: 0.9628 - val_loss: 0.1218 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9609 - loss: 0.1534 - val_accuracy: 0.9624 - val_loss: 0.1204 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.1451 - val_accuracy: 0.9638 - val_loss: 0.1172 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1430 - val_accuracy: 0.9639 - val_loss: 0.1148 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9645 - loss: 0.1372 - val_accuracy: 0.9641 - val_loss: 0.1136 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1321 - val_accuracy: 0.9644 - val_loss: 0.1099 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.1317 - val_accuracy: 0.9655 - val_loss: 0.1063 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.1255 - val_accuracy: 0.9666 - val_loss: 0.1040 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9694 - loss: 0.1020 - val_accuracy: 0.9701 - val_loss: 0.0892 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9697 - loss: 0.1003 - val_accuracy: 0.9711 - val_loss: 0.0867 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0962 - val_accuracy: 0.9711 - val_loss: 0.0863 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9712 - loss: 0.0946 - val_accuracy: 0.9719 - val_loss: 0.0852 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.0955 - val_accuracy: 0.9726 - val_loss: 0.0843 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0909 - val_accuracy: 0.9727 - val_loss: 0.0823 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0938 - val_accuracy: 0.9728 - val_loss: 0.0819 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.0882 - val_accuracy: 0.9737 - val_loss: 0.0816 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0896 - val_accuracy: 0.9745 - val_loss: 0.0808 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.0858 - val_accuracy: 0.9745 - val_loss: 0.0789 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0863 - val_accuracy: 0.9749 - val_loss: 0.0780 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.0836 - val_accuracy: 0.9733 - val_loss: 0.0790 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0820 - val_accuracy: 0.9749 - val_loss: 0.0772 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0808 - val_accuracy: 0.9748 - val_loss: 0.0769 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0803 - val_accuracy: 0.9749 - val_loss: 0.0806 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9747 - loss: 0.0830 - val_accuracy: 0.9747 - val_loss: 0.0750 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0801 - val_accuracy: 0.9760 - val_loss: 0.0746 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0785 - val_accuracy: 0.9744 - val_loss: 0.0763 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0792 - val_accuracy: 0.9751 - val_loss: 0.0758 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0772 - val_accuracy: 0.9760 - val_loss: 0.0744 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9772 - loss: 0.0759 - val_accuracy: 0.9774 - val_loss: 0.0715 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0749 - val_accuracy: 0.9760 - val_loss: 0.0738 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9779 - loss: 0.0729 - val_accuracy: 0.9754 - val_loss: 0.0723 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0725 - val_accuracy: 0.9756 - val_loss: 0.0727 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0742 - val_accuracy: 0.9757 - val_loss: 0.0727 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0727 - val_accuracy: 0.9759 - val_loss: 0.0714 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9783 - loss: 0.0708 - val_accuracy: 0.9767 - val_loss: 0.0696 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9783 - loss: 0.0694 - val_accuracy: 0.9762 - val_loss: 0.0714 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9780 - loss: 0.0714 - val_accuracy: 0.9763 - val_loss: 0.0714 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0704 - val_accuracy: 0.9769 - val_loss: 0.0714 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9787 - loss: 0.0689 - val_accuracy: 0.9759 - val_loss: 0.0724 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0683 - val_accuracy: 0.9770 - val_loss: 0.0694 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9798 - loss: 0.0659 - val_accuracy: 0.9789 - val_loss: 0.0672 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0657 - val_accuracy: 0.9778 - val_loss: 0.0701 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9790 - loss: 0.0671 - val_accuracy: 0.9781 - val_loss: 0.0696 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9791 - loss: 0.0664 - val_accuracy: 0.9770 - val_loss: 0.0691 - learning_rate: 1.0000e-04\n",
      "Epoch 62/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0677 - val_accuracy: 0.9777 - val_loss: 0.0710 - learning_rate: 1.0000e-04\n",
      "Epoch 63/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0640 - val_accuracy: 0.9774 - val_loss: 0.0689 - learning_rate: 1.0000e-04\n",
      "Epoch 64/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0649 - val_accuracy: 0.9790 - val_loss: 0.0657 - learning_rate: 1.0000e-04\n",
      "Epoch 65/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0649 - val_accuracy: 0.9786 - val_loss: 0.0697 - learning_rate: 1.0000e-04\n",
      "Epoch 66/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9804 - loss: 0.0617 - val_accuracy: 0.9775 - val_loss: 0.0689 - learning_rate: 1.0000e-04\n",
      "Epoch 67/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0608 - val_accuracy: 0.9794 - val_loss: 0.0671 - learning_rate: 1.0000e-04\n",
      "Epoch 68/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9803 - loss: 0.0627 - val_accuracy: 0.9791 - val_loss: 0.0662 - learning_rate: 1.0000e-04\n",
      "Epoch 69/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0604 - val_accuracy: 0.9787 - val_loss: 0.0684 - learning_rate: 1.0000e-04\n",
      "Epoch 70/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9815 - loss: 0.0616 - val_accuracy: 0.9786 - val_loss: 0.0682 - learning_rate: 1.0000e-04\n",
      "Epoch 71/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9812 - loss: 0.0605 - val_accuracy: 0.9780 - val_loss: 0.0678 - learning_rate: 1.0000e-04\n",
      "Epoch 72/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0606 - val_accuracy: 0.9788 - val_loss: 0.0678 - learning_rate: 5.0000e-05\n",
      "Epoch 73/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0554 - val_accuracy: 0.9784 - val_loss: 0.0677 - learning_rate: 5.0000e-05\n",
      "Epoch 74/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9820 - loss: 0.0598 - val_accuracy: 0.9785 - val_loss: 0.0681 - learning_rate: 5.0000e-05\n",
      "Epoch 75/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0556 - val_accuracy: 0.9787 - val_loss: 0.0675 - learning_rate: 5.0000e-05\n",
      "Epoch 76/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0585 - val_accuracy: 0.9780 - val_loss: 0.0692 - learning_rate: 5.0000e-05\n",
      "Epoch 77/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9827 - loss: 0.0553 - val_accuracy: 0.9788 - val_loss: 0.0686 - learning_rate: 5.0000e-05\n",
      "Epoch 78/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0568 - val_accuracy: 0.9789 - val_loss: 0.0689 - learning_rate: 5.0000e-05\n",
      "Epoch 79/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9836 - loss: 0.0519 - val_accuracy: 0.9782 - val_loss: 0.0699 - learning_rate: 2.5000e-05\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.9798 - loss: 0.0678\n",
      "Test Accuracy: 0.9817291498184204\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Persister       0.98      0.98      0.98      5255\n",
      "    Persister       0.98      0.98      0.98      5363\n",
      "\n",
      "     accuracy                           0.98     10618\n",
      "    macro avg       0.98      0.98      0.98     10618\n",
      " weighted avg       0.98      0.98      0.98     10618\n",
      "\n",
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - loss: 1.1200 - val_loss: 1.0324 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m 33/305\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 1.0055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9889 - val_loss: 0.9917 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9932 - val_loss: 0.9899 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9857 - val_loss: 0.9883 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9855 - val_loss: 0.9866 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9874 - val_loss: 0.9852 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9818 - val_loss: 0.9834 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9809 - val_loss: 0.9822 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9802 - val_loss: 0.9799 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9794 - val_loss: 0.9786 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9759 - val_loss: 0.9765 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9739 - val_loss: 0.9749 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9769 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9747 - val_loss: 0.9726 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - loss: 0.9711 - val_loss: 0.9708 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9754 - val_loss: 0.9697 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9698 - val_loss: 0.9683 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9667 - val_loss: 0.9670 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9682 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9676 - val_loss: 0.9659 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9656 - val_loss: 0.9646 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9675 - val_loss: 0.9645 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9669 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9750 - val_loss: 0.9633 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9658 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9644 - val_loss: 0.9620 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9652 - val_loss: 0.9609 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9603 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9649 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m 39/305\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.9575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9570 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9578 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9589 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9566 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9564 - val_loss: 0.9540 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9548 - val_loss: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9558 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - loss: 0.9591 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9570 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9556 - val_loss: 0.9526 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9603 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9545 - val_loss: 0.9524 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9568 - val_loss: 0.9516 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9574 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9561 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9527 - val_loss: 0.9521 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9576 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9574 - val_loss: 0.9503 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9560 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9551 - val_loss: 0.9494 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - loss: 0.9544 - val_loss: 0.9503 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9543 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9512 - val_loss: 0.9490 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9510 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9532 - val_loss: 0.9490 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9544 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9531 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9499 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9559 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9546 - val_loss: 0.9477 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9502 - val_loss: 0.9484 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9518 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9495 - val_loss: 0.9482 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9471 - val_loss: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9523 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9545 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9549 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9526 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9504 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9511 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9556 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9538 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9546 - val_loss: 0.9470 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9551 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9485 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9506 - val_loss: 0.9459 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9516 - val_loss: 0.9462 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9492 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9476 - val_loss: 0.9472 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9524 - val_loss: 0.9451 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9513 - val_loss: 0.9451 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9515 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9547 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9506 - val_loss: 0.9452 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9558 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9522 - val_loss: 0.9459 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9544 - val_loss: 0.9451 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9508 - val_loss: 0.9446 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9507 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9525 - val_loss: 0.9444 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9494 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9490 - val_loss: 0.9451 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9498 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - loss: 0.9502 - val_loss: 0.9454 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9547 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9525 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9448 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9474 - val_loss: 0.9442 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9522 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9511 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9496 - val_loss: 0.9437 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9482 - val_loss: 0.9437 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9467 - val_loss: 0.9436 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9454 - val_loss: 0.9422 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9477 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9478 - val_loss: 0.9425 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9518 - val_loss: 0.9432 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9465 - val_loss: 0.9427 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9490 - val_loss: 0.9422 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9498 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9473 - val_loss: 0.9422 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9506 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9444 - val_loss: 0.9414 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9443 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9459 - val_loss: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9454 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9464 - val_loss: 0.9419 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9472 - val_loss: 0.9414 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9439 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9506 - val_loss: 0.9411 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9456 - val_loss: 0.9418 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9454 - val_loss: 0.9414 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9467 - val_loss: 0.9413 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9459 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9411 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9450 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9465 - val_loss: 0.9408 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9461 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9453 - val_loss: 0.9400 - learning_rate: 5.0000e-04\n",
      "Epoch 149/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9493 - val_loss: 0.9406 - learning_rate: 5.0000e-04\n",
      "Epoch 150/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9454 - val_loss: 0.9404 - learning_rate: 5.0000e-04\n",
      "Epoch 151/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9474 - val_loss: 0.9398 - learning_rate: 5.0000e-04\n",
      "Epoch 152/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9434 - val_loss: 0.9397 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9442 - val_loss: 0.9400 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9460 - val_loss: 0.9396 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9436 - val_loss: 0.9394 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9442 - val_loss: 0.9398 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9400 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9458 - val_loss: 0.9394 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9459 - val_loss: 0.9394 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9470 - val_loss: 0.9391 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9451 - val_loss: 0.9393 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9416 - val_loss: 0.9390 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9406 - val_loss: 0.9388 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9390 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9445 - val_loss: 0.9384 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9411 - val_loss: 0.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 167/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9428 - val_loss: 0.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 168/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9474 - val_loss: 0.9387 - learning_rate: 5.0000e-04\n",
      "Epoch 169/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9394 - learning_rate: 5.0000e-04\n",
      "Epoch 170/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9391 - learning_rate: 5.0000e-04\n",
      "Epoch 171/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9422 - val_loss: 0.9382 - learning_rate: 5.0000e-04\n",
      "Epoch 172/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9452 - val_loss: 0.9386 - learning_rate: 5.0000e-04\n",
      "Epoch 173/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9382 - learning_rate: 5.0000e-04\n",
      "Epoch 174/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9447 - val_loss: 0.9384 - learning_rate: 5.0000e-04\n",
      "Epoch 175/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9390 - learning_rate: 5.0000e-04\n",
      "Epoch 176/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9405 - val_loss: 0.9381 - learning_rate: 5.0000e-04\n",
      "Epoch 177/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9469 - val_loss: 0.9379 - learning_rate: 5.0000e-04\n",
      "Epoch 178/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9455 - val_loss: 0.9380 - learning_rate: 5.0000e-04\n",
      "Epoch 179/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9381 - learning_rate: 5.0000e-04\n",
      "Epoch 180/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9385 - learning_rate: 5.0000e-04\n",
      "Epoch 181/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9380 - learning_rate: 5.0000e-04\n",
      "Epoch 182/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9388 - learning_rate: 5.0000e-04\n",
      "Epoch 183/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9381 - learning_rate: 5.0000e-04\n",
      "Epoch 184/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9406 - val_loss: 0.9378 - learning_rate: 5.0000e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9421 - val_loss: 0.9375 - learning_rate: 5.0000e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9473 - val_loss: 0.9379 - learning_rate: 5.0000e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9379 - learning_rate: 5.0000e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9438 - val_loss: 0.9376 - learning_rate: 5.0000e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9476 - val_loss: 0.9384 - learning_rate: 5.0000e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9381 - learning_rate: 5.0000e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9447 - val_loss: 0.9382 - learning_rate: 5.0000e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9446 - val_loss: 0.9385 - learning_rate: 5.0000e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9422 - val_loss: 0.9374 - learning_rate: 5.0000e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9413 - val_loss: 0.9377 - learning_rate: 5.0000e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9466 - val_loss: 0.9374 - learning_rate: 5.0000e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9374 - learning_rate: 2.5000e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9431 - val_loss: 0.9372 - learning_rate: 2.5000e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9396 - val_loss: 0.9367 - learning_rate: 2.5000e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9435 - val_loss: 0.9373 - learning_rate: 2.5000e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9374 - learning_rate: 2.5000e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9379 - val_loss: 0.9375 - learning_rate: 2.5000e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9370 - learning_rate: 2.5000e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9370 - learning_rate: 2.5000e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9451 - val_loss: 0.9375 - learning_rate: 2.5000e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9370 - learning_rate: 2.5000e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9373 - learning_rate: 2.5000e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9448 - val_loss: 0.9369 - learning_rate: 2.5000e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9369 - learning_rate: 2.5000e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9438 - val_loss: 0.9370 - learning_rate: 1.2500e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9369 - learning_rate: 1.2500e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9437 - val_loss: 0.9367 - learning_rate: 1.2500e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9367 - learning_rate: 1.2500e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9455 - val_loss: 0.9365 - learning_rate: 1.2500e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9367 - learning_rate: 1.2500e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9428 - val_loss: 0.9366 - learning_rate: 1.2500e-04\n",
      "Epoch 216/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9367 - learning_rate: 1.2500e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9366 - learning_rate: 1.2500e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9436 - val_loss: 0.9365 - learning_rate: 1.2500e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9368 - learning_rate: 1.2500e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9362 - val_loss: 0.9366 - learning_rate: 1.2500e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9428 - val_loss: 0.9368 - learning_rate: 1.2500e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9380 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9366 - learning_rate: 1.2500e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9364 - learning_rate: 1.2500e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9431 - val_loss: 0.9366 - learning_rate: 1.2500e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9408 - val_loss: 0.9361 - learning_rate: 1.2500e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9362 - learning_rate: 1.2500e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9365 - learning_rate: 1.2500e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9433 - val_loss: 0.9361 - learning_rate: 1.2500e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9476 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9387 - val_loss: 0.9361 - learning_rate: 1.2500e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - loss: 0.9408 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9417 - val_loss: 0.9363 - learning_rate: 1.2500e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9362 - learning_rate: 1.2500e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9436 - val_loss: 0.9362 - learning_rate: 6.2500e-05\n",
      "Epoch 238/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9430 - val_loss: 0.9361 - learning_rate: 6.2500e-05\n",
      "Epoch 239/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9396 - val_loss: 0.9362 - learning_rate: 6.2500e-05\n",
      "Epoch 240/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9361 - learning_rate: 6.2500e-05\n",
      "Epoch 241/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9361 - learning_rate: 6.2500e-05\n",
      "Epoch 242/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9362 - learning_rate: 6.2500e-05\n",
      "Epoch 243/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9433 - val_loss: 0.9362 - learning_rate: 6.2500e-05\n",
      "Epoch 244/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9363 - learning_rate: 6.2500e-05\n",
      "Epoch 245/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9362 - learning_rate: 6.2500e-05\n",
      "Epoch 246/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9363 - learning_rate: 6.2500e-05\n",
      "Epoch 247/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9395 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 248/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 249/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9418 - val_loss: 0.9360 - learning_rate: 3.1250e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9395 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9413 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9432 - val_loss: 0.9361 - learning_rate: 3.1250e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9423 - val_loss: 0.9362 - learning_rate: 1.5625e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9407 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9360 - learning_rate: 1.5625e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9362 - learning_rate: 1.5625e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9422 - val_loss: 0.9361 - learning_rate: 1.5625e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9361 - learning_rate: 7.8125e-06\n",
      "Epoch 268/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9361 - learning_rate: 7.8125e-06\n",
      "Epoch 269/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9361 - learning_rate: 7.8125e-06\n",
      "\u001b[1m2438/2438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7864 - loss: 0.6308 - val_accuracy: 0.9328 - val_loss: 0.2507 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8994 - loss: 0.3642 - val_accuracy: 0.9370 - val_loss: 0.2222 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9126 - loss: 0.3050 - val_accuracy: 0.9368 - val_loss: 0.2108 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9217 - loss: 0.2809 - val_accuracy: 0.9387 - val_loss: 0.2049 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9262 - loss: 0.2679 - val_accuracy: 0.9389 - val_loss: 0.2011 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9288 - loss: 0.2568 - val_accuracy: 0.9384 - val_loss: 0.2000 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9313 - loss: 0.2500 - val_accuracy: 0.9397 - val_loss: 0.1974 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9335 - loss: 0.2380 - val_accuracy: 0.9407 - val_loss: 0.1958 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9354 - loss: 0.2374 - val_accuracy: 0.9417 - val_loss: 0.1939 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9358 - loss: 0.2289 - val_accuracy: 0.9421 - val_loss: 0.1907 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9363 - loss: 0.2288 - val_accuracy: 0.9420 - val_loss: 0.1903 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.2224 - val_accuracy: 0.9420 - val_loss: 0.1887 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9387 - loss: 0.2183 - val_accuracy: 0.9438 - val_loss: 0.1862 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.2200 - val_accuracy: 0.9440 - val_loss: 0.1851 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9395 - loss: 0.2141 - val_accuracy: 0.9433 - val_loss: 0.1829 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9396 - loss: 0.2117 - val_accuracy: 0.9444 - val_loss: 0.1836 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.2071 - val_accuracy: 0.9448 - val_loss: 0.1809 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9423 - loss: 0.2039 - val_accuracy: 0.9438 - val_loss: 0.1807 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9395 - loss: 0.2033 - val_accuracy: 0.9436 - val_loss: 0.1784 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9417 - loss: 0.2000 - val_accuracy: 0.9456 - val_loss: 0.1777 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9415 - loss: 0.2014 - val_accuracy: 0.9463 - val_loss: 0.1750 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.1949 - val_accuracy: 0.9457 - val_loss: 0.1744 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.1934 - val_accuracy: 0.9457 - val_loss: 0.1737 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1897 - val_accuracy: 0.9464 - val_loss: 0.1749 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9440 - loss: 0.1866 - val_accuracy: 0.9460 - val_loss: 0.1740 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1891 - val_accuracy: 0.9458 - val_loss: 0.1724 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9456 - loss: 0.1838 - val_accuracy: 0.9467 - val_loss: 0.1711 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9455 - loss: 0.1809 - val_accuracy: 0.9460 - val_loss: 0.1720 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9443 - loss: 0.1825 - val_accuracy: 0.9474 - val_loss: 0.1684 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9455 - loss: 0.1792 - val_accuracy: 0.9467 - val_loss: 0.1692 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9452 - loss: 0.1787 - val_accuracy: 0.9476 - val_loss: 0.1671 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9460 - loss: 0.1767 - val_accuracy: 0.9470 - val_loss: 0.1665 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9477 - loss: 0.1720 - val_accuracy: 0.9479 - val_loss: 0.1687 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9473 - loss: 0.1722 - val_accuracy: 0.9471 - val_loss: 0.1678 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9479 - loss: 0.1719 - val_accuracy: 0.9478 - val_loss: 0.1655 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9474 - loss: 0.1712 - val_accuracy: 0.9468 - val_loss: 0.1671 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9482 - loss: 0.1669 - val_accuracy: 0.9474 - val_loss: 0.1655 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9475 - loss: 0.1711 - val_accuracy: 0.9469 - val_loss: 0.1682 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9493 - loss: 0.1665 - val_accuracy: 0.9473 - val_loss: 0.1610 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9478 - loss: 0.1653 - val_accuracy: 0.9481 - val_loss: 0.1635 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9485 - loss: 0.1657 - val_accuracy: 0.9476 - val_loss: 0.1649 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9487 - loss: 0.1643 - val_accuracy: 0.9484 - val_loss: 0.1622 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1626 - val_accuracy: 0.9495 - val_loss: 0.1624 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9509 - loss: 0.1579 - val_accuracy: 0.9482 - val_loss: 0.1631 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9520 - loss: 0.1556 - val_accuracy: 0.9498 - val_loss: 0.1615 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9526 - loss: 0.1514 - val_accuracy: 0.9500 - val_loss: 0.1578 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.1542 - val_accuracy: 0.9490 - val_loss: 0.1591 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9519 - loss: 0.1542 - val_accuracy: 0.9489 - val_loss: 0.1612 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9518 - loss: 0.1547 - val_accuracy: 0.9504 - val_loss: 0.1606 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1494 - val_accuracy: 0.9494 - val_loss: 0.1575 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.1505 - val_accuracy: 0.9500 - val_loss: 0.1549 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9532 - loss: 0.1471 - val_accuracy: 0.9498 - val_loss: 0.1574 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9521 - loss: 0.1534 - val_accuracy: 0.9504 - val_loss: 0.1568 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1469 - val_accuracy: 0.9503 - val_loss: 0.1577 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9537 - loss: 0.1432 - val_accuracy: 0.9508 - val_loss: 0.1584 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.1432 - val_accuracy: 0.9511 - val_loss: 0.1560 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9548 - loss: 0.1436 - val_accuracy: 0.9513 - val_loss: 0.1564 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9542 - loss: 0.1440 - val_accuracy: 0.9512 - val_loss: 0.1580 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.1424 - val_accuracy: 0.9513 - val_loss: 0.1575 - learning_rate: 5.0000e-05\n",
      "Epoch 60/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.1349 - val_accuracy: 0.9516 - val_loss: 0.1569 - learning_rate: 5.0000e-05\n",
      "Epoch 61/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9575 - loss: 0.1350 - val_accuracy: 0.9512 - val_loss: 0.1574 - learning_rate: 5.0000e-05\n",
      "Epoch 62/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9576 - loss: 0.1343 - val_accuracy: 0.9519 - val_loss: 0.1542 - learning_rate: 5.0000e-05\n",
      "Epoch 63/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9573 - loss: 0.1343 - val_accuracy: 0.9511 - val_loss: 0.1594 - learning_rate: 5.0000e-05\n",
      "Epoch 64/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9604 - loss: 0.1294 - val_accuracy: 0.9510 - val_loss: 0.1572 - learning_rate: 5.0000e-05\n",
      "Epoch 65/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9579 - loss: 0.1316 - val_accuracy: 0.9516 - val_loss: 0.1566 - learning_rate: 5.0000e-05\n",
      "Epoch 66/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9585 - loss: 0.1304 - val_accuracy: 0.9504 - val_loss: 0.1563 - learning_rate: 5.0000e-05\n",
      "Epoch 67/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9589 - loss: 0.1315 - val_accuracy: 0.9515 - val_loss: 0.1541 - learning_rate: 5.0000e-05\n",
      "Epoch 68/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9584 - loss: 0.1305 - val_accuracy: 0.9517 - val_loss: 0.1556 - learning_rate: 5.0000e-05\n",
      "Epoch 69/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1272 - val_accuracy: 0.9511 - val_loss: 0.1551 - learning_rate: 5.0000e-05\n",
      "Epoch 70/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9582 - loss: 0.1308 - val_accuracy: 0.9513 - val_loss: 0.1563 - learning_rate: 5.0000e-05\n",
      "Epoch 71/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.1266 - val_accuracy: 0.9515 - val_loss: 0.1556 - learning_rate: 5.0000e-05\n",
      "Epoch 72/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9598 - loss: 0.1291 - val_accuracy: 0.9520 - val_loss: 0.1567 - learning_rate: 5.0000e-05\n",
      "Epoch 73/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.1259 - val_accuracy: 0.9525 - val_loss: 0.1546 - learning_rate: 5.0000e-05\n",
      "Epoch 74/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1248 - val_accuracy: 0.9517 - val_loss: 0.1550 - learning_rate: 5.0000e-05\n",
      "Epoch 75/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1243 - val_accuracy: 0.9526 - val_loss: 0.1536 - learning_rate: 2.5000e-05\n",
      "Epoch 76/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9593 - loss: 0.1257 - val_accuracy: 0.9523 - val_loss: 0.1538 - learning_rate: 2.5000e-05\n",
      "Epoch 77/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1271 - val_accuracy: 0.9531 - val_loss: 0.1547 - learning_rate: 2.5000e-05\n",
      "Epoch 78/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9605 - loss: 0.1239 - val_accuracy: 0.9531 - val_loss: 0.1556 - learning_rate: 2.5000e-05\n",
      "Epoch 79/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1246 - val_accuracy: 0.9517 - val_loss: 0.1550 - learning_rate: 2.5000e-05\n",
      "Epoch 80/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9604 - loss: 0.1241 - val_accuracy: 0.9519 - val_loss: 0.1549 - learning_rate: 2.5000e-05\n",
      "Epoch 81/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.1196 - val_accuracy: 0.9528 - val_loss: 0.1554 - learning_rate: 2.5000e-05\n",
      "Epoch 82/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9594 - loss: 0.1255 - val_accuracy: 0.9533 - val_loss: 0.1560 - learning_rate: 2.5000e-05\n",
      "Epoch 83/150\n",
      "\u001b[1m184/610\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9581 - loss: 0.1281"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"Unique sample types:\", merged_data['sample_type'].unique())\n",
    "print(\"Sample type distribution:\\n\", merged_data['sample_type'].value_counts())\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "print(\"First few rows of merged data:\\n\", merged_data.head())\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "pca = PCA(n_components=500)  # n_components based on available memory and performance\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "def create_complex_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    x = Dense(2048, activation='relu')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(512, activation='relu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_ensemble_classifier(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def model_1():\n",
    "    # Define the mappings based on sample_type\n",
    "    non_persister_samples = [0]\n",
    "    persister_samples = [\"7\",\"3\",\"14_high\",\"14_med\",\"14_low\"]\n",
    "\n",
    "    # Update labels: 0 = non-persister, 1 = persister\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model1.keras')\n",
    "    encoder.save('complex_encoder_model1.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    classifier.save('classifier_model1.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Non-Persister', 'Persister']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "def model_2():\n",
    "    # Define the mappings based on sample_type\n",
    "    group1_samples = [\"14_high\"]  # Dint divide\n",
    "    group2_samples = [\"14_med\", \"14_low\"]  # Divide\n",
    "\n",
    "    # Update labels: 0 = Group1 (Dint divide), 1 = Group2 (Divide)\n",
    "    y_model2 = np.where(merged_data['sample_type'].isin(group1_samples), 0, 1)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model2:\", np.unique(y_model2, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model2)) != 2:\n",
    "        print(\"Not enough classes in y_model2 for Model 2\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model2)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model2.keras')\n",
    "    encoder.save('complex_encoder_model2.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    classifier.save('classifier_model2.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Group1 (Dint divide)', 'Group2 (Divide)']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "# Execute Model 1\n",
    "model_1()\n",
    "\n",
    "# Execute Model 2\n",
    "model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b90d0-d88f-469e-b539-b99caf535b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - loss: 1.1159 - val_loss: 1.0473 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 1.0088 - val_loss: 1.0067 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 1.0048 - val_loss: 1.0042 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 1.0001 - val_loss: 1.0037 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9974 - val_loss: 1.0035 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9990 - val_loss: 1.0030 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 1.0000 - val_loss: 1.0023 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - loss: 0.9999 - val_loss: 1.0010 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9982 - val_loss: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9951 - val_loss: 0.9973 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9960 - val_loss: 0.9938 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9900 - val_loss: 0.9920 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - loss: 0.9928 - val_loss: 0.9906 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9885 - val_loss: 0.9887 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9837 - val_loss: 0.9870 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9832 - val_loss: 0.9845 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9807 - val_loss: 0.9822 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9816 - val_loss: 0.9803 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9773 - val_loss: 0.9786 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9767 - val_loss: 0.9765 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9731 - val_loss: 0.9755 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9732 - val_loss: 0.9735 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9746 - val_loss: 0.9721 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9773 - val_loss: 0.9714 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9737 - val_loss: 0.9702 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9707 - val_loss: 0.9692 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9712 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9714 - val_loss: 0.9673 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9706 - val_loss: 0.9666 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9662 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9642 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9642 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9661 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9658 - val_loss: 0.9632 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9666 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9631 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9649 - val_loss: 0.9629 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9652 - val_loss: 0.9611 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9633 - val_loss: 0.9609 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9631 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9648 - val_loss: 0.9591 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9642 - val_loss: 0.9596 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9620 - val_loss: 0.9589 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9598 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9631 - val_loss: 0.9576 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9582 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - loss: 0.9584 - val_loss: 0.9567 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9579 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9565 - val_loss: 0.9561 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9597 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9576 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9598 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9574 - val_loss: 0.9554 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9547 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9603 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9574 - val_loss: 0.9537 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9596 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9587 - val_loss: 0.9541 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9563 - val_loss: 0.9523 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9562 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9548 - val_loss: 0.9523 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9538 - val_loss: 0.9520 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9561 - val_loss: 0.9517 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9586 - val_loss: 0.9530 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9550 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9533 - val_loss: 0.9509 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9644 - val_loss: 0.9577 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9608 - val_loss: 0.9548 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9610 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9587 - val_loss: 0.9526 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9578 - val_loss: 0.9526 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9523 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9531 - val_loss: 0.9504 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9568 - val_loss: 0.9505 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9555 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9564 - val_loss: 0.9503 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9514 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - loss: 0.9522 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9570 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9514 - val_loss: 0.9494 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9565 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9530 - val_loss: 0.9482 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9473 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9555 - val_loss: 0.9482 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9537 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9542 - val_loss: 0.9476 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9497 - val_loss: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9500 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9524 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9521 - val_loss: 0.9466 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9458 - val_loss: 0.9466 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9501 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - loss: 0.9516 - val_loss: 0.9454 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9485 - val_loss: 0.9460 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9517 - val_loss: 0.9453 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9505 - val_loss: 0.9460 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9522 - val_loss: 0.9454 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9495 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9465 - val_loss: 0.9448 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9508 - val_loss: 0.9448 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9522 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9531 - val_loss: 0.9445 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9518 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9484 - val_loss: 0.9442 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9533 - val_loss: 0.9438 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9497 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9496 - val_loss: 0.9428 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9481 - val_loss: 0.9439 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9502 - val_loss: 0.9434 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9503 - val_loss: 0.9435 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9493 - val_loss: 0.9425 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9496 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9470 - val_loss: 0.9423 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9496 - val_loss: 0.9426 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9492 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9515 - val_loss: 0.9424 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9471 - val_loss: 0.9424 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9468 - val_loss: 0.9414 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9465 - val_loss: 0.9421 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9421 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9500 - val_loss: 0.9420 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9534 - val_loss: 0.9415 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9455 - val_loss: 0.9413 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9463 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9470 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9429 - val_loss: 0.9413 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9484 - val_loss: 0.9406 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9457 - val_loss: 0.9404 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9465 - val_loss: 0.9404 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9458 - val_loss: 0.9408 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9456 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9437 - val_loss: 0.9404 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9406 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9421 - val_loss: 0.9400 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9477 - val_loss: 0.9405 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9445 - val_loss: 0.9390 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9451 - val_loss: 0.9411 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9441 - val_loss: 0.9401 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9471 - val_loss: 0.9405 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9458 - val_loss: 0.9398 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9450 - val_loss: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9470 - val_loss: 0.9404 - learning_rate: 0.0010\n",
      "Epoch 146/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9441 - val_loss: 0.9398 - learning_rate: 0.0010\n",
      "Epoch 147/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9402 - learning_rate: 0.0010\n",
      "Epoch 148/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9396 - learning_rate: 0.0010\n",
      "Epoch 149/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9485 - val_loss: 0.9389 - learning_rate: 0.0010\n",
      "Epoch 150/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9469 - val_loss: 0.9389 - learning_rate: 0.0010\n",
      "Epoch 151/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9458 - val_loss: 0.9379 - learning_rate: 0.0010\n",
      "Epoch 152/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9452 - val_loss: 0.9390 - learning_rate: 0.0010\n",
      "Epoch 153/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9390 - learning_rate: 0.0010\n",
      "Epoch 154/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9446 - val_loss: 0.9389 - learning_rate: 0.0010\n",
      "Epoch 155/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9467 - val_loss: 0.9388 - learning_rate: 0.0010\n",
      "Epoch 156/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9386 - learning_rate: 0.0010\n",
      "Epoch 157/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9441 - val_loss: 0.9384 - learning_rate: 0.0010\n",
      "Epoch 158/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9451 - val_loss: 0.9386 - learning_rate: 0.0010\n",
      "Epoch 159/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9457 - val_loss: 0.9393 - learning_rate: 0.0010\n",
      "Epoch 160/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9381 - learning_rate: 0.0010\n",
      "Epoch 161/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9473 - val_loss: 0.9377 - learning_rate: 0.0010\n",
      "Epoch 162/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9386 - learning_rate: 0.0010\n",
      "Epoch 163/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9384 - learning_rate: 0.0010\n",
      "Epoch 164/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9431 - val_loss: 0.9380 - learning_rate: 0.0010\n",
      "Epoch 165/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9467 - val_loss: 0.9387 - learning_rate: 0.0010\n",
      "Epoch 166/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9421 - val_loss: 0.9374 - learning_rate: 0.0010\n",
      "Epoch 167/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9444 - val_loss: 0.9378 - learning_rate: 0.0010\n",
      "Epoch 168/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9467 - val_loss: 0.9376 - learning_rate: 0.0010\n",
      "Epoch 169/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9380 - learning_rate: 0.0010\n",
      "Epoch 170/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9385 - learning_rate: 0.0010\n",
      "Epoch 171/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9435 - val_loss: 0.9377 - learning_rate: 0.0010\n",
      "Epoch 172/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9483 - val_loss: 0.9368 - learning_rate: 0.0010\n",
      "Epoch 173/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9420 - val_loss: 0.9366 - learning_rate: 0.0010\n",
      "Epoch 174/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9435 - val_loss: 0.9376 - learning_rate: 0.0010\n",
      "Epoch 175/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9367 - learning_rate: 0.0010\n",
      "Epoch 176/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9433 - val_loss: 0.9370 - learning_rate: 0.0010\n",
      "Epoch 177/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9456 - val_loss: 0.9376 - learning_rate: 0.0010\n",
      "Epoch 178/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9424 - val_loss: 0.9375 - learning_rate: 0.0010\n",
      "Epoch 179/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9377 - learning_rate: 0.0010\n",
      "Epoch 180/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9424 - val_loss: 0.9367 - learning_rate: 0.0010\n",
      "Epoch 181/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9372 - learning_rate: 0.0010\n",
      "Epoch 182/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9436 - val_loss: 0.9371 - learning_rate: 0.0010\n",
      "Epoch 183/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9377 - learning_rate: 0.0010\n",
      "Epoch 184/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9461 - val_loss: 0.9366 - learning_rate: 5.0000e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9482 - val_loss: 0.9367 - learning_rate: 5.0000e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9414 - val_loss: 0.9361 - learning_rate: 5.0000e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9362 - learning_rate: 5.0000e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9445 - val_loss: 0.9357 - learning_rate: 5.0000e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9361 - learning_rate: 5.0000e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9360 - learning_rate: 5.0000e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9438 - val_loss: 0.9351 - learning_rate: 5.0000e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9361 - learning_rate: 5.0000e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9356 - learning_rate: 5.0000e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9356 - learning_rate: 5.0000e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9354 - learning_rate: 5.0000e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9398 - val_loss: 0.9361 - learning_rate: 5.0000e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9355 - learning_rate: 5.0000e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9366 - val_loss: 0.9351 - learning_rate: 5.0000e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9401 - val_loss: 0.9363 - learning_rate: 5.0000e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9424 - val_loss: 0.9359 - learning_rate: 5.0000e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9423 - val_loss: 0.9347 - learning_rate: 5.0000e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9418 - val_loss: 0.9347 - learning_rate: 5.0000e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9441 - val_loss: 0.9354 - learning_rate: 5.0000e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9425 - val_loss: 0.9349 - learning_rate: 5.0000e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9411 - val_loss: 0.9349 - learning_rate: 5.0000e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9383 - val_loss: 0.9351 - learning_rate: 5.0000e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9422 - val_loss: 0.9351 - learning_rate: 5.0000e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9432 - val_loss: 0.9351 - learning_rate: 5.0000e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9441 - val_loss: 0.9347 - learning_rate: 5.0000e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9398 - val_loss: 0.9346 - learning_rate: 5.0000e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9350 - learning_rate: 5.0000e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9352 - learning_rate: 5.0000e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9435 - val_loss: 0.9342 - learning_rate: 5.0000e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9385 - val_loss: 0.9346 - learning_rate: 5.0000e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9387 - val_loss: 0.9348 - learning_rate: 5.0000e-04\n",
      "Epoch 216/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9418 - val_loss: 0.9343 - learning_rate: 5.0000e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9404 - val_loss: 0.9343 - learning_rate: 5.0000e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9404 - val_loss: 0.9340 - learning_rate: 5.0000e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9414 - val_loss: 0.9342 - learning_rate: 5.0000e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9341 - learning_rate: 5.0000e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9370 - val_loss: 0.9336 - learning_rate: 5.0000e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9440 - val_loss: 0.9344 - learning_rate: 5.0000e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9418 - val_loss: 0.9364 - learning_rate: 5.0000e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9409 - val_loss: 0.9348 - learning_rate: 5.0000e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9345 - learning_rate: 5.0000e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9345 - learning_rate: 5.0000e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9411 - val_loss: 0.9343 - learning_rate: 5.0000e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9438 - val_loss: 0.9337 - learning_rate: 5.0000e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9384 - val_loss: 0.9338 - learning_rate: 5.0000e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9343 - learning_rate: 5.0000e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9380 - val_loss: 0.9339 - learning_rate: 5.0000e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9404 - val_loss: 0.9336 - learning_rate: 2.5000e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9357 - val_loss: 0.9342 - learning_rate: 2.5000e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9413 - val_loss: 0.9338 - learning_rate: 2.5000e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9376 - val_loss: 0.9334 - learning_rate: 2.5000e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9385 - val_loss: 0.9339 - learning_rate: 2.5000e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.9443 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9345 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9342 - learning_rate: 2.5000e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9336 - learning_rate: 2.5000e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9438 - val_loss: 0.9332 - learning_rate: 2.5000e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9374 - val_loss: 0.9340 - learning_rate: 2.5000e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9336 - learning_rate: 2.5000e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9332 - learning_rate: 2.5000e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9422 - val_loss: 0.9333 - learning_rate: 2.5000e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9372 - val_loss: 0.9328 - learning_rate: 2.5000e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9422 - val_loss: 0.9329 - learning_rate: 2.5000e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9336 - learning_rate: 2.5000e-04\n",
      "Epoch 250/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9382 - val_loss: 0.9330 - learning_rate: 2.5000e-04\n",
      "Epoch 251/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9333 - learning_rate: 2.5000e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9366 - val_loss: 0.9335 - learning_rate: 2.5000e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9372 - val_loss: 0.9331 - learning_rate: 2.5000e-04\n",
      "Epoch 256/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9331 - learning_rate: 2.5000e-04\n",
      "Epoch 257/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9330 - learning_rate: 2.5000e-04\n",
      "Epoch 258/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9396 - val_loss: 0.9331 - learning_rate: 1.2500e-04\n",
      "Epoch 259/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9432 - val_loss: 0.9330 - learning_rate: 1.2500e-04\n",
      "Epoch 260/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9404 - val_loss: 0.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9330 - learning_rate: 1.2500e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9333 - learning_rate: 1.2500e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9375 - val_loss: 0.9328 - learning_rate: 1.2500e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9434 - val_loss: 0.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9371 - val_loss: 0.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9381 - val_loss: 0.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9329 - learning_rate: 6.2500e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9368 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 270/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 271/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9330 - learning_rate: 6.2500e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9377 - val_loss: 0.9326 - learning_rate: 6.2500e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9396 - val_loss: 0.9326 - learning_rate: 6.2500e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9348 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9413 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9382 - val_loss: 0.9326 - learning_rate: 6.2500e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9328 - learning_rate: 6.2500e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.9410 - val_loss: 0.9326 - learning_rate: 6.2500e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9361 - val_loss: 0.9326 - learning_rate: 6.2500e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9369 - val_loss: 0.9326 - learning_rate: 3.1250e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9342 - val_loss: 0.9327 - learning_rate: 3.1250e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9355 - val_loss: 0.9328 - learning_rate: 3.1250e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9402 - val_loss: 0.9327 - learning_rate: 3.1250e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m119/305\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.9321"
     ]
    }
   ],
   "source": [
    "model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d117fe3-4e68-4ee8-b840-933a7fdd1f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9430 - learning_rate: 5.0000e-04\n",
      "Epoch 131/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9435 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 132/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 42ms/step - loss: 0.9430 - val_loss: 0.9428 - learning_rate: 5.0000e-04\n",
      "Epoch 133/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9386 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 134/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9412 - val_loss: 0.9431 - learning_rate: 5.0000e-04\n",
      "Epoch 135/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9424 - val_loss: 0.9427 - learning_rate: 5.0000e-04\n",
      "Epoch 136/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9460 - val_loss: 0.9426 - learning_rate: 5.0000e-04\n",
      "Epoch 137/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9442 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 138/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9439 - val_loss: 0.9423 - learning_rate: 5.0000e-04\n",
      "Epoch 139/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9456 - val_loss: 0.9421 - learning_rate: 5.0000e-04\n",
      "Epoch 140/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9451 - val_loss: 0.9419 - learning_rate: 5.0000e-04\n",
      "Epoch 141/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9403 - val_loss: 0.9419 - learning_rate: 5.0000e-04\n",
      "Epoch 142/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9462 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 143/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9372 - val_loss: 0.9424 - learning_rate: 5.0000e-04\n",
      "Epoch 144/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9473 - val_loss: 0.9422 - learning_rate: 5.0000e-04\n",
      "Epoch 145/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9431 - val_loss: 0.9417 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9478 - val_loss: 0.9414 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9417 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9428 - val_loss: 0.9417 - learning_rate: 5.0000e-04\n",
      "Epoch 149/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9468 - val_loss: 0.9414 - learning_rate: 5.0000e-04\n",
      "Epoch 150/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9384 - val_loss: 0.9417 - learning_rate: 5.0000e-04\n",
      "Epoch 151/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9408 - val_loss: 0.9421 - learning_rate: 5.0000e-04\n",
      "Epoch 152/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9388 - val_loss: 0.9421 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9445 - val_loss: 0.9414 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9433 - val_loss: 0.9411 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9419 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9416 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9410 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9385 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9430 - val_loss: 0.9411 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9391 - val_loss: 0.9418 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9432 - val_loss: 0.9404 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9409 - val_loss: 0.9401 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9403 - val_loss: 0.9405 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9437 - val_loss: 0.9412 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9410 - val_loss: 0.9408 - learning_rate: 5.0000e-04\n",
      "Epoch 167/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9484 - val_loss: 0.9404 - learning_rate: 5.0000e-04\n",
      "Epoch 168/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9448 - val_loss: 0.9412 - learning_rate: 5.0000e-04\n",
      "Epoch 169/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9431 - val_loss: 0.9411 - learning_rate: 5.0000e-04\n",
      "Epoch 170/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9436 - val_loss: 0.9403 - learning_rate: 5.0000e-04\n",
      "Epoch 171/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9414 - val_loss: 0.9404 - learning_rate: 5.0000e-04\n",
      "Epoch 172/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9406 - learning_rate: 5.0000e-04\n",
      "Epoch 173/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9459 - val_loss: 0.9401 - learning_rate: 5.0000e-04\n",
      "Epoch 174/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9433 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 175/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9387 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 176/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9402 - learning_rate: 2.5000e-04\n",
      "Epoch 177/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9396 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 178/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9374 - val_loss: 0.9405 - learning_rate: 2.5000e-04\n",
      "Epoch 179/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 180/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9393 - val_loss: 0.9398 - learning_rate: 2.5000e-04\n",
      "Epoch 181/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9396 - val_loss: 0.9399 - learning_rate: 2.5000e-04\n",
      "Epoch 182/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9416 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 183/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9383 - val_loss: 0.9395 - learning_rate: 2.5000e-04\n",
      "Epoch 184/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9405 - learning_rate: 2.5000e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9407 - val_loss: 0.9395 - learning_rate: 2.5000e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9349 - val_loss: 0.9394 - learning_rate: 2.5000e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9403 - val_loss: 0.9398 - learning_rate: 2.5000e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9370 - val_loss: 0.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9412 - val_loss: 0.9395 - learning_rate: 2.5000e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9377 - val_loss: 0.9397 - learning_rate: 2.5000e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9347 - val_loss: 0.9396 - learning_rate: 2.5000e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9426 - val_loss: 0.9398 - learning_rate: 2.5000e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9382 - val_loss: 0.9397 - learning_rate: 2.5000e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9432 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9395 - val_loss: 0.9395 - learning_rate: 1.2500e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9371 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9427 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9353 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9423 - val_loss: 0.9390 - learning_rate: 1.2500e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9394 - learning_rate: 1.2500e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9391 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9381 - val_loss: 0.9390 - learning_rate: 1.2500e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9390 - learning_rate: 1.2500e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9383 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9392 - learning_rate: 1.2500e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9391 - learning_rate: 1.2500e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9397 - val_loss: 0.9393 - learning_rate: 1.2500e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9442 - val_loss: 0.9391 - learning_rate: 1.2500e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9390 - learning_rate: 1.2500e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9404 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9376 - val_loss: 0.9389 - learning_rate: 1.2500e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9392 - learning_rate: 1.2500e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9389 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 216/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9408 - val_loss: 0.9390 - learning_rate: 6.2500e-05\n",
      "Epoch 217/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9449 - val_loss: 0.9390 - learning_rate: 6.2500e-05\n",
      "Epoch 218/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9386 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 219/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9383 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 220/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9368 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 221/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 222/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9364 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 223/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9404 - val_loss: 0.9388 - learning_rate: 6.2500e-05\n",
      "Epoch 224/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9362 - val_loss: 0.9388 - learning_rate: 6.2500e-05\n",
      "Epoch 225/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9421 - val_loss: 0.9388 - learning_rate: 6.2500e-05\n",
      "Epoch 226/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9394 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 227/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 228/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9383 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 229/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 230/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9360 - val_loss: 0.9389 - learning_rate: 6.2500e-05\n",
      "Epoch 231/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9391 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 232/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9371 - val_loss: 0.9386 - learning_rate: 6.2500e-05\n",
      "Epoch 233/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9429 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 234/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9385 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 235/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9378 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 236/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9409 - val_loss: 0.9386 - learning_rate: 6.2500e-05\n",
      "Epoch 237/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9405 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 238/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9371 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 239/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9392 - val_loss: 0.9387 - learning_rate: 6.2500e-05\n",
      "Epoch 240/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9407 - val_loss: 0.9385 - learning_rate: 6.2500e-05\n",
      "Epoch 241/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9393 - val_loss: 0.9388 - learning_rate: 6.2500e-05\n",
      "Epoch 242/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9410 - val_loss: 0.9386 - learning_rate: 6.2500e-05\n",
      "Epoch 243/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9447 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 244/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 245/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9399 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 246/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9333 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 247/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9380 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 248/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9392 - val_loss: 0.9385 - learning_rate: 3.1250e-05\n",
      "Epoch 249/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9415 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9337 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - loss: 0.9355 - val_loss: 0.9385 - learning_rate: 3.1250e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9420 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9385 - learning_rate: 3.1250e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9388 - learning_rate: 3.1250e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9335 - val_loss: 0.9387 - learning_rate: 3.1250e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9400 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9384 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9364 - val_loss: 0.9386 - learning_rate: 3.1250e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9351 - val_loss: 0.9386 - learning_rate: 1.5625e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9395 - val_loss: 0.9387 - learning_rate: 1.5625e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9406 - val_loss: 0.9385 - learning_rate: 1.5625e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9365 - val_loss: 0.9387 - learning_rate: 1.5625e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9407 - val_loss: 0.9385 - learning_rate: 1.5625e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9376 - val_loss: 0.9386 - learning_rate: 1.5625e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9355 - val_loss: 0.9386 - learning_rate: 1.5625e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9387 - val_loss: 0.9386 - learning_rate: 1.5625e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9419 - val_loss: 0.9385 - learning_rate: 1.5625e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9395 - val_loss: 0.9386 - learning_rate: 1.5625e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9394 - val_loss: 0.9385 - learning_rate: 7.8125e-06\n",
      "Epoch 270/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9403 - val_loss: 0.9386 - learning_rate: 7.8125e-06\n",
      "Epoch 271/300\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 0.9375 - val_loss: 0.9385 - learning_rate: 7.8125e-06\n",
      "\u001b[1m2655/2655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8172 - loss: 0.5764 - val_accuracy: 0.9593 - val_loss: 0.1707 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9383 - loss: 0.2587 - val_accuracy: 0.9609 - val_loss: 0.1503 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9450 - loss: 0.2267 - val_accuracy: 0.9615 - val_loss: 0.1405 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9516 - loss: 0.1995 - val_accuracy: 0.9625 - val_loss: 0.1348 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.1911 - val_accuracy: 0.9627 - val_loss: 0.1318 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9573 - loss: 0.1728 - val_accuracy: 0.9631 - val_loss: 0.1294 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9605 - loss: 0.1670 - val_accuracy: 0.9635 - val_loss: 0.1256 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9593 - loss: 0.1671 - val_accuracy: 0.9633 - val_loss: 0.1231 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1535 - val_accuracy: 0.9640 - val_loss: 0.1202 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9611 - loss: 0.1509 - val_accuracy: 0.9640 - val_loss: 0.1177 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1469 - val_accuracy: 0.9645 - val_loss: 0.1145 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9635 - loss: 0.1382 - val_accuracy: 0.9648 - val_loss: 0.1109 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.1369 - val_accuracy: 0.9654 - val_loss: 0.1081 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.1365 - val_accuracy: 0.9662 - val_loss: 0.1040 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1299 - val_accuracy: 0.9668 - val_loss: 0.1019 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1272 - val_accuracy: 0.9667 - val_loss: 0.0994 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.1211 - val_accuracy: 0.9671 - val_loss: 0.0974 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.1215 - val_accuracy: 0.9678 - val_loss: 0.0945 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.1154 - val_accuracy: 0.9684 - val_loss: 0.0915 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9666 - loss: 0.1149 - val_accuracy: 0.9698 - val_loss: 0.0882 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1113 - val_accuracy: 0.9693 - val_loss: 0.0884 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9683 - loss: 0.1105 - val_accuracy: 0.9698 - val_loss: 0.0846 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.1088 - val_accuracy: 0.9713 - val_loss: 0.0838 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9687 - loss: 0.1065 - val_accuracy: 0.9714 - val_loss: 0.0831 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.1023 - val_accuracy: 0.9712 - val_loss: 0.0808 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9712 - loss: 0.0960 - val_accuracy: 0.9721 - val_loss: 0.0789 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0962 - val_accuracy: 0.9728 - val_loss: 0.0777 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.0953 - val_accuracy: 0.9741 - val_loss: 0.0760 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0942 - val_accuracy: 0.9747 - val_loss: 0.0744 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0949 - val_accuracy: 0.9752 - val_loss: 0.0744 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9716 - loss: 0.0931 - val_accuracy: 0.9742 - val_loss: 0.0746 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9740 - loss: 0.0876 - val_accuracy: 0.9758 - val_loss: 0.0721 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9740 - loss: 0.0858 - val_accuracy: 0.9760 - val_loss: 0.0724 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0881 - val_accuracy: 0.9765 - val_loss: 0.0699 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0887 - val_accuracy: 0.9765 - val_loss: 0.0702 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0847 - val_accuracy: 0.9769 - val_loss: 0.0697 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9761 - loss: 0.0803 - val_accuracy: 0.9775 - val_loss: 0.0687 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0794 - val_accuracy: 0.9774 - val_loss: 0.0689 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9755 - loss: 0.0827 - val_accuracy: 0.9773 - val_loss: 0.0699 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0809 - val_accuracy: 0.9779 - val_loss: 0.0678 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0811 - val_accuracy: 0.9770 - val_loss: 0.0677 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.0783 - val_accuracy: 0.9789 - val_loss: 0.0651 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9764 - loss: 0.0772 - val_accuracy: 0.9784 - val_loss: 0.0668 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m596/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0750"
     ]
    }
   ],
   "source": [
    "model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ad4aba-8051-4a6a-a66e-c5bde34c3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 05:59:20.057247: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-25 05:59:24.049476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 05:59:49.715367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample types: ['0' '7' '3' '14_high' '14_med' '14_low']\n",
      "Sample type distribution:\n",
      " sample_type\n",
      "3          15338\n",
      "14_high    11571\n",
      "14_med      9931\n",
      "7           8891\n",
      "14_low      7358\n",
      "0           7226\n",
      "Name: count, dtype: int64\n",
      "Merged data shape: (60315, 22169)\n",
      "First few rows of merged data:\n",
      "                cell  0  1  2  3  4  5  6  7  8  ... 22158 22159 22160 22161  \\\n",
      "0  AAACCTGAGACAAGCC  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "1  AAACCTGAGCAGACTG  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "2  AAACCTGAGCGAGAAA  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "3  AAACCTGAGGACAGAA  0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
      "4  AAACCTGAGGCCGAAT  0  0  0  1  0  0  0  0  0  ...     0     0     0     0   \n",
      "\n",
      "  22162 22163 22164 22165 sample_name sample_type  \n",
      "0     0     0     0     0           0           0  \n",
      "1     0     0     0     0           0           0  \n",
      "2     0     0     0     0           0           0  \n",
      "3     0     0     0     0           0           0  \n",
      "4     0     0     0     0           0           0  \n",
      "\n",
      "[5 rows x 22169 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model1: (array([0, 1]), array([ 7226, 53089]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 06:17:45.845859: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "664/664 - 22s - 33ms/step - loss: 1.0328 - val_loss: 1.0142 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9987 - val_loss: 1.0078 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9946 - val_loss: 1.0047 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9921 - val_loss: 1.0017 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9895 - val_loss: 0.9990 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9871 - val_loss: 0.9974 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9853 - val_loss: 0.9935 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9832 - val_loss: 0.9914 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9820 - val_loss: 0.9906 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9804 - val_loss: 0.9886 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9785 - val_loss: 0.9859 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9769 - val_loss: 0.9842 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9751 - val_loss: 0.9819 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9734 - val_loss: 0.9812 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9726 - val_loss: 0.9795 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9716 - val_loss: 0.9780 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9706 - val_loss: 0.9768 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9698 - val_loss: 0.9762 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9689 - val_loss: 0.9754 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9685 - val_loss: 0.9745 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9681 - val_loss: 0.9746 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9667 - val_loss: 0.9726 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9660 - val_loss: 0.9715 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9651 - val_loss: 0.9699 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9650 - val_loss: 0.9700 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9642 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9636 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9629 - val_loss: 0.9681 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9623 - val_loss: 0.9669 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9623 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9620 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9612 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9615 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9607 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9605 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9600 - val_loss: 0.9632 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9594 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9593 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9594 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9590 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9588 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9584 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9585 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9581 - val_loss: 0.9614 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9576 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9572 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9568 - val_loss: 0.9599 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9564 - val_loss: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9562 - val_loss: 0.9590 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9559 - val_loss: 0.9577 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9559 - val_loss: 0.9585 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9557 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9553 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9548 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9546 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9544 - val_loss: 0.9567 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9546 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9548 - val_loss: 0.9563 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9542 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9539 - val_loss: 0.9563 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9537 - val_loss: 0.9562 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9538 - val_loss: 0.9558 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9533 - val_loss: 0.9551 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9530 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9532 - val_loss: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9534 - val_loss: 0.9550 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9530 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9527 - val_loss: 0.9538 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9524 - val_loss: 0.9538 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9521 - val_loss: 0.9539 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9518 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9518 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9520 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9519 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9518 - val_loss: 0.9524 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9518 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9514 - val_loss: 0.9538 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9511 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9510 - val_loss: 0.9536 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9510 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9508 - val_loss: 0.9517 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9505 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9505 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9508 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9507 - val_loss: 0.9523 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9503 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9502 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9500 - val_loss: 0.9504 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9497 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9495 - val_loss: 0.9503 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9496 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9494 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9493 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9492 - val_loss: 0.9510 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9493 - val_loss: 0.9501 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9492 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9490 - val_loss: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9489 - val_loss: 0.9492 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9485 - val_loss: 0.9496 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9487 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9488 - val_loss: 0.9492 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9483 - val_loss: 0.9487 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9484 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9484 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9481 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9482 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9482 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9479 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9476 - val_loss: 0.9483 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9475 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9474 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9472 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9471 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9474 - val_loss: 0.9487 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9473 - val_loss: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9475 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9471 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9471 - val_loss: 0.9466 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9471 - val_loss: 0.9470 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9470 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9463 - val_loss: 0.9475 - learning_rate: 5.0000e-04\n",
      "Epoch 122/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9461 - val_loss: 0.9458 - learning_rate: 5.0000e-04\n",
      "Epoch 123/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9459 - val_loss: 0.9460 - learning_rate: 5.0000e-04\n",
      "Epoch 124/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9455 - val_loss: 0.9461 - learning_rate: 5.0000e-04\n",
      "Epoch 125/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9455 - val_loss: 0.9466 - learning_rate: 5.0000e-04\n",
      "Epoch 126/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9454 - val_loss: 0.9458 - learning_rate: 5.0000e-04\n",
      "Epoch 127/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9456 - val_loss: 0.9457 - learning_rate: 5.0000e-04\n",
      "Epoch 128/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9453 - val_loss: 0.9454 - learning_rate: 5.0000e-04\n",
      "Epoch 129/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9453 - val_loss: 0.9451 - learning_rate: 5.0000e-04\n",
      "Epoch 130/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9453 - val_loss: 0.9454 - learning_rate: 5.0000e-04\n",
      "Epoch 131/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9450 - val_loss: 0.9458 - learning_rate: 5.0000e-04\n",
      "Epoch 132/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9449 - val_loss: 0.9445 - learning_rate: 5.0000e-04\n",
      "Epoch 133/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9449 - val_loss: 0.9450 - learning_rate: 5.0000e-04\n",
      "Epoch 134/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9447 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 135/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9446 - val_loss: 0.9450 - learning_rate: 5.0000e-04\n",
      "Epoch 136/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9446 - val_loss: 0.9455 - learning_rate: 5.0000e-04\n",
      "Epoch 137/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9446 - val_loss: 0.9441 - learning_rate: 5.0000e-04\n",
      "Epoch 138/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9444 - val_loss: 0.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 139/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9445 - val_loss: 0.9453 - learning_rate: 5.0000e-04\n",
      "Epoch 140/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9444 - val_loss: 0.9445 - learning_rate: 5.0000e-04\n",
      "Epoch 141/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9444 - val_loss: 0.9446 - learning_rate: 5.0000e-04\n",
      "Epoch 142/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9441 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 143/300\n",
      "664/664 - 19s - 28ms/step - loss: 0.9442 - val_loss: 0.9435 - learning_rate: 5.0000e-04\n",
      "Epoch 144/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9442 - val_loss: 0.9454 - learning_rate: 5.0000e-04\n",
      "Epoch 145/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9439 - val_loss: 0.9453 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9439 - val_loss: 0.9448 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9439 - val_loss: 0.9454 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9438 - val_loss: 0.9446 - learning_rate: 5.0000e-04\n",
      "Epoch 149/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9438 - val_loss: 0.9441 - learning_rate: 5.0000e-04\n",
      "Epoch 150/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9437 - val_loss: 0.9432 - learning_rate: 5.0000e-04\n",
      "Epoch 151/300\n",
      "664/664 - 20s - 30ms/step - loss: 0.9435 - val_loss: 0.9430 - learning_rate: 5.0000e-04\n",
      "Epoch 152/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9436 - val_loss: 0.9436 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9436 - val_loss: 0.9436 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9436 - val_loss: 0.9438 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9436 - val_loss: 0.9434 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9435 - val_loss: 0.9432 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9436 - val_loss: 0.9428 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9434 - val_loss: 0.9436 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9432 - val_loss: 0.9435 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9431 - val_loss: 0.9434 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9433 - val_loss: 0.9433 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9433 - val_loss: 0.9429 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9433 - val_loss: 0.9427 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9430 - val_loss: 0.9426 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9432 - val_loss: 0.9426 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9430 - val_loss: 0.9434 - learning_rate: 5.0000e-04\n",
      "Epoch 167/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9428 - val_loss: 0.9435 - learning_rate: 5.0000e-04\n",
      "Epoch 168/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9431 - val_loss: 0.9427 - learning_rate: 5.0000e-04\n",
      "Epoch 169/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9431 - val_loss: 0.9428 - learning_rate: 5.0000e-04\n",
      "Epoch 170/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9430 - val_loss: 0.9431 - learning_rate: 5.0000e-04\n",
      "Epoch 171/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9430 - val_loss: 0.9429 - learning_rate: 5.0000e-04\n",
      "Epoch 172/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9430 - val_loss: 0.9431 - learning_rate: 5.0000e-04\n",
      "Epoch 173/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9432 - val_loss: 0.9429 - learning_rate: 5.0000e-04\n",
      "Epoch 174/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9429 - val_loss: 0.9426 - learning_rate: 5.0000e-04\n",
      "Epoch 175/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9427 - val_loss: 0.9419 - learning_rate: 2.5000e-04\n",
      "Epoch 176/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9423 - val_loss: 0.9421 - learning_rate: 2.5000e-04\n",
      "Epoch 177/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9425 - val_loss: 0.9421 - learning_rate: 2.5000e-04\n",
      "Epoch 178/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9423 - val_loss: 0.9416 - learning_rate: 2.5000e-04\n",
      "Epoch 179/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9423 - val_loss: 0.9425 - learning_rate: 2.5000e-04\n",
      "Epoch 180/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9423 - val_loss: 0.9418 - learning_rate: 2.5000e-04\n",
      "Epoch 181/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9422 - val_loss: 0.9418 - learning_rate: 2.5000e-04\n",
      "Epoch 182/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9422 - val_loss: 0.9418 - learning_rate: 2.5000e-04\n",
      "Epoch 183/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9421 - val_loss: 0.9421 - learning_rate: 2.5000e-04\n",
      "Epoch 184/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9420 - val_loss: 0.9418 - learning_rate: 2.5000e-04\n",
      "Epoch 185/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9420 - val_loss: 0.9422 - learning_rate: 2.5000e-04\n",
      "Epoch 186/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9420 - val_loss: 0.9420 - learning_rate: 2.5000e-04\n",
      "Epoch 187/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9419 - val_loss: 0.9421 - learning_rate: 2.5000e-04\n",
      "Epoch 188/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9418 - val_loss: 0.9421 - learning_rate: 2.5000e-04\n",
      "Epoch 189/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9418 - val_loss: 0.9417 - learning_rate: 1.2500e-04\n",
      "Epoch 190/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9418 - val_loss: 0.9416 - learning_rate: 1.2500e-04\n",
      "Epoch 191/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9418 - val_loss: 0.9413 - learning_rate: 1.2500e-04\n",
      "Epoch 192/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9417 - val_loss: 0.9416 - learning_rate: 1.2500e-04\n",
      "Epoch 193/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9416 - val_loss: 0.9415 - learning_rate: 1.2500e-04\n",
      "Epoch 194/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9416 - val_loss: 0.9414 - learning_rate: 1.2500e-04\n",
      "Epoch 195/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9415 - val_loss: 0.9417 - learning_rate: 1.2500e-04\n",
      "Epoch 196/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9415 - val_loss: 0.9416 - learning_rate: 1.2500e-04\n",
      "Epoch 197/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9415 - val_loss: 0.9415 - learning_rate: 1.2500e-04\n",
      "Epoch 198/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9421 - learning_rate: 1.2500e-04\n",
      "Epoch 199/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9415 - val_loss: 0.9416 - learning_rate: 1.2500e-04\n",
      "Epoch 200/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9415 - learning_rate: 1.2500e-04\n",
      "Epoch 201/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9417 - learning_rate: 1.2500e-04\n",
      "Epoch 202/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9414 - learning_rate: 6.2500e-05\n",
      "Epoch 203/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9416 - learning_rate: 6.2500e-05\n",
      "Epoch 204/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9414 - val_loss: 0.9413 - learning_rate: 6.2500e-05\n",
      "Epoch 205/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9414 - learning_rate: 6.2500e-05\n",
      "Epoch 206/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9415 - learning_rate: 6.2500e-05\n",
      "Epoch 207/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9413 - val_loss: 0.9411 - learning_rate: 6.2500e-05\n",
      "Epoch 208/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9413 - val_loss: 0.9415 - learning_rate: 6.2500e-05\n",
      "Epoch 209/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9411 - val_loss: 0.9411 - learning_rate: 6.2500e-05\n",
      "Epoch 210/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9413 - val_loss: 0.9414 - learning_rate: 6.2500e-05\n",
      "Epoch 211/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9412 - learning_rate: 6.2500e-05\n",
      "Epoch 212/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9413 - learning_rate: 6.2500e-05\n",
      "Epoch 213/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9413 - learning_rate: 6.2500e-05\n",
      "Epoch 214/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9416 - learning_rate: 6.2500e-05\n",
      "Epoch 215/300\n",
      "664/664 - 25s - 37ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 6.2500e-05\n",
      "Epoch 216/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9411 - learning_rate: 6.2500e-05\n",
      "Epoch 217/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9412 - learning_rate: 6.2500e-05\n",
      "Epoch 218/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 219/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 220/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9413 - learning_rate: 3.1250e-05\n",
      "Epoch 221/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 222/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9413 - learning_rate: 3.1250e-05\n",
      "Epoch 223/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 224/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 225/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 226/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 3.1250e-05\n",
      "Epoch 227/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9409 - val_loss: 0.9411 - learning_rate: 3.1250e-05\n",
      "Epoch 228/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9410 - learning_rate: 1.5625e-05\n",
      "Epoch 229/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9409 - val_loss: 0.9412 - learning_rate: 1.5625e-05\n",
      "Epoch 230/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 1.5625e-05\n",
      "Epoch 231/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9411 - learning_rate: 1.5625e-05\n",
      "Epoch 232/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 1.5625e-05\n",
      "Epoch 233/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9412 - val_loss: 0.9411 - learning_rate: 1.5625e-05\n",
      "Epoch 234/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9411 - learning_rate: 1.5625e-05\n",
      "Epoch 235/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 1.5625e-05\n",
      "Epoch 236/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 1.5625e-05\n",
      "Epoch 237/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 1.5625e-05\n",
      "Epoch 238/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 1.5625e-05\n",
      "Epoch 239/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9412 - learning_rate: 7.8125e-06\n",
      "Epoch 240/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 241/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9409 - val_loss: 0.9412 - learning_rate: 7.8125e-06\n",
      "Epoch 242/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9412 - learning_rate: 7.8125e-06\n",
      "Epoch 243/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 244/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 245/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 246/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9409 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 247/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9411 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "Epoch 248/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9410 - val_loss: 0.9411 - learning_rate: 7.8125e-06\n",
      "\u001b[1m2655/2655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "664/664 - 5s - 8ms/step - accuracy: 0.8973 - loss: 0.3697 - val_accuracy: 0.9589 - val_loss: 0.1688 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9448 - loss: 0.2348 - val_accuracy: 0.9623 - val_loss: 0.1480 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9497 - loss: 0.2085 - val_accuracy: 0.9628 - val_loss: 0.1402 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9549 - loss: 0.1916 - val_accuracy: 0.9625 - val_loss: 0.1350 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9570 - loss: 0.1807 - val_accuracy: 0.9631 - val_loss: 0.1315 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9594 - loss: 0.1719 - val_accuracy: 0.9632 - val_loss: 0.1296 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9605 - loss: 0.1647 - val_accuracy: 0.9633 - val_loss: 0.1270 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9618 - loss: 0.1597 - val_accuracy: 0.9630 - val_loss: 0.1257 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9618 - loss: 0.1540 - val_accuracy: 0.9635 - val_loss: 0.1218 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9624 - loss: 0.1502 - val_accuracy: 0.9640 - val_loss: 0.1206 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9626 - loss: 0.1449 - val_accuracy: 0.9646 - val_loss: 0.1159 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9634 - loss: 0.1413 - val_accuracy: 0.9643 - val_loss: 0.1124 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9639 - loss: 0.1378 - val_accuracy: 0.9658 - val_loss: 0.1111 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9644 - loss: 0.1328 - val_accuracy: 0.9662 - val_loss: 0.1063 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9641 - loss: 0.1321 - val_accuracy: 0.9666 - val_loss: 0.1049 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9645 - loss: 0.1297 - val_accuracy: 0.9666 - val_loss: 0.1027 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9650 - loss: 0.1242 - val_accuracy: 0.9671 - val_loss: 0.0997 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9656 - loss: 0.1217 - val_accuracy: 0.9673 - val_loss: 0.0978 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9657 - loss: 0.1194 - val_accuracy: 0.9684 - val_loss: 0.0961 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9660 - loss: 0.1172 - val_accuracy: 0.9692 - val_loss: 0.0943 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9668 - loss: 0.1144 - val_accuracy: 0.9695 - val_loss: 0.0911 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9672 - loss: 0.1122 - val_accuracy: 0.9709 - val_loss: 0.0904 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9680 - loss: 0.1100 - val_accuracy: 0.9706 - val_loss: 0.0887 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9687 - loss: 0.1071 - val_accuracy: 0.9700 - val_loss: 0.0877 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9686 - loss: 0.1061 - val_accuracy: 0.9702 - val_loss: 0.0879 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9685 - loss: 0.1059 - val_accuracy: 0.9698 - val_loss: 0.0865 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9697 - loss: 0.1017 - val_accuracy: 0.9707 - val_loss: 0.0855 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9698 - loss: 0.1005 - val_accuracy: 0.9710 - val_loss: 0.0839 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9695 - loss: 0.0994 - val_accuracy: 0.9712 - val_loss: 0.0824 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9701 - loss: 0.0997 - val_accuracy: 0.9716 - val_loss: 0.0830 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9704 - loss: 0.0968 - val_accuracy: 0.9723 - val_loss: 0.0812 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9711 - loss: 0.0950 - val_accuracy: 0.9717 - val_loss: 0.0815 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9720 - loss: 0.0937 - val_accuracy: 0.9734 - val_loss: 0.0797 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9721 - loss: 0.0917 - val_accuracy: 0.9741 - val_loss: 0.0786 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9721 - loss: 0.0914 - val_accuracy: 0.9735 - val_loss: 0.0803 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9721 - loss: 0.0909 - val_accuracy: 0.9754 - val_loss: 0.0773 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9726 - loss: 0.0889 - val_accuracy: 0.9745 - val_loss: 0.0778 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9723 - loss: 0.0899 - val_accuracy: 0.9751 - val_loss: 0.0773 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9731 - loss: 0.0871 - val_accuracy: 0.9759 - val_loss: 0.0761 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9740 - loss: 0.0853 - val_accuracy: 0.9748 - val_loss: 0.0760 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9735 - loss: 0.0849 - val_accuracy: 0.9759 - val_loss: 0.0755 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9737 - loss: 0.0863 - val_accuracy: 0.9760 - val_loss: 0.0758 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9752 - loss: 0.0812 - val_accuracy: 0.9765 - val_loss: 0.0740 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9754 - loss: 0.0809 - val_accuracy: 0.9766 - val_loss: 0.0726 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9749 - loss: 0.0817 - val_accuracy: 0.9763 - val_loss: 0.0742 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9762 - loss: 0.0778 - val_accuracy: 0.9763 - val_loss: 0.0753 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9758 - loss: 0.0792 - val_accuracy: 0.9764 - val_loss: 0.0725 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9754 - loss: 0.0790 - val_accuracy: 0.9762 - val_loss: 0.0725 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9770 - loss: 0.0758 - val_accuracy: 0.9777 - val_loss: 0.0708 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9771 - loss: 0.0762 - val_accuracy: 0.9783 - val_loss: 0.0687 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9767 - loss: 0.0755 - val_accuracy: 0.9787 - val_loss: 0.0692 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9765 - loss: 0.0760 - val_accuracy: 0.9789 - val_loss: 0.0679 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9779 - loss: 0.0719 - val_accuracy: 0.9788 - val_loss: 0.0675 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9777 - loss: 0.0724 - val_accuracy: 0.9786 - val_loss: 0.0683 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9777 - loss: 0.0718 - val_accuracy: 0.9788 - val_loss: 0.0680 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9783 - loss: 0.0702 - val_accuracy: 0.9784 - val_loss: 0.0681 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9779 - loss: 0.0705 - val_accuracy: 0.9782 - val_loss: 0.0697 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9779 - loss: 0.0706 - val_accuracy: 0.9781 - val_loss: 0.0691 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9786 - loss: 0.0703 - val_accuracy: 0.9784 - val_loss: 0.0693 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9786 - loss: 0.0695 - val_accuracy: 0.9782 - val_loss: 0.0692 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9795 - loss: 0.0674 - val_accuracy: 0.9789 - val_loss: 0.0672 - learning_rate: 5.0000e-05\n",
      "Epoch 62/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9800 - loss: 0.0653 - val_accuracy: 0.9795 - val_loss: 0.0676 - learning_rate: 5.0000e-05\n",
      "Epoch 63/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9803 - loss: 0.0647 - val_accuracy: 0.9797 - val_loss: 0.0678 - learning_rate: 5.0000e-05\n",
      "Epoch 64/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9803 - loss: 0.0643 - val_accuracy: 0.9794 - val_loss: 0.0677 - learning_rate: 5.0000e-05\n",
      "Epoch 65/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9809 - loss: 0.0631 - val_accuracy: 0.9796 - val_loss: 0.0678 - learning_rate: 5.0000e-05\n",
      "Epoch 66/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9806 - loss: 0.0637 - val_accuracy: 0.9798 - val_loss: 0.0664 - learning_rate: 5.0000e-05\n",
      "Epoch 67/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9807 - loss: 0.0624 - val_accuracy: 0.9796 - val_loss: 0.0680 - learning_rate: 5.0000e-05\n",
      "Epoch 68/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9810 - loss: 0.0620 - val_accuracy: 0.9805 - val_loss: 0.0664 - learning_rate: 5.0000e-05\n",
      "Epoch 69/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9815 - loss: 0.0613 - val_accuracy: 0.9797 - val_loss: 0.0686 - learning_rate: 5.0000e-05\n",
      "Epoch 70/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9814 - loss: 0.0613 - val_accuracy: 0.9797 - val_loss: 0.0669 - learning_rate: 5.0000e-05\n",
      "Epoch 71/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9814 - loss: 0.0601 - val_accuracy: 0.9791 - val_loss: 0.0676 - learning_rate: 5.0000e-05\n",
      "Epoch 72/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9812 - loss: 0.0614 - val_accuracy: 0.9798 - val_loss: 0.0663 - learning_rate: 5.0000e-05\n",
      "Epoch 73/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9816 - loss: 0.0609 - val_accuracy: 0.9802 - val_loss: 0.0663 - learning_rate: 5.0000e-05\n",
      "Epoch 74/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9817 - loss: 0.0604 - val_accuracy: 0.9809 - val_loss: 0.0661 - learning_rate: 5.0000e-05\n",
      "Epoch 75/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9820 - loss: 0.0583 - val_accuracy: 0.9805 - val_loss: 0.0669 - learning_rate: 5.0000e-05\n",
      "Epoch 76/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9816 - loss: 0.0605 - val_accuracy: 0.9801 - val_loss: 0.0666 - learning_rate: 5.0000e-05\n",
      "Epoch 77/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9820 - loss: 0.0577 - val_accuracy: 0.9798 - val_loss: 0.0693 - learning_rate: 5.0000e-05\n",
      "Epoch 78/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9818 - loss: 0.0594 - val_accuracy: 0.9803 - val_loss: 0.0668 - learning_rate: 5.0000e-05\n",
      "Epoch 79/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9818 - loss: 0.0581 - val_accuracy: 0.9803 - val_loss: 0.0671 - learning_rate: 5.0000e-05\n",
      "Epoch 80/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9823 - loss: 0.0577 - val_accuracy: 0.9804 - val_loss: 0.0667 - learning_rate: 5.0000e-05\n",
      "Epoch 81/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9816 - loss: 0.0587 - val_accuracy: 0.9809 - val_loss: 0.0661 - learning_rate: 5.0000e-05\n",
      "Epoch 82/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9823 - loss: 0.0567 - val_accuracy: 0.9803 - val_loss: 0.0692 - learning_rate: 2.5000e-05\n",
      "Epoch 83/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9829 - loss: 0.0560 - val_accuracy: 0.9798 - val_loss: 0.0684 - learning_rate: 2.5000e-05\n",
      "Epoch 84/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9828 - loss: 0.0556 - val_accuracy: 0.9803 - val_loss: 0.0681 - learning_rate: 2.5000e-05\n",
      "Epoch 85/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9829 - loss: 0.0551 - val_accuracy: 0.9807 - val_loss: 0.0679 - learning_rate: 2.5000e-05\n",
      "Epoch 86/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9834 - loss: 0.0547 - val_accuracy: 0.9802 - val_loss: 0.0688 - learning_rate: 2.5000e-05\n",
      "Epoch 87/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9829 - loss: 0.0541 - val_accuracy: 0.9802 - val_loss: 0.0693 - learning_rate: 2.5000e-05\n",
      "Epoch 88/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9830 - loss: 0.0547 - val_accuracy: 0.9809 - val_loss: 0.0690 - learning_rate: 2.5000e-05\n",
      "Epoch 89/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9835 - loss: 0.0541 - val_accuracy: 0.9809 - val_loss: 0.0696 - learning_rate: 1.2500e-05\n",
      "Epoch 90/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9834 - loss: 0.0541 - val_accuracy: 0.9809 - val_loss: 0.0692 - learning_rate: 1.2500e-05\n",
      "Epoch 91/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9836 - loss: 0.0541 - val_accuracy: 0.9811 - val_loss: 0.0690 - learning_rate: 1.2500e-05\n",
      "Epoch 92/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9837 - loss: 0.0540 - val_accuracy: 0.9810 - val_loss: 0.0686 - learning_rate: 1.2500e-05\n",
      "Epoch 93/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9835 - loss: 0.0531 - val_accuracy: 0.9809 - val_loss: 0.0691 - learning_rate: 1.2500e-05\n",
      "Epoch 94/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9838 - loss: 0.0524 - val_accuracy: 0.9807 - val_loss: 0.0702 - learning_rate: 1.2500e-05\n",
      "Epoch 95/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9846 - loss: 0.0518 - val_accuracy: 0.9810 - val_loss: 0.0697 - learning_rate: 1.2500e-05\n",
      "Epoch 96/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9840 - loss: 0.0523 - val_accuracy: 0.9806 - val_loss: 0.0695 - learning_rate: 6.2500e-06\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.9781 - loss: 0.0775\n",
      "Test Accuracy: 0.9810698628425598\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Persister       0.98      0.98      0.98      5255\n",
      "    Persister       0.98      0.98      0.98      5363\n",
      "\n",
      "     accuracy                           0.98     10618\n",
      "    macro avg       0.98      0.98      0.98     10618\n",
      " weighted avg       0.98      0.98      0.98     10618\n",
      "\n",
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/300\n",
      "610/610 - 20s - 33ms/step - loss: 1.0369 - val_loss: 1.0055 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9996 - val_loss: 1.0032 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9984 - val_loss: 1.0017 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9966 - val_loss: 0.9994 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9946 - val_loss: 0.9963 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9917 - val_loss: 0.9931 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9888 - val_loss: 0.9899 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9859 - val_loss: 0.9863 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9833 - val_loss: 0.9833 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9817 - val_loss: 0.9811 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9796 - val_loss: 0.9785 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9780 - val_loss: 0.9765 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9759 - val_loss: 0.9752 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9745 - val_loss: 0.9725 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9732 - val_loss: 0.9706 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9717 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9703 - val_loss: 0.9679 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9694 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9680 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9672 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9665 - val_loss: 0.9632 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9654 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9648 - val_loss: 0.9614 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9642 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9638 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9631 - val_loss: 0.9591 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9625 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9624 - val_loss: 0.9579 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9623 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9623 - val_loss: 0.9587 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9617 - val_loss: 0.9570 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9617 - val_loss: 0.9575 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9608 - val_loss: 0.9564 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9601 - val_loss: 0.9561 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9604 - val_loss: 0.9558 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9599 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9596 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9597 - val_loss: 0.9547 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9592 - val_loss: 0.9551 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9591 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9586 - val_loss: 0.9538 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9584 - val_loss: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9580 - val_loss: 0.9531 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9576 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9572 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9573 - val_loss: 0.9526 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9573 - val_loss: 0.9519 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9571 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9569 - val_loss: 0.9516 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9563 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9563 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9563 - val_loss: 0.9510 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9556 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9553 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9551 - val_loss: 0.9499 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9547 - val_loss: 0.9501 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9545 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9545 - val_loss: 0.9496 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9541 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9539 - val_loss: 0.9487 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9536 - val_loss: 0.9492 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9534 - val_loss: 0.9489 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9533 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9530 - val_loss: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9529 - val_loss: 0.9477 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9528 - val_loss: 0.9472 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9524 - val_loss: 0.9482 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9526 - val_loss: 0.9468 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9522 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9518 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9517 - val_loss: 0.9461 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9515 - val_loss: 0.9465 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9515 - val_loss: 0.9460 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9512 - val_loss: 0.9450 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9508 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9508 - val_loss: 0.9454 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9507 - val_loss: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9507 - val_loss: 0.9450 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9504 - val_loss: 0.9438 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9503 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9505 - val_loss: 0.9457 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9500 - val_loss: 0.9444 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9500 - val_loss: 0.9443 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9501 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9499 - val_loss: 0.9444 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9497 - val_loss: 0.9453 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9497 - val_loss: 0.9447 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9494 - val_loss: 0.9438 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9492 - val_loss: 0.9424 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9490 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9493 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9492 - val_loss: 0.9431 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9493 - val_loss: 0.9429 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9489 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9490 - val_loss: 0.9428 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9486 - val_loss: 0.9431 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9483 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9483 - val_loss: 0.9429 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9486 - val_loss: 0.9432 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9484 - val_loss: 0.9429 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9483 - val_loss: 0.9427 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9482 - val_loss: 0.9413 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9480 - val_loss: 0.9423 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9480 - val_loss: 0.9430 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9482 - val_loss: 0.9424 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9477 - val_loss: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9478 - val_loss: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9475 - val_loss: 0.9418 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9476 - val_loss: 0.9418 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9477 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9474 - val_loss: 0.9407 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9475 - val_loss: 0.9415 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9477 - val_loss: 0.9416 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9473 - val_loss: 0.9423 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9471 - val_loss: 0.9409 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9473 - val_loss: 0.9409 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9474 - val_loss: 0.9414 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9472 - val_loss: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9472 - val_loss: 0.9419 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9472 - val_loss: 0.9407 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9471 - val_loss: 0.9419 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9464 - val_loss: 0.9405 - learning_rate: 5.0000e-04\n",
      "Epoch 123/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9460 - val_loss: 0.9395 - learning_rate: 5.0000e-04\n",
      "Epoch 124/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9460 - val_loss: 0.9402 - learning_rate: 5.0000e-04\n",
      "Epoch 125/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9460 - val_loss: 0.9398 - learning_rate: 5.0000e-04\n",
      "Epoch 126/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9458 - val_loss: 0.9391 - learning_rate: 5.0000e-04\n",
      "Epoch 127/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9458 - val_loss: 0.9396 - learning_rate: 5.0000e-04\n",
      "Epoch 128/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9456 - val_loss: 0.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 129/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9457 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 130/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9454 - val_loss: 0.9396 - learning_rate: 5.0000e-04\n",
      "Epoch 131/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9456 - val_loss: 0.9407 - learning_rate: 5.0000e-04\n",
      "Epoch 132/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9454 - val_loss: 0.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 133/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9452 - val_loss: 0.9386 - learning_rate: 5.0000e-04\n",
      "Epoch 134/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9454 - val_loss: 0.9390 - learning_rate: 5.0000e-04\n",
      "Epoch 135/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9452 - val_loss: 0.9389 - learning_rate: 5.0000e-04\n",
      "Epoch 136/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9450 - val_loss: 0.9389 - learning_rate: 5.0000e-04\n",
      "Epoch 137/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9451 - val_loss: 0.9385 - learning_rate: 5.0000e-04\n",
      "Epoch 138/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9451 - val_loss: 0.9393 - learning_rate: 5.0000e-04\n",
      "Epoch 139/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9449 - val_loss: 0.9388 - learning_rate: 5.0000e-04\n",
      "Epoch 140/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9448 - val_loss: 0.9389 - learning_rate: 5.0000e-04\n",
      "Epoch 141/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9447 - val_loss: 0.9389 - learning_rate: 5.0000e-04\n",
      "Epoch 142/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9447 - val_loss: 0.9393 - learning_rate: 5.0000e-04\n",
      "Epoch 143/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9449 - val_loss: 0.9419 - learning_rate: 5.0000e-04\n",
      "Epoch 144/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9447 - val_loss: 0.9387 - learning_rate: 5.0000e-04\n",
      "Epoch 145/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9447 - val_loss: 0.9390 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9447 - val_loss: 0.9393 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9447 - val_loss: 0.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9443 - val_loss: 0.9386 - learning_rate: 2.5000e-04\n",
      "Epoch 149/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9445 - val_loss: 0.9387 - learning_rate: 2.5000e-04\n",
      "Epoch 150/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9443 - val_loss: 0.9387 - learning_rate: 2.5000e-04\n",
      "Epoch 151/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9442 - val_loss: 0.9384 - learning_rate: 2.5000e-04\n",
      "Epoch 152/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9441 - val_loss: 0.9386 - learning_rate: 2.5000e-04\n",
      "Epoch 153/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9440 - val_loss: 0.9385 - learning_rate: 2.5000e-04\n",
      "Epoch 154/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9439 - val_loss: 0.9387 - learning_rate: 2.5000e-04\n",
      "Epoch 155/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9442 - val_loss: 0.9384 - learning_rate: 2.5000e-04\n",
      "Epoch 156/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9440 - val_loss: 0.9379 - learning_rate: 2.5000e-04\n",
      "Epoch 157/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9439 - val_loss: 0.9387 - learning_rate: 2.5000e-04\n",
      "Epoch 158/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9440 - val_loss: 0.9385 - learning_rate: 2.5000e-04\n",
      "Epoch 159/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9439 - val_loss: 0.9384 - learning_rate: 2.5000e-04\n",
      "Epoch 160/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9438 - val_loss: 0.9378 - learning_rate: 2.5000e-04\n",
      "Epoch 161/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9436 - val_loss: 0.9385 - learning_rate: 2.5000e-04\n",
      "Epoch 162/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9437 - val_loss: 0.9386 - learning_rate: 2.5000e-04\n",
      "Epoch 163/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9436 - val_loss: 0.9382 - learning_rate: 2.5000e-04\n",
      "Epoch 164/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9437 - val_loss: 0.9379 - learning_rate: 2.5000e-04\n",
      "Epoch 165/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9435 - val_loss: 0.9379 - learning_rate: 2.5000e-04\n",
      "Epoch 166/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9436 - val_loss: 0.9379 - learning_rate: 2.5000e-04\n",
      "Epoch 167/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9433 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 168/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9433 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 169/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9432 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 170/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9431 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 171/300\n",
      "610/610 - 18s - 29ms/step - loss: 0.9432 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 172/300\n",
      "610/610 - 18s - 29ms/step - loss: 0.9433 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 173/300\n",
      "610/610 - 18s - 29ms/step - loss: 0.9432 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 174/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9431 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 175/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9430 - val_loss: 0.9375 - learning_rate: 1.2500e-04\n",
      "Epoch 176/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9433 - val_loss: 0.9374 - learning_rate: 1.2500e-04\n",
      "Epoch 177/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9430 - val_loss: 0.9375 - learning_rate: 1.2500e-04\n",
      "Epoch 178/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9430 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 179/300\n",
      "610/610 - 19s - 32ms/step - loss: 0.9431 - val_loss: 0.9376 - learning_rate: 1.2500e-04\n",
      "Epoch 180/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9429 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 181/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9431 - val_loss: 0.9379 - learning_rate: 1.2500e-04\n",
      "Epoch 182/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9431 - val_loss: 0.9377 - learning_rate: 1.2500e-04\n",
      "Epoch 183/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9429 - val_loss: 0.9374 - learning_rate: 1.2500e-04\n",
      "Epoch 184/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9428 - val_loss: 0.9374 - learning_rate: 1.2500e-04\n",
      "Epoch 185/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9431 - val_loss: 0.9378 - learning_rate: 1.2500e-04\n",
      "Epoch 186/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9427 - val_loss: 0.9374 - learning_rate: 1.2500e-04\n",
      "Epoch 187/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9429 - val_loss: 0.9377 - learning_rate: 6.2500e-05\n",
      "Epoch 188/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9426 - val_loss: 0.9375 - learning_rate: 6.2500e-05\n",
      "Epoch 189/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9427 - val_loss: 0.9375 - learning_rate: 6.2500e-05\n",
      "Epoch 190/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9428 - val_loss: 0.9373 - learning_rate: 6.2500e-05\n",
      "Epoch 191/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9426 - val_loss: 0.9375 - learning_rate: 6.2500e-05\n",
      "Epoch 192/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 6.2500e-05\n",
      "Epoch 193/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9429 - val_loss: 0.9371 - learning_rate: 6.2500e-05\n",
      "Epoch 194/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9425 - val_loss: 0.9373 - learning_rate: 6.2500e-05\n",
      "Epoch 195/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9427 - val_loss: 0.9372 - learning_rate: 6.2500e-05\n",
      "Epoch 196/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9372 - learning_rate: 6.2500e-05\n",
      "Epoch 197/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9427 - val_loss: 0.9371 - learning_rate: 6.2500e-05\n",
      "Epoch 198/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9426 - val_loss: 0.9375 - learning_rate: 6.2500e-05\n",
      "Epoch 199/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9374 - learning_rate: 6.2500e-05\n",
      "Epoch 200/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 6.2500e-05\n",
      "Epoch 201/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 6.2500e-05\n",
      "Epoch 202/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9425 - val_loss: 0.9373 - learning_rate: 6.2500e-05\n",
      "Epoch 203/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 3.1250e-05\n",
      "Epoch 204/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9372 - learning_rate: 3.1250e-05\n",
      "Epoch 205/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9425 - val_loss: 0.9370 - learning_rate: 3.1250e-05\n",
      "Epoch 206/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9372 - learning_rate: 3.1250e-05\n",
      "Epoch 207/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 3.1250e-05\n",
      "Epoch 208/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9427 - val_loss: 0.9371 - learning_rate: 3.1250e-05\n",
      "Epoch 209/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 3.1250e-05\n",
      "Epoch 210/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9425 - val_loss: 0.9373 - learning_rate: 3.1250e-05\n",
      "Epoch 211/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9423 - val_loss: 0.9372 - learning_rate: 3.1250e-05\n",
      "Epoch 212/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 3.1250e-05\n",
      "Epoch 213/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9373 - learning_rate: 3.1250e-05\n",
      "Epoch 214/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9426 - val_loss: 0.9372 - learning_rate: 3.1250e-05\n",
      "Epoch 215/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 3.1250e-05\n",
      "Epoch 216/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 217/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9370 - learning_rate: 1.5625e-05\n",
      "Epoch 218/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 219/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9425 - val_loss: 0.9372 - learning_rate: 1.5625e-05\n",
      "Epoch 220/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 221/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 222/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.5625e-05\n",
      "Epoch 223/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 224/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 1.5625e-05\n",
      "Epoch 225/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9422 - val_loss: 0.9370 - learning_rate: 1.5625e-05\n",
      "Epoch 226/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 227/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 228/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 7.8125e-06\n",
      "Epoch 229/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 230/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9423 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 231/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9422 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 232/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9372 - learning_rate: 7.8125e-06\n",
      "Epoch 233/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 234/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 7.8125e-06\n",
      "Epoch 235/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 7.8125e-06\n",
      "Epoch 236/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9423 - val_loss: 0.9370 - learning_rate: 3.9063e-06\n",
      "Epoch 237/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 238/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 239/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 3.9063e-06\n",
      "Epoch 240/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 241/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 242/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 243/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 3.9063e-06\n",
      "Epoch 244/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9422 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 245/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 3.9063e-06\n",
      "Epoch 246/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9370 - learning_rate: 1.9531e-06\n",
      "Epoch 247/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.9531e-06\n",
      "Epoch 248/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.9531e-06\n",
      "Epoch 249/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9425 - val_loss: 0.9370 - learning_rate: 1.9531e-06\n",
      "Epoch 250/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9423 - val_loss: 0.9371 - learning_rate: 1.9531e-06\n",
      "Epoch 251/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 1.9531e-06\n",
      "Epoch 252/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.9531e-06\n",
      "Epoch 253/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.9531e-06\n",
      "Epoch 254/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9369 - learning_rate: 1.9531e-06\n",
      "Epoch 255/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9423 - val_loss: 0.9370 - learning_rate: 1.9531e-06\n",
      "Epoch 256/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 257/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 258/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9423 - val_loss: 0.9372 - learning_rate: 1.0000e-06\n",
      "Epoch 259/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 1.0000e-06\n",
      "Epoch 260/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 261/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 262/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 263/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 264/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9422 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 265/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9423 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 266/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 267/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 268/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 269/300\n",
      "610/610 - 19s - 31ms/step - loss: 0.9424 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 270/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9423 - val_loss: 0.9370 - learning_rate: 1.0000e-06\n",
      "Epoch 271/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9426 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 272/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9425 - val_loss: 0.9373 - learning_rate: 1.0000e-06\n",
      "Epoch 273/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9371 - learning_rate: 1.0000e-06\n",
      "Epoch 274/300\n",
      "610/610 - 19s - 30ms/step - loss: 0.9424 - val_loss: 0.9372 - learning_rate: 1.0000e-06\n",
      "\u001b[1m2438/2438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "610/610 - 5s - 8ms/step - accuracy: 0.8400 - loss: 0.5264 - val_accuracy: 0.9311 - val_loss: 0.2575 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.8976 - loss: 0.3631 - val_accuracy: 0.9338 - val_loss: 0.2297 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9092 - loss: 0.3194 - val_accuracy: 0.9358 - val_loss: 0.2163 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9190 - loss: 0.2868 - val_accuracy: 0.9368 - val_loss: 0.2067 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9224 - loss: 0.2725 - val_accuracy: 0.9386 - val_loss: 0.2018 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9281 - loss: 0.2585 - val_accuracy: 0.9384 - val_loss: 0.1994 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9302 - loss: 0.2522 - val_accuracy: 0.9391 - val_loss: 0.1968 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9324 - loss: 0.2424 - val_accuracy: 0.9398 - val_loss: 0.1943 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9340 - loss: 0.2384 - val_accuracy: 0.9404 - val_loss: 0.1955 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9353 - loss: 0.2333 - val_accuracy: 0.9399 - val_loss: 0.1934 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9368 - loss: 0.2280 - val_accuracy: 0.9413 - val_loss: 0.1916 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9366 - loss: 0.2277 - val_accuracy: 0.9417 - val_loss: 0.1907 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9382 - loss: 0.2227 - val_accuracy: 0.9415 - val_loss: 0.1897 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9387 - loss: 0.2174 - val_accuracy: 0.9424 - val_loss: 0.1887 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9391 - loss: 0.2150 - val_accuracy: 0.9426 - val_loss: 0.1888 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9394 - loss: 0.2161 - val_accuracy: 0.9432 - val_loss: 0.1863 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9391 - loss: 0.2123 - val_accuracy: 0.9438 - val_loss: 0.1845 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9401 - loss: 0.2099 - val_accuracy: 0.9436 - val_loss: 0.1833 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9416 - loss: 0.2063 - val_accuracy: 0.9438 - val_loss: 0.1820 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9413 - loss: 0.2057 - val_accuracy: 0.9436 - val_loss: 0.1829 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9422 - loss: 0.2018 - val_accuracy: 0.9438 - val_loss: 0.1808 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9420 - loss: 0.2010 - val_accuracy: 0.9443 - val_loss: 0.1795 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9418 - loss: 0.1986 - val_accuracy: 0.9446 - val_loss: 0.1784 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9438 - loss: 0.1946 - val_accuracy: 0.9445 - val_loss: 0.1769 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9432 - loss: 0.1936 - val_accuracy: 0.9444 - val_loss: 0.1765 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9432 - loss: 0.1936 - val_accuracy: 0.9451 - val_loss: 0.1765 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9431 - loss: 0.1926 - val_accuracy: 0.9455 - val_loss: 0.1767 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9440 - loss: 0.1888 - val_accuracy: 0.9458 - val_loss: 0.1745 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9437 - loss: 0.1889 - val_accuracy: 0.9448 - val_loss: 0.1742 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "610/610 - 3s - 4ms/step - accuracy: 0.9444 - loss: 0.1869 - val_accuracy: 0.9448 - val_loss: 0.1751 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9434 - loss: 0.1848 - val_accuracy: 0.9466 - val_loss: 0.1718 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9449 - loss: 0.1840 - val_accuracy: 0.9454 - val_loss: 0.1726 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9453 - loss: 0.1816 - val_accuracy: 0.9452 - val_loss: 0.1710 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9464 - loss: 0.1776 - val_accuracy: 0.9455 - val_loss: 0.1702 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9451 - loss: 0.1781 - val_accuracy: 0.9452 - val_loss: 0.1700 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9457 - loss: 0.1786 - val_accuracy: 0.9470 - val_loss: 0.1670 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9467 - loss: 0.1770 - val_accuracy: 0.9464 - val_loss: 0.1690 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9465 - loss: 0.1733 - val_accuracy: 0.9457 - val_loss: 0.1691 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9473 - loss: 0.1721 - val_accuracy: 0.9466 - val_loss: 0.1701 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9477 - loss: 0.1721 - val_accuracy: 0.9474 - val_loss: 0.1671 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9478 - loss: 0.1692 - val_accuracy: 0.9470 - val_loss: 0.1690 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "610/610 - 3s - 4ms/step - accuracy: 0.9476 - loss: 0.1700 - val_accuracy: 0.9469 - val_loss: 0.1673 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9484 - loss: 0.1677 - val_accuracy: 0.9484 - val_loss: 0.1674 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9488 - loss: 0.1638 - val_accuracy: 0.9477 - val_loss: 0.1667 - learning_rate: 5.0000e-05\n",
      "Epoch 45/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9502 - loss: 0.1608 - val_accuracy: 0.9491 - val_loss: 0.1652 - learning_rate: 5.0000e-05\n",
      "Epoch 46/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9505 - loss: 0.1598 - val_accuracy: 0.9488 - val_loss: 0.1667 - learning_rate: 5.0000e-05\n",
      "Epoch 47/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9502 - loss: 0.1602 - val_accuracy: 0.9484 - val_loss: 0.1656 - learning_rate: 5.0000e-05\n",
      "Epoch 48/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9500 - loss: 0.1597 - val_accuracy: 0.9488 - val_loss: 0.1648 - learning_rate: 5.0000e-05\n",
      "Epoch 49/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9505 - loss: 0.1584 - val_accuracy: 0.9486 - val_loss: 0.1642 - learning_rate: 5.0000e-05\n",
      "Epoch 50/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9512 - loss: 0.1568 - val_accuracy: 0.9494 - val_loss: 0.1633 - learning_rate: 5.0000e-05\n",
      "Epoch 51/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9514 - loss: 0.1571 - val_accuracy: 0.9485 - val_loss: 0.1633 - learning_rate: 5.0000e-05\n",
      "Epoch 52/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9514 - loss: 0.1563 - val_accuracy: 0.9482 - val_loss: 0.1627 - learning_rate: 5.0000e-05\n",
      "Epoch 53/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9514 - loss: 0.1533 - val_accuracy: 0.9483 - val_loss: 0.1640 - learning_rate: 5.0000e-05\n",
      "Epoch 54/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9516 - loss: 0.1535 - val_accuracy: 0.9488 - val_loss: 0.1640 - learning_rate: 5.0000e-05\n",
      "Epoch 55/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9524 - loss: 0.1531 - val_accuracy: 0.9489 - val_loss: 0.1644 - learning_rate: 5.0000e-05\n",
      "Epoch 56/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9517 - loss: 0.1531 - val_accuracy: 0.9486 - val_loss: 0.1631 - learning_rate: 5.0000e-05\n",
      "Epoch 57/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9517 - loss: 0.1533 - val_accuracy: 0.9484 - val_loss: 0.1643 - learning_rate: 5.0000e-05\n",
      "Epoch 58/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9523 - loss: 0.1526 - val_accuracy: 0.9487 - val_loss: 0.1637 - learning_rate: 5.0000e-05\n",
      "Epoch 59/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9531 - loss: 0.1504 - val_accuracy: 0.9494 - val_loss: 0.1621 - learning_rate: 5.0000e-05\n",
      "Epoch 60/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9521 - loss: 0.1511 - val_accuracy: 0.9489 - val_loss: 0.1625 - learning_rate: 5.0000e-05\n",
      "Epoch 61/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9521 - loss: 0.1511 - val_accuracy: 0.9488 - val_loss: 0.1632 - learning_rate: 5.0000e-05\n",
      "Epoch 62/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9539 - loss: 0.1478 - val_accuracy: 0.9487 - val_loss: 0.1621 - learning_rate: 5.0000e-05\n",
      "Epoch 63/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9534 - loss: 0.1460 - val_accuracy: 0.9491 - val_loss: 0.1620 - learning_rate: 5.0000e-05\n",
      "Epoch 64/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9539 - loss: 0.1473 - val_accuracy: 0.9492 - val_loss: 0.1627 - learning_rate: 5.0000e-05\n",
      "Epoch 65/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9538 - loss: 0.1457 - val_accuracy: 0.9488 - val_loss: 0.1615 - learning_rate: 5.0000e-05\n",
      "Epoch 66/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9550 - loss: 0.1448 - val_accuracy: 0.9492 - val_loss: 0.1630 - learning_rate: 5.0000e-05\n",
      "Epoch 67/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9535 - loss: 0.1480 - val_accuracy: 0.9495 - val_loss: 0.1609 - learning_rate: 5.0000e-05\n",
      "Epoch 68/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9544 - loss: 0.1450 - val_accuracy: 0.9492 - val_loss: 0.1604 - learning_rate: 5.0000e-05\n",
      "Epoch 69/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9547 - loss: 0.1454 - val_accuracy: 0.9491 - val_loss: 0.1616 - learning_rate: 5.0000e-05\n",
      "Epoch 70/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9547 - loss: 0.1434 - val_accuracy: 0.9497 - val_loss: 0.1604 - learning_rate: 5.0000e-05\n",
      "Epoch 71/150\n",
      "610/610 - 3s - 4ms/step - accuracy: 0.9541 - loss: 0.1441 - val_accuracy: 0.9499 - val_loss: 0.1606 - learning_rate: 5.0000e-05\n",
      "Epoch 72/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9554 - loss: 0.1442 - val_accuracy: 0.9495 - val_loss: 0.1621 - learning_rate: 5.0000e-05\n",
      "Epoch 73/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9552 - loss: 0.1428 - val_accuracy: 0.9495 - val_loss: 0.1614 - learning_rate: 5.0000e-05\n",
      "Epoch 74/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9556 - loss: 0.1405 - val_accuracy: 0.9505 - val_loss: 0.1614 - learning_rate: 5.0000e-05\n",
      "Epoch 75/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9549 - loss: 0.1411 - val_accuracy: 0.9506 - val_loss: 0.1590 - learning_rate: 5.0000e-05\n",
      "Epoch 76/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9560 - loss: 0.1395 - val_accuracy: 0.9505 - val_loss: 0.1595 - learning_rate: 5.0000e-05\n",
      "Epoch 77/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9558 - loss: 0.1380 - val_accuracy: 0.9515 - val_loss: 0.1603 - learning_rate: 5.0000e-05\n",
      "Epoch 78/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9552 - loss: 0.1391 - val_accuracy: 0.9509 - val_loss: 0.1595 - learning_rate: 5.0000e-05\n",
      "Epoch 79/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9560 - loss: 0.1379 - val_accuracy: 0.9510 - val_loss: 0.1576 - learning_rate: 5.0000e-05\n",
      "Epoch 80/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9557 - loss: 0.1387 - val_accuracy: 0.9505 - val_loss: 0.1611 - learning_rate: 5.0000e-05\n",
      "Epoch 81/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9558 - loss: 0.1368 - val_accuracy: 0.9518 - val_loss: 0.1592 - learning_rate: 5.0000e-05\n",
      "Epoch 82/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9574 - loss: 0.1351 - val_accuracy: 0.9515 - val_loss: 0.1572 - learning_rate: 5.0000e-05\n",
      "Epoch 83/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9570 - loss: 0.1344 - val_accuracy: 0.9512 - val_loss: 0.1591 - learning_rate: 5.0000e-05\n",
      "Epoch 84/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9565 - loss: 0.1356 - val_accuracy: 0.9511 - val_loss: 0.1605 - learning_rate: 5.0000e-05\n",
      "Epoch 85/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9575 - loss: 0.1328 - val_accuracy: 0.9518 - val_loss: 0.1585 - learning_rate: 5.0000e-05\n",
      "Epoch 86/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9580 - loss: 0.1318 - val_accuracy: 0.9513 - val_loss: 0.1596 - learning_rate: 5.0000e-05\n",
      "Epoch 87/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9570 - loss: 0.1332 - val_accuracy: 0.9510 - val_loss: 0.1590 - learning_rate: 5.0000e-05\n",
      "Epoch 88/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9574 - loss: 0.1338 - val_accuracy: 0.9525 - val_loss: 0.1573 - learning_rate: 5.0000e-05\n",
      "Epoch 89/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9578 - loss: 0.1340 - val_accuracy: 0.9521 - val_loss: 0.1590 - learning_rate: 5.0000e-05\n",
      "Epoch 90/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9578 - loss: 0.1330 - val_accuracy: 0.9516 - val_loss: 0.1585 - learning_rate: 2.5000e-05\n",
      "Epoch 91/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9581 - loss: 0.1311 - val_accuracy: 0.9510 - val_loss: 0.1584 - learning_rate: 2.5000e-05\n",
      "Epoch 92/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9585 - loss: 0.1308 - val_accuracy: 0.9516 - val_loss: 0.1586 - learning_rate: 2.5000e-05\n",
      "Epoch 93/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9588 - loss: 0.1295 - val_accuracy: 0.9520 - val_loss: 0.1587 - learning_rate: 2.5000e-05\n",
      "Epoch 94/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9585 - loss: 0.1279 - val_accuracy: 0.9525 - val_loss: 0.1584 - learning_rate: 2.5000e-05\n",
      "Epoch 95/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9588 - loss: 0.1286 - val_accuracy: 0.9513 - val_loss: 0.1584 - learning_rate: 2.5000e-05\n",
      "Epoch 96/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9584 - loss: 0.1282 - val_accuracy: 0.9517 - val_loss: 0.1589 - learning_rate: 2.5000e-05\n",
      "Epoch 97/150\n",
      "610/610 - 3s - 5ms/step - accuracy: 0.9598 - loss: 0.1278 - val_accuracy: 0.9520 - val_loss: 0.1581 - learning_rate: 1.2500e-05\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - accuracy: 0.9538 - loss: 0.1441\n",
      "Test Accuracy: 0.9544568657875061\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Group1 (Dint divide)       0.96      0.95      0.95      4919\n",
      "     Group2 (Divide)       0.95      0.95      0.95      4830\n",
      "\n",
      "            accuracy                           0.95      9749\n",
      "           macro avg       0.95      0.95      0.95      9749\n",
      "        weighted avg       0.95      0.95      0.95      9749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    # Clean and prepare metadata\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Prepare scRNA data\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Find common cells\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    # Filter metadata and scRNA data based on common cells\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    # Merge metadata with scRNA data\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_pc9_count_matrix.csv'\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_path, data_path)\n",
    "\n",
    "# Debugging steps to verify data\n",
    "print(\"Unique sample types:\", merged_data['sample_type'].unique())\n",
    "print(\"Sample type distribution:\\n\", merged_data['sample_type'].value_counts())\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "print(\"First few rows of merged data:\\n\", merged_data.head())\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "pca = PCA(n_components=500)  # n_components based on available memory and performance\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "def create_complex_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    x = Dense(2048, activation='relu')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(512, activation='relu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_ensemble_classifier(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def model_1():\n",
    "    # Define the mappings based on sample_type\n",
    "    non_persister_samples = [0]\n",
    "    persister_samples = [\"7\",\"3\",\"14_high\",\"14_med\",\"14_low\"]\n",
    "\n",
    "    # Update labels: 0 = non-persister, 1 = persister\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model1.keras')\n",
    "    encoder.save('complex_encoder_model1.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model1.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Non-Persister', 'Persister']))\n",
    "\n",
    "def model_2():\n",
    "    # Define the mappings based on sample_type\n",
    "    group1_samples = ['14_high']  # Dint divide\n",
    "    group2_samples = ['14_med', '14_low']  # Divide\n",
    "\n",
    "    # Update labels: 0 = Group1 (Dint divide), 1 = Group2 (Divide)\n",
    "    y_model2 = np.where(merged_data['sample_type'].isin(group1_samples), 0, 1)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model2:\", np.unique(y_model2, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model2)) != 2:\n",
    "        print(\"Not enough classes in y_model2 for Model 2\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model2)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model2.keras')\n",
    "    encoder.save('complex_encoder_model2.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model2.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Group1 (Dint divide)', 'Group2 (Divide)']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "# Execute Model 1\n",
    "model_1()\n",
    "\n",
    "# Execute Model 2\n",
    "model_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e3633-5a36-42d8-a2a1-e6e6f98654e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "--26062024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1407891f-d08a-4d93-920a-35f2ab862a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata first rows:\n",
      "                     nGene  nUMI orig.ident  percent.mito  sample_id  \\\n",
      "AAACCTGAGACAAGCC-1   1811  6438    pc9_14d      0.065548          1   \n",
      "AAACCTGAGCAGACTG-1   1764  6229    pc9_14d      0.085568          1   \n",
      "AAACCTGAGCGAGAAA-1   1759  6076    pc9_14d      0.090685          1   \n",
      "AAACCTGAGGACAGAA-1   2198  7264    pc9_14d      0.090308          1   \n",
      "AAACCTGAGGCCGAAT-1   2201  8666    pc9_14d      0.050658          1   \n",
      "\n",
      "                    time_point sample_name sample_type   full_cell_barcode  \\\n",
      "AAACCTGAGACAAGCC-1           0           0           0                 NaN   \n",
      "AAACCTGAGCAGACTG-1           0           0           0                 NaN   \n",
      "AAACCTGAGCGAGAAA-1           0           0           0  AAACCTGAGCGAGAAA-1   \n",
      "AAACCTGAGGACAGAA-1           0           0           0  AAACCTGAGGACAGAA-1   \n",
      "AAACCTGAGGCCGAAT-1           0           0           0                 NaN   \n",
      "\n",
      "                                                      lineage_barcode  \\\n",
      "AAACCTGAGACAAGCC-1                                                NaN   \n",
      "AAACCTGAGCAGACTG-1                                                NaN   \n",
      "AAACCTGAGCGAGAAA-1                     TCTGTCTCACAGACACTGACTGACTGTCTC   \n",
      "AAACCTGAGGACAGAA-1  TCTGAGACTCTCTGTCTCAGTCAGTGTCTG,TGAGTGTGTCTGAGT...   \n",
      "AAACCTGAGGCCGAAT-1                                                NaN   \n",
      "\n",
      "                    clone_size  \n",
      "AAACCTGAGACAAGCC-1         NaN  \n",
      "AAACCTGAGCAGACTG-1         NaN  \n",
      "AAACCTGAGCGAGAAA-1         1.0  \n",
      "AAACCTGAGGACAGAA-1         1.0  \n",
      "AAACCTGAGGCCGAAT-1         NaN  \n",
      "scRNA Data first rows:\n",
      "                            0             1           2           3      \\\n",
      "Unnamed: 0          RP11-34P13.7  RP11-34P13.8  FO538757.2  AP006222.2   \n",
      "AAACCTGAGACAAGCC.1     -0.046855     -0.006325   -0.364544   -0.428827   \n",
      "AAACCTGAGCAGACTG.1     -0.046855     -0.006325   -0.364544   -0.428827   \n",
      "AAACCTGAGCGAGAAA.1     -0.046855     -0.006325   -0.364544   -0.428827   \n",
      "AAACCTGAGGACAGAA.1     -0.046855     -0.006325   -0.364544   -0.428827   \n",
      "\n",
      "                            4             5              6             7      \\\n",
      "Unnamed: 0          RP5-857K21.15  RP4-669L17.2  RP4-669L17.10  RP5-857K21.4   \n",
      "AAACCTGAGACAAGCC.1      -0.008942     -0.068597      -0.045149     -0.053783   \n",
      "AAACCTGAGCAGACTG.1      -0.008942     -0.068597      -0.045149     -0.053783   \n",
      "AAACCTGAGCGAGAAA.1      -0.008942     -0.068597      -0.045149     -0.053783   \n",
      "AAACCTGAGGACAGAA.1      -0.008942     -0.068597      -0.045149     -0.053783   \n",
      "\n",
      "                           8              9      ...     12384        12385  \\\n",
      "Unnamed: 0          RP5-857K21.2  RP11-206L10.4  ...     BUD13  AP006216.10   \n",
      "AAACCTGAGACAAGCC.1     -0.056811      -0.010725  ... -0.170407    -0.024764   \n",
      "AAACCTGAGCAGACTG.1     -0.056811      -0.010725  ... -0.170407    -0.024764   \n",
      "AAACCTGAGCGAGAAA.1     -0.056811      -0.010725  ... -0.170407    -0.024764   \n",
      "AAACCTGAGGACAGAA.1     -0.056811      -0.010725  ... -0.170407    -0.024764   \n",
      "\n",
      "                       12386     12387     12388     12389     12390    12391  \\\n",
      "Unnamed: 0              ZPR1     APOC3     APOA1      SIK3  PAFAH1B2    SIDT2   \n",
      "AAACCTGAGACAAGCC.1  1.202008 -0.012171 -0.016337 -0.257034  1.287206 -0.20366   \n",
      "AAACCTGAGCAGACTG.1  1.244137 -0.012171 -0.016337 -0.257034 -0.727567 -0.20366   \n",
      "AAACCTGAGCGAGAAA.1 -0.752408 -0.012171 -0.016337 -0.257034   1.36373 -0.20366   \n",
      "AAACCTGAGGACAGAA.1  1.052546 -0.012171 -0.016337 -0.257034  2.115269 -0.20366   \n",
      "\n",
      "                       12392     12393  \n",
      "Unnamed: 0             TAGLN     PCSK7  \n",
      "AAACCTGAGACAAGCC.1 -0.281751 -0.609183  \n",
      "AAACCTGAGCAGACTG.1 -0.281751  1.669834  \n",
      "AAACCTGAGCGAGAAA.1 -0.281751 -0.609183  \n",
      "AAACCTGAGGACAGAA.1 -0.281751 -0.609183  \n",
      "\n",
      "[5 rows x 12394 columns]\n",
      "Common cells count: 54517\n",
      "Labels after encoding: [0 0 0 ... 2 2 2]\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    # Debugging: Print the first few rows to check the format\n",
    "    print(\"Metadata first rows:\\n\", metadata_df.head())\n",
    "    print(\"scRNA Data first rows:\\n\", scRNA_data.head())\n",
    "\n",
    "    # Prepare and clean cell identifiers\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('.').str[0].str.strip().str.upper()\n",
    "\n",
    "    # Find common cells\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "    print(\"Common cells count:\", len(common_cells))  # Debugging statement\n",
    "\n",
    "    # Filter metadata and scRNA data based on common cells\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    # Merge metadata with scRNA data\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    if merged_data.empty:\n",
    "        print(\"No data was merged. Check cell identifiers and filters.\")\n",
    "        return None  # Early exit if no data to process\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Debugging: Print to confirm non-empty labels\n",
    "    print(\"Labels after encoding:\", y)\n",
    "\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/normalized_GSE150949_pc9_count.csv'\n",
    "\n",
    "result = preprocess_data(metadata_path, data_path)\n",
    "if result:\n",
    "    X, y, merged_data, label_encoder = result\n",
    "    print(\"Data preprocessing completed successfully.\")\n",
    "else:\n",
    "    print(\"Data preprocessing failed due to empty merge or other issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847eb7f9-0e8c-4bc7-8e5a-7d5f6209c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 06:29:35.503564: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 06:29:39.645053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 06:29:58.636269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer  # Correct import for imputation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('.').str[0].str.strip().str.upper()\n",
    "\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    if merged_data.empty:\n",
    "        print(\"No data was merged. Check cell identifiers and filters.\")\n",
    "        return None\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/normalized_GSE150949_pc9_count.csv'\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_path, data_path)\n",
    "\n",
    "# Proceed with PCA\n",
    "pca = PCA(n_components=500)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8335598-6449-466f-ad8e-44fb1ccd6ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample types: ['0' '7' '3' '14_high' '14_med' '14_low']\n",
      "Sample type distribution:\n",
      " sample_type\n",
      "3          15338\n",
      "14_high    11571\n",
      "14_med      9931\n",
      "7           8891\n",
      "14_low      7358\n",
      "0           7226\n",
      "Name: count, dtype: int64\n",
      "Merged data shape: (60315, 12397)\n",
      "First few rows of merged data:\n",
      "                cell         0         1         2         3         4  \\\n",
      "0  AAACCTGAGACAAGCC -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "1  AAACCTGAGCAGACTG -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "2  AAACCTGAGCGAGAAA -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "3  AAACCTGAGGACAGAA -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "4  AAACCTGAGGCCGAAT -0.046855 -0.006325 -0.364544  2.032753 -0.008942   \n",
      "\n",
      "          5         6         7         8  ...     12386     12387     12388  \\\n",
      "0 -0.068597 -0.045149 -0.053783 -0.056811  ...  1.202008 -0.012171 -0.016337   \n",
      "1 -0.068597 -0.045149 -0.053783 -0.056811  ...  1.244137 -0.012171 -0.016337   \n",
      "2 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.752408 -0.012171 -0.016337   \n",
      "3 -0.068597 -0.045149 -0.053783 -0.056811  ...  1.052546 -0.012171 -0.016337   \n",
      "4 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.752408 -0.012171 -0.016337   \n",
      "\n",
      "      12389     12390    12391     12392     12393 sample_name sample_type  \n",
      "0 -0.257034  1.287206 -0.20366 -0.281751 -0.609183           0           0  \n",
      "1 -0.257034 -0.727567 -0.20366 -0.281751  1.669834           0           0  \n",
      "2 -0.257034   1.36373 -0.20366 -0.281751 -0.609183           0           0  \n",
      "3 -0.257034  2.115269 -0.20366 -0.281751 -0.609183           0           0  \n",
      "4 -0.257034  0.921638 -0.20366 -0.281751 -0.609183           0           0  \n",
      "\n",
      "[5 rows x 12397 columns]\n"
     ]
    }
   ],
   "source": [
    "# Debugging steps to verify data\n",
    "print(\"Unique sample types:\", merged_data['sample_type'].unique())\n",
    "print(\"Sample type distribution:\\n\", merged_data['sample_type'].value_counts())\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "print(\"First few rows of merged data:\\n\", merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e206c8-2005-4bea-8c86-f6cc829b8731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model1: (array([0, 1]), array([ 7226, 53089]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 06:57:20.281828: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "664/664 - 21s - 32ms/step - loss: 1.0347 - val_loss: 1.0094 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9983 - val_loss: 1.0037 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9941 - val_loss: 0.9987 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9897 - val_loss: 0.9936 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9858 - val_loss: 0.9898 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9828 - val_loss: 0.9862 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9793 - val_loss: 0.9831 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9767 - val_loss: 0.9808 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9738 - val_loss: 0.9774 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9722 - val_loss: 0.9756 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9709 - val_loss: 0.9742 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9692 - val_loss: 0.9728 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9682 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9671 - val_loss: 0.9702 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9661 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9650 - val_loss: 0.9690 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9642 - val_loss: 0.9674 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9636 - val_loss: 0.9667 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9627 - val_loss: 0.9667 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9622 - val_loss: 0.9666 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9617 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9612 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9605 - val_loss: 0.9651 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9603 - val_loss: 0.9640 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9596 - val_loss: 0.9631 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9593 - val_loss: 0.9629 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9588 - val_loss: 0.9625 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9589 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9588 - val_loss: 0.9622 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9582 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9576 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9579 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9575 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9569 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9568 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9567 - val_loss: 0.9601 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9567 - val_loss: 0.9597 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9567 - val_loss: 0.9603 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9560 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9558 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9559 - val_loss: 0.9589 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9557 - val_loss: 0.9587 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9553 - val_loss: 0.9587 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9552 - val_loss: 0.9584 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9548 - val_loss: 0.9581 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9548 - val_loss: 0.9579 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9544 - val_loss: 0.9580 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9543 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9543 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9538 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9537 - val_loss: 0.9575 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9535 - val_loss: 0.9567 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9536 - val_loss: 0.9564 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9536 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9535 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9538 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9533 - val_loss: 0.9563 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9532 - val_loss: 0.9560 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9527 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9527 - val_loss: 0.9561 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9525 - val_loss: 0.9554 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9528 - val_loss: 0.9561 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9528 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9526 - val_loss: 0.9555 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9525 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9521 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9520 - val_loss: 0.9550 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9519 - val_loss: 0.9550 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9516 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9518 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9520 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9517 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9513 - val_loss: 0.9548 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9511 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9511 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9509 - val_loss: 0.9538 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9509 - val_loss: 0.9545 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9508 - val_loss: 0.9539 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9504 - val_loss: 0.9535 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9508 - val_loss: 0.9543 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9507 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9507 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9505 - val_loss: 0.9539 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9503 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9503 - val_loss: 0.9531 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9502 - val_loss: 0.9528 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9502 - val_loss: 0.9533 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9502 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9499 - val_loss: 0.9528 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9500 - val_loss: 0.9535 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "664/664 - 18s - 28ms/step - loss: 0.9497 - val_loss: 0.9524 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9497 - val_loss: 0.9536 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9519 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9527 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9524 - val_loss: 0.9551 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9522 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9517 - val_loss: 0.9541 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9521 - val_loss: 0.9547 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9521 - val_loss: 0.9550 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9521 - val_loss: 0.9549 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9520 - val_loss: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9514 - val_loss: 0.9544 - learning_rate: 5.0000e-04\n",
      "Epoch 103/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9510 - val_loss: 0.9539 - learning_rate: 5.0000e-04\n",
      "Epoch 104/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9508 - val_loss: 0.9540 - learning_rate: 5.0000e-04\n",
      "Epoch 105/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9504 - val_loss: 0.9538 - learning_rate: 5.0000e-04\n",
      "Epoch 106/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9503 - val_loss: 0.9537 - learning_rate: 5.0000e-04\n",
      "Epoch 107/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9501 - val_loss: 0.9536 - learning_rate: 5.0000e-04\n",
      "Epoch 108/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9499 - val_loss: 0.9534 - learning_rate: 5.0000e-04\n",
      "Epoch 109/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9500 - val_loss: 0.9533 - learning_rate: 5.0000e-04\n",
      "Epoch 110/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9499 - val_loss: 0.9532 - learning_rate: 5.0000e-04\n",
      "Epoch 111/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9499 - val_loss: 0.9531 - learning_rate: 5.0000e-04\n",
      "\u001b[1m2655/2655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "664/664 - 4s - 7ms/step - accuracy: 0.9004 - loss: 0.3149 - val_accuracy: 0.9571 - val_loss: 0.1552 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9427 - loss: 0.2068 - val_accuracy: 0.9599 - val_loss: 0.1390 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9492 - loss: 0.1844 - val_accuracy: 0.9609 - val_loss: 0.1297 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9528 - loss: 0.1693 - val_accuracy: 0.9616 - val_loss: 0.1241 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9563 - loss: 0.1609 - val_accuracy: 0.9624 - val_loss: 0.1205 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9576 - loss: 0.1545 - val_accuracy: 0.9637 - val_loss: 0.1178 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9598 - loss: 0.1476 - val_accuracy: 0.9640 - val_loss: 0.1165 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9605 - loss: 0.1434 - val_accuracy: 0.9641 - val_loss: 0.1151 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9618 - loss: 0.1392 - val_accuracy: 0.9643 - val_loss: 0.1138 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9627 - loss: 0.1366 - val_accuracy: 0.9655 - val_loss: 0.1126 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9646 - loss: 0.1320 - val_accuracy: 0.9662 - val_loss: 0.1114 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9640 - loss: 0.1307 - val_accuracy: 0.9646 - val_loss: 0.1111 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9647 - loss: 0.1287 - val_accuracy: 0.9662 - val_loss: 0.1086 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9656 - loss: 0.1250 - val_accuracy: 0.9664 - val_loss: 0.1063 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9654 - loss: 0.1240 - val_accuracy: 0.9668 - val_loss: 0.1054 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9663 - loss: 0.1218 - val_accuracy: 0.9667 - val_loss: 0.1041 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9666 - loss: 0.1192 - val_accuracy: 0.9672 - val_loss: 0.1037 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9669 - loss: 0.1186 - val_accuracy: 0.9678 - val_loss: 0.1021 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9679 - loss: 0.1143 - val_accuracy: 0.9677 - val_loss: 0.1021 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9681 - loss: 0.1140 - val_accuracy: 0.9677 - val_loss: 0.0982 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9685 - loss: 0.1129 - val_accuracy: 0.9678 - val_loss: 0.1001 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9684 - loss: 0.1097 - val_accuracy: 0.9684 - val_loss: 0.0992 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9694 - loss: 0.1097 - val_accuracy: 0.9682 - val_loss: 0.0966 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9689 - loss: 0.1082 - val_accuracy: 0.9682 - val_loss: 0.0973 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9693 - loss: 0.1090 - val_accuracy: 0.9688 - val_loss: 0.0955 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9697 - loss: 0.1045 - val_accuracy: 0.9697 - val_loss: 0.0938 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9696 - loss: 0.1037 - val_accuracy: 0.9695 - val_loss: 0.0951 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9702 - loss: 0.1026 - val_accuracy: 0.9701 - val_loss: 0.0927 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9705 - loss: 0.1014 - val_accuracy: 0.9699 - val_loss: 0.0935 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9709 - loss: 0.0996 - val_accuracy: 0.9705 - val_loss: 0.0906 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9711 - loss: 0.0982 - val_accuracy: 0.9712 - val_loss: 0.0905 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9713 - loss: 0.0985 - val_accuracy: 0.9706 - val_loss: 0.0920 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9717 - loss: 0.0973 - val_accuracy: 0.9701 - val_loss: 0.0923 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9715 - loss: 0.0964 - val_accuracy: 0.9708 - val_loss: 0.0893 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9716 - loss: 0.0956 - val_accuracy: 0.9704 - val_loss: 0.0902 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9716 - loss: 0.0956 - val_accuracy: 0.9715 - val_loss: 0.0892 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9720 - loss: 0.0943 - val_accuracy: 0.9723 - val_loss: 0.0869 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9723 - loss: 0.0923 - val_accuracy: 0.9721 - val_loss: 0.0885 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9722 - loss: 0.0924 - val_accuracy: 0.9715 - val_loss: 0.0887 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9724 - loss: 0.0907 - val_accuracy: 0.9720 - val_loss: 0.0890 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9721 - loss: 0.0904 - val_accuracy: 0.9718 - val_loss: 0.0879 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9737 - loss: 0.0886 - val_accuracy: 0.9717 - val_loss: 0.0884 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9731 - loss: 0.0893 - val_accuracy: 0.9724 - val_loss: 0.0864 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9733 - loss: 0.0859 - val_accuracy: 0.9706 - val_loss: 0.0874 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9740 - loss: 0.0868 - val_accuracy: 0.9732 - val_loss: 0.0869 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9740 - loss: 0.0851 - val_accuracy: 0.9725 - val_loss: 0.0855 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9746 - loss: 0.0835 - val_accuracy: 0.9723 - val_loss: 0.0861 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9739 - loss: 0.0851 - val_accuracy: 0.9728 - val_loss: 0.0845 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9749 - loss: 0.0823 - val_accuracy: 0.9723 - val_loss: 0.0853 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9746 - loss: 0.0823 - val_accuracy: 0.9724 - val_loss: 0.0853 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9754 - loss: 0.0805 - val_accuracy: 0.9730 - val_loss: 0.0860 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9747 - loss: 0.0819 - val_accuracy: 0.9723 - val_loss: 0.0855 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9744 - loss: 0.0833 - val_accuracy: 0.9727 - val_loss: 0.0843 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9749 - loss: 0.0798 - val_accuracy: 0.9717 - val_loss: 0.0859 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9749 - loss: 0.0792 - val_accuracy: 0.9735 - val_loss: 0.0834 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9748 - loss: 0.0800 - val_accuracy: 0.9730 - val_loss: 0.0853 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9756 - loss: 0.0797 - val_accuracy: 0.9719 - val_loss: 0.0836 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9762 - loss: 0.0760 - val_accuracy: 0.9730 - val_loss: 0.0843 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9762 - loss: 0.0757 - val_accuracy: 0.9717 - val_loss: 0.0874 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9760 - loss: 0.0753 - val_accuracy: 0.9722 - val_loss: 0.0849 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9759 - loss: 0.0768 - val_accuracy: 0.9724 - val_loss: 0.0832 - learning_rate: 1.0000e-04\n",
      "Epoch 62/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9767 - loss: 0.0756 - val_accuracy: 0.9725 - val_loss: 0.0836 - learning_rate: 1.0000e-04\n",
      "Epoch 63/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9765 - loss: 0.0757 - val_accuracy: 0.9721 - val_loss: 0.0866 - learning_rate: 1.0000e-04\n",
      "Epoch 64/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9759 - loss: 0.0758 - val_accuracy: 0.9732 - val_loss: 0.0818 - learning_rate: 1.0000e-04\n",
      "Epoch 65/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9756 - loss: 0.0764 - val_accuracy: 0.9732 - val_loss: 0.0842 - learning_rate: 1.0000e-04\n",
      "Epoch 66/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9768 - loss: 0.0731 - val_accuracy: 0.9727 - val_loss: 0.0847 - learning_rate: 1.0000e-04\n",
      "Epoch 67/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9769 - loss: 0.0738 - val_accuracy: 0.9733 - val_loss: 0.0828 - learning_rate: 1.0000e-04\n",
      "Epoch 68/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9773 - loss: 0.0711 - val_accuracy: 0.9731 - val_loss: 0.0857 - learning_rate: 1.0000e-04\n",
      "Epoch 69/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9770 - loss: 0.0721 - val_accuracy: 0.9732 - val_loss: 0.0836 - learning_rate: 1.0000e-04\n",
      "Epoch 70/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9773 - loss: 0.0717 - val_accuracy: 0.9736 - val_loss: 0.0816 - learning_rate: 1.0000e-04\n",
      "Epoch 71/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9774 - loss: 0.0712 - val_accuracy: 0.9727 - val_loss: 0.0856 - learning_rate: 1.0000e-04\n",
      "Epoch 72/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9770 - loss: 0.0720 - val_accuracy: 0.9728 - val_loss: 0.0845 - learning_rate: 1.0000e-04\n",
      "Epoch 73/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9776 - loss: 0.0710 - val_accuracy: 0.9729 - val_loss: 0.0846 - learning_rate: 1.0000e-04\n",
      "Epoch 74/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9775 - loss: 0.0706 - val_accuracy: 0.9727 - val_loss: 0.0865 - learning_rate: 1.0000e-04\n",
      "Epoch 75/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9779 - loss: 0.0692 - val_accuracy: 0.9729 - val_loss: 0.0862 - learning_rate: 1.0000e-04\n",
      "Epoch 76/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9788 - loss: 0.0682 - val_accuracy: 0.9724 - val_loss: 0.0875 - learning_rate: 1.0000e-04\n",
      "Epoch 77/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9780 - loss: 0.0711 - val_accuracy: 0.9721 - val_loss: 0.0863 - learning_rate: 1.0000e-04\n",
      "Epoch 78/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9796 - loss: 0.0650 - val_accuracy: 0.9726 - val_loss: 0.0852 - learning_rate: 5.0000e-05\n",
      "Epoch 79/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9793 - loss: 0.0655 - val_accuracy: 0.9728 - val_loss: 0.0857 - learning_rate: 5.0000e-05\n",
      "Epoch 80/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9792 - loss: 0.0647 - val_accuracy: 0.9727 - val_loss: 0.0864 - learning_rate: 5.0000e-05\n",
      "Epoch 81/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9798 - loss: 0.0643 - val_accuracy: 0.9733 - val_loss: 0.0841 - learning_rate: 5.0000e-05\n",
      "Epoch 82/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9793 - loss: 0.0647 - val_accuracy: 0.9723 - val_loss: 0.0858 - learning_rate: 5.0000e-05\n",
      "Epoch 83/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9798 - loss: 0.0629 - val_accuracy: 0.9730 - val_loss: 0.0859 - learning_rate: 5.0000e-05\n",
      "Epoch 84/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9802 - loss: 0.0629 - val_accuracy: 0.9734 - val_loss: 0.0862 - learning_rate: 5.0000e-05\n",
      "Epoch 85/150\n",
      "664/664 - 3s - 4ms/step - accuracy: 0.9804 - loss: 0.0607 - val_accuracy: 0.9733 - val_loss: 0.0855 - learning_rate: 2.5000e-05\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.9735 - loss: 0.0872\n",
      "Test Accuracy: 0.9745714664459229\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Persister       0.98      0.97      0.97      5255\n",
      "    Persister       0.97      0.98      0.97      5363\n",
      "\n",
      "     accuracy                           0.97     10618\n",
      "    macro avg       0.97      0.97      0.97     10618\n",
      " weighted avg       0.97      0.97      0.97     10618\n",
      "\n",
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/300\n",
      "610/610 - 20s - 33ms/step - loss: 1.0385 - val_loss: 1.0116 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "610/610 - 17s - 28ms/step - loss: 1.0001 - val_loss: 1.0097 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9992 - val_loss: 1.0085 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "610/610 - 18s - 29ms/step - loss: 0.9979 - val_loss: 1.0065 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9958 - val_loss: 1.0036 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9932 - val_loss: 1.0003 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9907 - val_loss: 0.9970 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9879 - val_loss: 0.9940 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9855 - val_loss: 0.9916 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9836 - val_loss: 0.9908 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9817 - val_loss: 0.9876 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9803 - val_loss: 0.9864 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9787 - val_loss: 0.9851 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9772 - val_loss: 0.9830 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9764 - val_loss: 0.9823 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9754 - val_loss: 0.9809 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9742 - val_loss: 0.9795 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9731 - val_loss: 0.9783 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9720 - val_loss: 0.9775 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9716 - val_loss: 0.9766 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9710 - val_loss: 0.9770 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9702 - val_loss: 0.9758 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9695 - val_loss: 0.9748 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9690 - val_loss: 0.9744 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9681 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9679 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9683 - val_loss: 0.9734 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9679 - val_loss: 0.9727 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9670 - val_loss: 0.9725 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9663 - val_loss: 0.9715 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9659 - val_loss: 0.9712 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9662 - val_loss: 0.9711 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9656 - val_loss: 0.9709 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9651 - val_loss: 0.9703 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9646 - val_loss: 0.9703 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9643 - val_loss: 0.9699 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9639 - val_loss: 0.9692 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9634 - val_loss: 0.9690 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9630 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9629 - val_loss: 0.9683 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9631 - val_loss: 0.9685 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9626 - val_loss: 0.9681 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9623 - val_loss: 0.9683 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9621 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9619 - val_loss: 0.9671 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9614 - val_loss: 0.9669 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9618 - val_loss: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9614 - val_loss: 0.9668 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9612 - val_loss: 0.9669 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9608 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9611 - val_loss: 0.9671 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9612 - val_loss: 0.9666 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9607 - val_loss: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9603 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9605 - val_loss: 0.9662 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9604 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9601 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9600 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9598 - val_loss: 0.9657 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9596 - val_loss: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9592 - val_loss: 0.9658 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9592 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9592 - val_loss: 0.9661 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9591 - val_loss: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9590 - val_loss: 0.9645 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9583 - val_loss: 0.9644 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9587 - val_loss: 0.9651 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9588 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9590 - val_loss: 0.9646 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9589 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9585 - val_loss: 0.9651 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9582 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9582 - val_loss: 0.9640 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9579 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9577 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9578 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9575 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9580 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9575 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9575 - val_loss: 0.9635 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9573 - val_loss: 0.9636 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9573 - val_loss: 0.9629 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9571 - val_loss: 0.9630 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9568 - val_loss: 0.9631 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9569 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9569 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9567 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9565 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9563 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9562 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9560 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9561 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9560 - val_loss: 0.9633 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9562 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9562 - val_loss: 0.9623 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9558 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9558 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9557 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9556 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9553 - val_loss: 0.9617 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9551 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9551 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9554 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9551 - val_loss: 0.9615 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9551 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9552 - val_loss: 0.9614 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9549 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9552 - val_loss: 0.9616 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9550 - val_loss: 0.9614 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9547 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9547 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9552 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9553 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9549 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9546 - val_loss: 0.9610 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9544 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9543 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9543 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9543 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9541 - val_loss: 0.9603 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9541 - val_loss: 0.9604 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9539 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9540 - val_loss: 0.9605 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9540 - val_loss: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9539 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9540 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9539 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9541 - val_loss: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9539 - val_loss: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9539 - val_loss: 0.9599 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9537 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9540 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9536 - val_loss: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9536 - val_loss: 0.9592 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9538 - val_loss: 0.9617 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9536 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9535 - val_loss: 0.9601 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9536 - val_loss: 0.9599 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9537 - val_loss: 0.9604 - learning_rate: 0.0010\n",
      "Epoch 140/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9539 - val_loss: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 141/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9537 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 142/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9533 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 143/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9534 - val_loss: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 144/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9535 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 145/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9529 - val_loss: 0.9591 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9527 - val_loss: 0.9590 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9524 - val_loss: 0.9590 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9526 - val_loss: 0.9588 - learning_rate: 5.0000e-04\n",
      "Epoch 149/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9523 - val_loss: 0.9587 - learning_rate: 5.0000e-04\n",
      "Epoch 150/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9521 - val_loss: 0.9589 - learning_rate: 5.0000e-04\n",
      "Epoch 151/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9520 - val_loss: 0.9582 - learning_rate: 5.0000e-04\n",
      "Epoch 152/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9519 - val_loss: 0.9581 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9518 - val_loss: 0.9589 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9517 - val_loss: 0.9581 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9517 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9516 - val_loss: 0.9581 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9516 - val_loss: 0.9579 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9515 - val_loss: 0.9578 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9512 - val_loss: 0.9582 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9514 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9513 - val_loss: 0.9579 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9513 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9511 - val_loss: 0.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9511 - val_loss: 0.9581 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "610/610 - 17s - 28ms/step - loss: 0.9512 - val_loss: 0.9577 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n"
     ]
    }
   ],
   "source": [
    "def create_complex_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    x = Dense(2048, activation='relu')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(512, activation='relu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_ensemble_classifier(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def model_1():\n",
    "    # Define the mappings based on sample_type\n",
    "    non_persister_samples = [0]\n",
    "    persister_samples = [\"7\",\"3\",\"14_high\",\"14_med\",\"14_low\"]\n",
    "\n",
    "    # Update labels: 0 = non-persister, 1 = persister\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model1.keras')\n",
    "    encoder.save('complex_encoder_model1.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model1.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Non-Persister', 'Persister']))\n",
    "\n",
    "def model_2():\n",
    "    # Define the mappings based on sample_type\n",
    "    group1_samples = ['14_high']  # Dint divide\n",
    "    group2_samples = ['14_med', '14_low']  # Divide\n",
    "\n",
    "    # Update labels: 0 = Group1 (Dint divide), 1 = Group2 (Divide)\n",
    "    y_model2 = np.where(merged_data['sample_type'].isin(group1_samples), 0, 1)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model2:\", np.unique(y_model2, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model2)) != 2:\n",
    "        print(\"Not enough classes in y_model2 for Model 2\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model2)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model2.keras')\n",
    "    encoder.save('complex_encoder_model2.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model2.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Group1 (Dint divide)', 'Group2 (Divide)']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "# Execute Model 1\n",
    "model_1()\n",
    "\n",
    "# Execute Model 2\n",
    "model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb72d05-5617-443d-9e94-bea46f132722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
