{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78690db2-1cbb-470d-ae8c-c70a90301334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory to sys.path\n",
    "import sys\n",
    "sys.path.append('/scratch/project_2010376')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392189e1-4597-4bf4-a813-c45af88dccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sample types: ['0' '7' '3' '14_high' '14_med' '14_low']\n",
      "Sample type distribution:\n",
      " sample_type\n",
      "3          15338\n",
      "14_high    11571\n",
      "14_med      9931\n",
      "7           8891\n",
      "14_low      7358\n",
      "0           7226\n",
      "Name: count, dtype: int64\n",
      "Merged data shape: (60315, 22169)\n",
      "First few rows of merged data:\n",
      "                cell         0         1         2         3         4  \\\n",
      "0  AAACCTGAGACAAGCC -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "1  AAACCTGAGCAGACTG -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "2  AAACCTGAGCGAGAAA -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "3  AAACCTGAGGACAGAA -0.046855 -0.006325 -0.364544 -0.428827 -0.008942   \n",
      "4  AAACCTGAGGCCGAAT -0.046855 -0.006325 -0.364544  2.032753 -0.008942   \n",
      "\n",
      "          5         6         7         8  ...     22158 22159     22160  \\\n",
      "0 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.222335   0.0 -0.064259   \n",
      "1 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.222335   0.0 -0.064259   \n",
      "2 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.222335   0.0 -0.064259   \n",
      "3 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.222335   0.0 -0.064259   \n",
      "4 -0.068597 -0.045149 -0.053783 -0.056811  ... -0.222335   0.0 -0.064259   \n",
      "\n",
      "      22161    22162     22163    22164    22165 sample_name sample_type  \n",
      "0 -0.026803 -0.10722 -0.458786 -0.24572 -0.14929           0           0  \n",
      "1 -0.026803 -0.10722 -0.458786 -0.24572 -0.14929           0           0  \n",
      "2 -0.026803 -0.10722 -0.458786 -0.24572 -0.14929           0           0  \n",
      "3 -0.026803 -0.10722 -0.458786 -0.24572 -0.14929           0           0  \n",
      "4 -0.026803 -0.10722 -0.458786 -0.24572 -0.14929           0           0  \n",
      "\n",
      "[5 rows x 22169 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAHUCAYAAABVveuUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABH5ElEQVR4nO3dfVwVdd7/8feJO5HgKBAQGyoVmgqZYiFaqZeKuiKZu2tFHa01tfUuErVcLwv3MiktteTS1DW1zGh3S7e2IjHN1gRvMDJv1u5M1EBM8SCEgDi/P7qcX0fUFNFBeD0fj3k8PN/5zMzn656tfe93Zo7NMAxDAAAAAIAr7hqrGwAAAACAhopABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAKix7du365FHHlF4eLgaNWqka6+9Vh06dNCMGTN09OhRs65bt27q1q2bdY2eg81mMzc3Nzc1bdpU7dq104gRI5SdnV2t/vvvv5fNZtPSpUsv6jorVqzQnDlzLuqYs10rJSVFNptNP/7440Wd63x27dqllJQUff/999X2Pfzww2rRokWtXQsAUB2BDABQI4sWLVJ0dLS2bNmiCRMmKCMjQytXrtQf/vAHvfLKKxo6dKjVLV6Q3//+98rKytKGDRuUnp6uwYMHKzs7W7GxsXr88cddaq+//nplZWWpX79+F3WNmgSyml7rYu3atUtTp049ayCbMmWKVq5ceVmvDwANnbvVDQAArj5ZWVn605/+pF69emnVqlXy8vIy9/Xq1UvJycnKyMiwsMMLFxwcrE6dOpmfe/furaSkJA0fPlwvv/yybrnlFv3pT3+SJHl5ebnUXg5VVVU6efLkFbnWr7npppssvT4ANASskAEALtr06dNls9m0cOFClzB2mqenpxISEs57jqlTpyomJkb+/v7y8/NThw4dtHjxYhmG4VK3du1adevWTQEBAfL29lazZs30u9/9Tj/99JNZM3/+fLVr107XXnutfH19dcstt+jPf/5zjefn5uamtLQ0BQYGaubMmeb42W4jPHz4sIYPH66wsDB5eXnpuuuuU5cuXbRmzRpJP9+u+f7772vfvn0ut0j+8nwzZszQtGnTFB4eLi8vL61bt+68t0fu379fAwcOlJ+fn+x2ux566CEdPnzYpcZmsyklJaXasS1atNDDDz8sSVq6dKn+8Ic/SJK6d+9u9nb6mme7ZfHEiROaNGmSwsPD5enpqd/85jcaNWqUjh07Vu068fHxysjIUIcOHeTt7a1bbrlFr7766q/87QNAw8IKGQDgolRVVWnt2rWKjo5WWFhYjc/z/fffa8SIEWrWrJkkKTs7W2PGjNHBgwf19NNPmzX9+vXTXXfdpVdffVVNmjTRwYMHlZGRoYqKCjVu3Fjp6ekaOXKkxowZoxdeeEHXXHONvvnmG+3ateuS5unt7a2ePXsqPT1dBw4c0A033HDWOofDoW3btunZZ59Vy5YtdezYMW3btk1HjhyRJM2bN0/Dhw/Xt99+e87b/15++WW1bNlSL7zwgvz8/BQREXHe3u69914NGjRIjz32mHbu3KkpU6Zo165d2rRpkzw8PC54jv369dP06dP15z//Wf/7v/+rDh06SDr3yphhGBowYIA+/vhjTZo0SXfddZe2b9+uZ555RllZWcrKynIJ6F988YWSk5P11FNPKTg4WH/96181dOhQ3Xzzzbr77rsvuE8AqM8IZACAi/Ljjz/qp59+Unh4+CWdZ8mSJeafT506pW7duskwDL300kuaMmWKbDabcnJydOLECc2cOVPt2rUz6xMTE80/f/bZZ2rSpIlefvllc6xHjx6X1NtpzZs3lyT98MMP5wxkn332mR599FENGzbMHLvnnnvMP7dp00ZNmjQ57y2IjRo10kcffeQSps72TNdpAwcO1IwZMyRJcXFxCg4O1oMPPqi//e1vevDBBy94ftddd50Z/tq0afOrt0iuXr1aH330kWbMmKEJEyZI+vkW1bCwMN1333167bXXXP4efvzxR3322Wdm6L777rv18ccfa8WKFQQyAPg/3LIIALDE2rVr1bNnT9ntdrm5ucnDw0NPP/20jhw5osLCQknSbbfdJk9PTw0fPlzLli3Td999V+08d9xxh44dO6YHHnhA//znP2v1DYRn3j55NnfccYeWLl2qadOmKTs7W5WVlRd9nYSEhIta2TozdA0aNEju7u5at27dRV/7Yqxdu1aSzFseT/vDH/4gHx8fffzxxy7jt912mxnGpJ+DZ8uWLbVv377L2icAXE0IZACAixIYGKjGjRtr7969NT7H5s2bFRcXJ+nntzV+9tln2rJliyZPnixJKisrk/TzrXNr1qxRUFCQRo0apZtuukk33XSTXnrpJfNcDodDr776qvbt26ff/e53CgoKUkxMjDIzMy9hlj87HRxCQ0PPWfPWW29pyJAh+utf/6rY2Fj5+/tr8ODBKigouODrXH/99RfVV0hIiMtnd3d3BQQEmLdJXi5HjhyRu7u7rrvuOpdxm82mkJCQatcPCAiodg4vLy/zP18AAIEMAHCR3Nzc1KNHD+Xk5OjAgQM1Okd6ero8PDz0r3/9S4MGDVLnzp3VsWPHs9beddddeu+99+R0Os3X0SclJSk9Pd2seeSRR7Rx40Y5nU69//77MgxD8fHxl7QSU1ZWpjVr1uimm2465+2K0s8Bdc6cOfr++++1b98+paam6p133qm2inQ+p1/ycaHODHsnT57UkSNHXAKQl5eXysvLqx17KaEtICBAJ0+erPYCEcMwVFBQoMDAwBqfGwAaKgIZAOCiTZo0SYZhaNiwYaqoqKi2v7KyUu+99945j7fZbHJ3d5ebm5s5VlZWptdff/2cx7i5uSkmJkb/+7//K0natm1btRofHx/17dtXkydPVkVFhXbu3Hkx0zJVVVVp9OjROnLkiJ588skLPq5Zs2YaPXq0evXq5dJfba8KvfHGGy6f//a3v+nkyZMuP77dokULbd++3aVu7dq1KikpcRk7/RKOC+nv9LN5y5cvdxl/++23VVpaWmvP7gFAQ8JLPQAAFy02Nlbz58/XyJEjFR0drT/96U9q27atKisr9fnnn2vhwoWKjIxU//79z3p8v379NGvWLCUmJmr48OE6cuSIXnjhhWqv0H/llVe0du1a9evXT82aNdOJEyfM16b37NlTkjRs2DB5e3urS5cuuv7661VQUKDU1FTZ7XbdfvvtvzqXQ4cOKTs7W4Zh6Pjx49qxY4dee+01ffHFF3riiSdcXlJxJqfTqe7duysxMVG33HKLfH19tWXLFmVkZGjgwIFmXVRUlN555x3Nnz9f0dHRuuaaa865Ingh3nnnHbm7u6tXr17mWxbbtWunQYMGmTUOh0NTpkzR008/ra5du2rXrl1KS0uT3W53OVdkZKQkaeHChfL19VWjRo0UHh5+1tsNe/Xqpd69e+vJJ59UcXGxunTpYr5lsX379nI4HDWeEwA0WAYAADWUm5trDBkyxGjWrJnh6elp+Pj4GO3btzeefvppo7Cw0Kzr2rWr0bVrV5djX331VaNVq1aGl5eXceONNxqpqanG4sWLDUnG3r17DcMwjKysLOPee+81mjdvbnh5eRkBAQFG165djXfffdc8z7Jly4zu3bsbwcHBhqenpxEaGmoMGjTI2L59+6/2L8ncrrnmGsPPz8+Iiooyhg8fbmRlZVWr37t3ryHJWLJkiWEYhnHixAnjscceM2699VbDz8/P8Pb2Nlq1amU888wzRmlpqXnc0aNHjd///vdGkyZNDJvNZpz+1+/p882cOfNXr2UYhvHMM88YkoycnByjf//+xrXXXmv4+voaDzzwgHHo0CGX48vLy42JEycaYWFhhre3t9G1a1cjNzfXaN68uTFkyBCX2jlz5hjh4eGGm5ubyzWHDBliNG/e3KW2rKzMePLJJ43mzZsbHh4exvXXX2/86U9/MoqKilzqmjdvbvTr16/avM72XQCAhsxmGBfwCikAAAAAQK3jGTIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALCIpYHs008/Vf/+/RUaGiqbzaZVq1ZVq9m9e7cSEhJkt9vl6+urTp06KS8vz9xfXl6uMWPGKDAwUD4+PkpISNCBAwdczlFUVCSHwyG73S673S6Hw6Fjx4651OTl5al///7y8fFRYGCgxo4de9YfOwUAAACA2mLpD0OXlpaqXbt2euSRR/S73/2u2v5vv/1Wd955p4YOHaqpU6fKbrdr9+7datSokVmTlJSk9957T+np6QoICFBycrLi4+OVk5MjNzc3SVJiYqIOHDigjIwMSdLw4cPlcDj03nvvSZKqqqrUr18/XXfdddqwYYOOHDmiIUOGyDAMzZ0794Lnc+rUKf3www/y9fWVzWa7lL8aAAAAAFcxwzB0/PhxhYaG6pprzrMOZu3PoP1/koyVK1e6jN13333GQw89dM5jjh07Znh4eBjp6enm2MGDB41rrrnGyMjIMAzDMHbt2mVIMrKzs82arKwsQ5Lxn//8xzAMw/jggw+Ma665xjh48KBZ8+abbxpeXl6G0+m84Dns37/f5UdG2djY2NjY2NjY2Nga9rZ///7zZghLV8jO59SpU3r//fc1ceJE9e7dW59//rnCw8M1adIkDRgwQJKUk5OjyspKxcXFmceFhoYqMjJSGzduVO/evZWVlSW73a6YmBizplOnTrLb7dq4caNatWqlrKwsRUZGKjQ01Kzp3bu3ysvLlZOTo+7du5+1x/LycpWXl5ufjf/7je39+/fLz8+vNv86AAAAAFxFiouLFRYWJl9f3/PW1dlAVlhYqJKSEj333HOaNm2ann/+eWVkZGjgwIFat26dunbtqoKCAnl6eqpp06YuxwYHB6ugoECSVFBQoKCgoGrnDwoKcqkJDg522d+0aVN5enqaNWeTmpqqqVOnVhv38/MjkAEAAAD41UeZ6uxbFk+dOiVJuueee/TEE0/otttu01NPPaX4+Hi98sor5z3WMAyXiZ/tL6EmNWeaNGmSnE6nue3fv/9X5wUAAAAAp9XZQBYYGCh3d3e1adPGZbx169bmWxZDQkJUUVGhoqIil5rCwkJzxSskJESHDh2qdv7Dhw+71Jy5ElZUVKTKyspqK2e/5OXlZa6GsSoGAAAA4GLV2UDm6emp22+/XXv27HEZ/+qrr9S8eXNJUnR0tDw8PJSZmWnuz8/P144dO9S5c2dJUmxsrJxOpzZv3mzWbNq0SU6n06Vmx44dys/PN2tWr14tLy8vRUdHX7Y5AgAAAGjYLH2GrKSkRN988435ee/evcrNzZW/v7+aNWumCRMm6L777tPdd9+t7t27KyMjQ++9954++eQTSZLdbtfQoUOVnJysgIAA+fv7a/z48YqKilLPnj0l/byi1qdPHw0bNkwLFiyQ9PNr7+Pj49WqVStJUlxcnNq0aSOHw6GZM2fq6NGjGj9+vIYNG8aqFwAAAIDLxmacfjWgBT755JOzvsFwyJAhWrp0qSTp1VdfVWpqqg4cOKBWrVpp6tSpuueee8zaEydOaMKECVqxYoXKysrUo0cPzZs3T2FhYWbN0aNHNXbsWL377ruSpISEBKWlpalJkyZmTV5enkaOHKm1a9fK29tbiYmJeuGFF+Tl5XXB8ykuLpbdbpfT6STIAQAAAA3YhWYDSwNZfUMgAwAAACBdeDaos8+QAQAAAEB9RyADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACzibnUDAAAAkKInvGZ1CziPnJmDrW4B9RQrZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFrE0kH366afq37+/QkNDZbPZtGrVqnPWjhgxQjabTXPmzHEZLy8v15gxYxQYGCgfHx8lJCTowIEDLjVFRUVyOByy2+2y2+1yOBw6duyYS01eXp769+8vHx8fBQYGauzYsaqoqKilmQIAAABAdZYGstLSUrVr105paWnnrVu1apU2bdqk0NDQavuSkpK0cuVKpaena8OGDSopKVF8fLyqqqrMmsTEROXm5iojI0MZGRnKzc2Vw+Ew91dVValfv34qLS3Vhg0blJ6errffflvJycm1N1kAAAAAOIO7lRfv27ev+vbte96agwcPavTo0froo4/Ur18/l31Op1OLFy/W66+/rp49e0qSli9frrCwMK1Zs0a9e/fW7t27lZGRoezsbMXExEiSFi1apNjYWO3Zs0etWrXS6tWrtWvXLu3fv98MfS+++KIefvhhPfvss/Lz87sMswcAAADQ0NXpZ8hOnTolh8OhCRMmqG3bttX25+TkqLKyUnFxceZYaGioIiMjtXHjRklSVlaW7Ha7GcYkqVOnTrLb7S41kZGRLitwvXv3Vnl5uXJycs7ZX3l5uYqLi102AAAAALhQdTqQPf/883J3d9fYsWPPur+goECenp5q2rSpy3hwcLAKCgrMmqCgoGrHBgUFudQEBwe77G/atKk8PT3NmrNJTU01n0uz2+0KCwu7qPkBAAAAaNjqbCDLycnRSy+9pKVLl8pms13UsYZhuBxztuNrUnOmSZMmyel0mtv+/fsvqk8AAAAADVudDWT//ve/VVhYqGbNmsnd3V3u7u7at2+fkpOT1aJFC0lSSEiIKioqVFRU5HJsYWGhueIVEhKiQ4cOVTv/4cOHXWrOXAkrKipSZWVltZWzX/Ly8pKfn5/LBgAAAAAXqs4GMofDoe3btys3N9fcQkNDNWHCBH300UeSpOjoaHl4eCgzM9M8Lj8/Xzt27FDnzp0lSbGxsXI6ndq8ebNZs2nTJjmdTpeaHTt2KD8/36xZvXq1vLy8FB0dfSWmCwAAAKABsvQtiyUlJfrmm2/Mz3v37lVubq78/f3VrFkzBQQEuNR7eHgoJCRErVq1kiTZ7XYNHTpUycnJCggIkL+/v8aPH6+oqCjzrYutW7dWnz59NGzYMC1YsECSNHz4cMXHx5vniYuLU5s2beRwODRz5kwdPXpU48eP17Bhw1j1AgAAAHDZWLpCtnXrVrVv317t27eXJI0bN07t27fX008/fcHnmD17tgYMGKBBgwapS5cuaty4sd577z25ubmZNW+88YaioqIUFxenuLg43XrrrXr99dfN/W5ubnr//ffVqFEjdenSRYMGDdKAAQP0wgsv1N5kAQAAAOAMNsMwDKubqC+Ki4tlt9vldDpZWQMAABclesJrVreA88iZOdjqFnCVudBsUGefIQMAAACA+o5ABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWMTSQPbpp5+qf//+Cg0Nlc1m06pVq8x9lZWVevLJJxUVFSUfHx+FhoZq8ODB+uGHH1zOUV5erjFjxigwMFA+Pj5KSEjQgQMHXGqKiorkcDhkt9tlt9vlcDh07Ngxl5q8vDz1799fPj4+CgwM1NixY1VRUXG5pg4AAAAA1gay0tJStWvXTmlpadX2/fTTT9q2bZumTJmibdu26Z133tFXX32lhIQEl7qkpCStXLlS6enp2rBhg0pKShQfH6+qqiqzJjExUbm5ucrIyFBGRoZyc3PlcDjM/VVVVerXr59KS0u1YcMGpaen6+2331ZycvLlmzwAAACABs9mGIZhdROSZLPZtHLlSg0YMOCcNVu2bNEdd9yhffv2qVmzZnI6nbruuuv0+uuv67777pMk/fDDDwoLC9MHH3yg3r17a/fu3WrTpo2ys7MVExMjScrOzlZsbKz+85//qFWrVvrwww8VHx+v/fv3KzQ0VJKUnp6uhx9+WIWFhfLz87ugORQXF8tut8vpdF7wMQAAAJIUPeE1q1vAeeTMHGx1C7jKXGg2uKqeIXM6nbLZbGrSpIkkKScnR5WVlYqLizNrQkNDFRkZqY0bN0qSsrKyZLfbzTAmSZ06dZLdbnepiYyMNMOYJPXu3Vvl5eXKyck5Zz/l5eUqLi522QAAAADgQl01gezEiRN66qmnlJiYaCbMgoICeXp6qmnTpi61wcHBKigoMGuCgoKqnS8oKMilJjg42GV/06ZN5enpadacTWpqqvlcmt1uV1hY2CXNEQAAAEDDclUEssrKSt1///06deqU5s2b96v1hmHIZrOZn3/550upOdOkSZPkdDrNbf/+/b/aGwAAAACcVucDWWVlpQYNGqS9e/cqMzPT5f7LkJAQVVRUqKioyOWYwsJCc8UrJCREhw4dqnbew4cPu9ScuRJWVFSkysrKaitnv+Tl5SU/Pz+XDQAAAAAuVJ0OZKfD2Ndff601a9YoICDAZX90dLQ8PDyUmZlpjuXn52vHjh3q3LmzJCk2NlZOp1ObN282azZt2iSn0+lSs2PHDuXn55s1q1evlpeXl6Kjoy/nFAEAAAA0YO5WXrykpETffPON+Xnv3r3Kzc2Vv7+/QkND9fvf/17btm3Tv/71L1VVVZmrWP7+/vL09JTdbtfQoUOVnJysgIAA+fv7a/z48YqKilLPnj0lSa1bt1afPn00bNgwLViwQJI0fPhwxcfHq1WrVpKkuLg4tWnTRg6HQzNnztTRo0c1fvx4DRs2jFUvAAAAAJeNpYFs69at6t69u/l53LhxkqQhQ4YoJSVF7777riTptttuczlu3bp16tatmyRp9uzZcnd316BBg1RWVqYePXpo6dKlcnNzM+vfeOMNjR071nwbY0JCgstvn7m5uen999/XyJEj1aVLF3l7eysxMVEvvPDC5Zg2AAAAAEiqQ79DVh/wO2QAAKCm+B2yuo3fIcPFqpe/QwYAAAAA9QmBDAAAAAAsQiADAAAAAItY+lIPAD/juYG6i2cGAADA5cQKGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFjE0kD26aefqn///goNDZXNZtOqVatc9huGoZSUFIWGhsrb21vdunXTzp07XWrKy8s1ZswYBQYGysfHRwkJCTpw4IBLTVFRkRwOh+x2u+x2uxwOh44dO+ZSk5eXp/79+8vHx0eBgYEaO3asKioqLse0AQAAAECSxYGstLRU7dq1U1pa2ln3z5gxQ7NmzVJaWpq2bNmikJAQ9erVS8ePHzdrkpKStHLlSqWnp2vDhg0qKSlRfHy8qqqqzJrExETl5uYqIyNDGRkZys3NlcPhMPdXVVWpX79+Ki0t1YYNG5Senq63335bycnJl2/yAAAAABo8dysv3rdvX/Xt2/es+wzD0Jw5czR58mQNHDhQkrRs2TIFBwdrxYoVGjFihJxOpxYvXqzXX39dPXv2lCQtX75cYWFhWrNmjXr37q3du3crIyND2dnZiomJkSQtWrRIsbGx2rNnj1q1aqXVq1dr165d2r9/v0JDQyVJL774oh5++GE9++yz8vPzuwJ/GwAAAAAamjr7DNnevXtVUFCguLg4c8zLy0tdu3bVxo0bJUk5OTmqrKx0qQkNDVVkZKRZk5WVJbvdboYxSerUqZPsdrtLTWRkpBnGJKl3794qLy9XTk7OOXssLy9XcXGxywYAAAAAF6rOBrKCggJJUnBwsMt4cHCwua+goECenp5q2rTpeWuCgoKqnT8oKMil5szrNG3aVJ6enmbN2aSmpprPpdntdoWFhV3kLAEAAAA0ZJbesnghbDaby2fDMKqNnenMmrPV16TmTJMmTdK4cePMz8XFxYQyAECNRE94zeoWcA45Mwdb3QKAeqzOrpCFhIRIUrUVqsLCQnM1KyQkRBUVFSoqKjpvzaFDh6qd//Dhwy41Z16nqKhIlZWV1VbOfsnLy0t+fn4uGwAAAABcqDobyMLDwxUSEqLMzExzrKKiQuvXr1fnzp0lSdHR0fLw8HCpyc/P144dO8ya2NhYOZ1Obd682azZtGmTnE6nS82OHTuUn59v1qxevVpeXl6Kjo6+rPMEAAAA0HBZestiSUmJvvnmG/Pz3r17lZubK39/fzVr1kxJSUmaPn26IiIiFBERoenTp6tx48ZKTEyUJNntdg0dOlTJyckKCAiQv7+/xo8fr6ioKPOti61bt1afPn00bNgwLViwQJI0fPhwxcfHq1WrVpKkuLg4tWnTRg6HQzNnztTRo0c1fvx4DRs2jFUvAAAAAJeNpYFs69at6t69u/n59PNYQ4YM0dKlSzVx4kSVlZVp5MiRKioqUkxMjFavXi1fX1/zmNmzZ8vd3V2DBg1SWVmZevTooaVLl8rNzc2seeONNzR27FjzbYwJCQkuv33m5uam999/XyNHjlSXLl3k7e2txMREvfDCC5f7rwAAAABAA2YzDMOwuon6ori4WHa7XU6nk5U1XBQe5q+7eJgfVwr/HKi7rtQ/B/gO1G38+wAX60KzQZ19hgwAAAAA6jsCGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWqVEg27t3b233AQAAAAANTo0C2c0336zu3btr+fLlOnHiRG33BAAAAAANQo0C2RdffKH27dsrOTlZISEhGjFihDZv3lzbvQEAAABAvVajQBYZGalZs2bp4MGDWrJkiQoKCnTnnXeqbdu2mjVrlg4fPlzbfQIAAABAvXNJL/Vwd3fXvffeq7/97W96/vnn9e2332r8+PG64YYbNHjwYOXn59dWnwAAAABQ71xSINu6datGjhyp66+/XrNmzdL48eP17bffau3atTp48KDuueee2uoTAAAAAOod95ocNGvWLC1ZskR79uzRb3/7W7322mv67W9/q2uu+TnfhYeHa8GCBbrllltqtVkAAAAAqE9qFMjmz5+vP/7xj3rkkUcUEhJy1ppmzZpp8eLFl9QcAAAAANRnNQpkX3/99a/WeHp6asiQITU5PQAAAAA0CDV6hmzJkiX6+9//Xm3873//u5YtW3bJTQEAAABAQ1CjQPbcc88pMDCw2nhQUJCmT59+yU0BAAAAQENQo0C2b98+hYeHVxtv3ry58vLyLrkpAAAAAGgIavQMWVBQkLZv364WLVq4jH/xxRcKCAiojb4AAACABiV6wmtWt4DzyJk5+LKct0YrZPfff7/Gjh2rdevWqaqqSlVVVVq7dq0ef/xx3X///bXdIwAAAADUSzVaIZs2bZr27dunHj16yN3951OcOnVKgwcP5hkyAAAAALhANQpknp6eeuutt/Q///M/+uKLL+Tt7a2oqCg1b968tvsDAAAAgHqrRoHstJYtW6ply5a11QsAAAAANCg1CmRVVVVaunSpPv74YxUWFurUqVMu+9euXVsrzQEAAABAfVajQPb4449r6dKl6tevnyIjI2Wz2Wq7LwAAAACo92oUyNLT0/W3v/1Nv/3tb2u7HwAAAABoMGr02ntPT0/dfPPNtd0LAAAAADQoNQpkycnJeumll2QYRm33AwAAAAANRo1uWdywYYPWrVunDz/8UG3btpWHh4fL/nfeeadWmgMAAACA+qxGgaxJkya69957a7sXAAAAAGhQahTIlixZUtt9AAAAAECDU6NnyCTp5MmTWrNmjRYsWKDjx49Lkn744QeVlJTUWnMAAAAAUJ/VaIVs37596tOnj/Ly8lReXq5evXrJ19dXM2bM0IkTJ/TKK6/Udp8AAAAAUO/UaIXs8ccfV8eOHVVUVCRvb29z/N5779XHH39ca80BAAAAQH1W47csfvbZZ/L09HQZb968uQ4ePFgrjQEAAABAfVejFbJTp06pqqqq2viBAwfk6+t7yU0BAAAAQENQo0DWq1cvzZkzx/xss9lUUlKiZ555Rr/97W9rqzedPHlS//3f/63w8HB5e3vrxhtv1F/+8hedOnXKrDEMQykpKQoNDZW3t7e6deumnTt3upynvLxcY8aMUWBgoHx8fJSQkKADBw641BQVFcnhcMhut8tut8vhcOjYsWO1NhcAAAAAOFONAtns2bO1fv16tWnTRidOnFBiYqJatGihgwcP6vnnn6+15p5//nm98sorSktL0+7duzVjxgzNnDlTc+fONWtmzJihWbNmKS0tTVu2bFFISIh69eplvvlRkpKSkrRy5Uqlp6drw4YNKikpUXx8vMsqX2JionJzc5WRkaGMjAzl5ubK4XDU2lwAAAAA4Ew1eoYsNDRUubm5evPNN7Vt2zadOnVKQ4cO1YMPPujyko9LlZWVpXvuuUf9+vWTJLVo0UJvvvmmtm7dKunn1bE5c+Zo8uTJGjhwoCRp2bJlCg4O1ooVKzRixAg5nU4tXrxYr7/+unr27ClJWr58ucLCwrRmzRr17t1bu3fvVkZGhrKzsxUTEyNJWrRokWJjY7Vnzx61atXqrP2Vl5ervLzc/FxcXFxrcwcAAABQ/9X4d8i8vb31xz/+UWlpaZo3b54effTRWg1jknTnnXfq448/1ldffSVJ+uKLL7Rhwwbztsi9e/eqoKBAcXFx5jFeXl7q2rWrNm7cKEnKyclRZWWlS01oaKgiIyPNmqysLNntdjOMSVKnTp1kt9vNmrNJTU01b3G02+0KCwurvckDAAAAqPdqtEL22muvnXf/4MGDa9TMmZ588kk5nU7dcsstcnNzU1VVlZ599lk98MADkqSCggJJUnBwsMtxwcHB2rdvn1nj6emppk2bVqs5fXxBQYGCgoKqXT8oKMisOZtJkyZp3Lhx5ufi4mJCGQAAAIALVqNA9vjjj7t8rqys1E8//SRPT081bty41gLZW2+9peXLl2vFihVq27atcnNzlZSUpNDQUA0ZMsSss9lsLscZhlFt7Exn1pyt/tfO4+XlJS8vrwudDgAAAAC4qNEti0VFRS5bSUmJ9uzZozvvvFNvvvlmrTU3YcIEPfXUU7r//vsVFRUlh8OhJ554QqmpqZKkkJAQSaq2ilVYWGiumoWEhKiiokJFRUXnrTl06FC16x8+fLja6hsAAAAA1JYaP0N2poiICD333HPVVs8uxU8//aRrrnFt0c3NzXztfXh4uEJCQpSZmWnur6io0Pr169W5c2dJUnR0tDw8PFxq8vPztWPHDrMmNjZWTqdTmzdvNms2bdokp9Np1gAAAABAbavRLYvn4ubmph9++KHWzte/f389++yzatasmdq2bavPP/9cs2bN0h//+EdJP99mmJSUpOnTpysiIkIRERGaPn26GjdurMTEREmS3W7X0KFDlZycrICAAPn7+2v8+PGKiooy37rYunVr9enTR8OGDdOCBQskScOHD1d8fPw537AIAAAAAJeqRoHs3XffdflsGIby8/OVlpamLl261EpjkjR37lxNmTJFI0eOVGFhoUJDQzVixAg9/fTTZs3EiRNVVlamkSNHqqioSDExMVq9erV8fX3NmtmzZ8vd3V2DBg1SWVmZevTooaVLl8rNzc2seeONNzR27FjzbYwJCQlKS0urtbkAAAAAwJlshmEYF3vQmbcR2mw2XXfddfqv//ovvfjii7r++utrrcGrSXFxsex2u5xOp/z8/KxuB1eR6Annf3MprJMzs3ZeUgT8Gv45UHddqX8O8B2o267E94DvQN12sd+BC80GNVohO/0MFwAAAACg5mrtpR4AAAAAgItToxWyX/4Y8q+ZNWtWTS4BAAAAAPVejQLZ559/rm3btunkyZPmWwi/+uorubm5qUOHDmbdr/04MwAAAAA0ZDUKZP3795evr6+WLVumpk2bSvr5x6IfeeQR3XXXXUpOTq7VJgEAAACgPqrRM2QvvviiUlNTzTAmSU2bNtW0adP04osv1lpzAAAAAFCf1SiQFRcX69ChQ9XGCwsLdfz48UtuCgAAAAAaghoFsnvvvVePPPKI/vGPf+jAgQM6cOCA/vGPf2jo0KEaOHBgbfcIAAAAAPVSjZ4he+WVVzR+/Hg99NBDqqys/PlE7u4aOnSoZs6cWasNAkBDwI+B1l38ODgA4HKqUSBr3Lix5s2bp5kzZ+rbb7+VYRi6+eab5ePjU9v9AQAAAEC9dUk/DJ2fn6/8/Hy1bNlSPj4+MgyjtvoCAAAAgHqvRitkR44c0aBBg7Ru3TrZbDZ9/fXXuvHGG/Xoo4+qSZMmvGnxInGrUt3FrUoAAAC4nGq0QvbEE0/Iw8NDeXl5aty4sTl+3333KSMjo9aaAwAAAID6rEYrZKtXr9ZHH32kG264wWU8IiJC+/btq5XGAAAAAKC+q9EKWWlpqcvK2Gk//vijvLy8LrkpAAAAAGgIahTI7r77br322v9/7slms+nUqVOaOXOmunfvXmvNAQAAAEB9VqNbFmfOnKlu3bpp69atqqio0MSJE7Vz504dPXpUn332WW33CAAAAAD1Uo1WyNq0aaPt27frjjvuUK9evVRaWqqBAwfq888/10033VTbPQIAAABAvXTRK2SVlZWKi4vTggULNHXq1MvREwAAAAA0CBe9Qubh4aEdO3bIZrNdjn4AAAAAoMGo0S2LgwcP1uLFi2u7FwAAAABoUGr0Uo+Kigr99a9/VWZmpjp27CgfHx+X/bNmzaqV5gAAAACgPruoQPbdd9+pRYsW2rFjhzp06CBJ+uqrr1xquJURAAAAAC7MRQWyiIgI5efna926dZKk++67Ty+//LKCg4MvS3MAAAAAUJ9d1DNkhmG4fP7www9VWlpaqw0BAAAAQENRo5d6nHZmQAMAAAAAXLiLCmQ2m63aM2I8MwYAAAAANXNRz5AZhqGHH35YXl5ekqQTJ07oscceq/aWxXfeeaf2OgQAAACAeuqiAtmQIUNcPj/00EO12gwAAAAANCQXFciWLFlyufoAAAAAgAbnkl7qAQAAAACoOQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYpM4HsoMHD+qhhx5SQECAGjdurNtuu005OTnmfsMwlJKSotDQUHl7e6tbt27auXOnyznKy8s1ZswYBQYGysfHRwkJCTpw4IBLTVFRkRwOh+x2u+x2uxwOh44dO3YlpggAAACggarTgayoqEhdunSRh4eHPvzwQ+3atUsvvviimjRpYtbMmDFDs2bNUlpamrZs2aKQkBD16tVLx48fN2uSkpK0cuVKpaena8OGDSopKVF8fLyqqqrMmsTEROXm5iojI0MZGRnKzc2Vw+G4ktMFAAAA0MBc1A9DX2nPP/+8wsLCXH6QukWLFuafDcPQnDlzNHnyZA0cOFCStGzZMgUHB2vFihUaMWKEnE6nFi9erNdff109e/aUJC1fvlxhYWFas2aNevfurd27dysjI0PZ2dmKiYmRJC1atEixsbHas2ePWrVqdeUmDQAAAKDBqNMrZO+++646duyoP/zhDwoKClL79u21aNEic//evXtVUFCguLg4c8zLy0tdu3bVxo0bJUk5OTmqrKx0qQkNDVVkZKRZk5WVJbvdboYxSerUqZPsdrtZczbl5eUqLi522QAAAADgQtXpQPbdd99p/vz5ioiI0EcffaTHHntMY8eO1WuvvSZJKigokCQFBwe7HBccHGzuKygokKenp5o2bXremqCgoGrXDwoKMmvOJjU11XzmzG63KywsrOaTBQAAANDg1OlAdurUKXXo0EHTp09X+/btNWLECA0bNkzz5893qbPZbC6fDcOoNnamM2vOVv9r55k0aZKcTqe57d+//0KmBQAAAACS6nggu/7669WmTRuXsdatWysvL0+SFBISIknVVrEKCwvNVbOQkBBVVFSoqKjovDWHDh2qdv3Dhw9XW337JS8vL/n5+blsAAAAAHCh6nQg69Kli/bs2eMy9tVXX6l58+aSpPDwcIWEhCgzM9PcX1FRofXr16tz586SpOjoaHl4eLjU5Ofna8eOHWZNbGysnE6nNm/ebNZs2rRJTqfTrAEAAACA2lan37L4xBNPqHPnzpo+fboGDRqkzZs3a+HChVq4cKGkn28zTEpK0vTp0xUREaGIiAhNnz5djRs3VmJioiTJbrdr6NChSk5OVkBAgPz9/TV+/HhFRUWZb11s3bq1+vTpo2HDhmnBggWSpOHDhys+Pp43LAIAAAC4bOp0ILv99tu1cuVKTZo0SX/5y18UHh6uOXPm6MEHHzRrJk6cqLKyMo0cOVJFRUWKiYnR6tWr5evra9bMnj1b7u7uGjRokMrKytSjRw8tXbpUbm5uZs0bb7yhsWPHmm9jTEhIUFpa2pWbLAAAAIAGp04HMkmKj49XfHz8OffbbDalpKQoJSXlnDWNGjXS3LlzNXfu3HPW+Pv7a/ny5ZfSKgAAAABclDr9DBkAAAAA1GcEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCJXVSBLTU2VzWZTUlKSOWYYhlJSUhQaGipvb29169ZNO3fudDmuvLxcY8aMUWBgoHx8fJSQkKADBw641BQVFcnhcMhut8tut8vhcOjYsWNXYFYAAAAAGqqrJpBt2bJFCxcu1K233uoyPmPGDM2aNUtpaWnasmWLQkJC1KtXLx0/ftysSUpK0sqVK5Wenq4NGzaopKRE8fHxqqqqMmsSExOVm5urjIwMZWRkKDc3Vw6H44rNDwAAAEDDc1UEspKSEj344INatGiRmjZtao4bhqE5c+Zo8uTJGjhwoCIjI7Vs2TL99NNPWrFihSTJ6XRq8eLFevHFF9WzZ0+1b99ey5cv15dffqk1a9ZIknbv3q2MjAz99a9/VWxsrGJjY7Vo0SL961//0p49eyyZMwAAAID676oIZKNGjVK/fv3Us2dPl/G9e/eqoKBAcXFx5piXl5e6du2qjRs3SpJycnJUWVnpUhMaGqrIyEizJisrS3a7XTExMWZNp06dZLfbzZqzKS8vV3FxscsGAAAAABfK3eoGfk16erq2bdumLVu2VNtXUFAgSQoODnYZDw4O1r59+8waT09Pl5W10zWnjy8oKFBQUFC18wcFBZk1Z5OamqqpU6de3IQAAAAA4P/U6RWy/fv36/HHH9fy5cvVqFGjc9bZbDaXz4ZhVBs705k1Z6v/tfNMmjRJTqfT3Pbv33/eawIAAADAL9XpQJaTk6PCwkJFR0fL3d1d7u7uWr9+vV5++WW5u7ubK2NnrmIVFhaa+0JCQlRRUaGioqLz1hw6dKja9Q8fPlxt9e2XvLy85Ofn57IBAAAAwIWq04GsR48e+vLLL5Wbm2tuHTt21IMPPqjc3FzdeOONCgkJUWZmpnlMRUWF1q9fr86dO0uSoqOj5eHh4VKTn5+vHTt2mDWxsbFyOp3avHmzWbNp0yY5nU6zBgAAAABqW51+hszX11eRkZEuYz4+PgoICDDHk5KSNH36dEVERCgiIkLTp09X48aNlZiYKEmy2+0aOnSokpOTFRAQIH9/f40fP15RUVHmS0Jat26tPn36aNiwYVqwYIEkafjw4YqPj1erVq2u4IwBAAAANCR1OpBdiIkTJ6qsrEwjR45UUVGRYmJitHr1avn6+po1s2fPlru7uwYNGqSysjL16NFDS5culZubm1nzxhtvaOzYsebbGBMSEpSWlnbF5wMAAACg4bjqAtknn3zi8tlmsyklJUUpKSnnPKZRo0aaO3eu5s6de84af39/LV++vJa6BAAAAIBfV6efIQMAAACA+oxABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWKROB7LU1FTdfvvt8vX1VVBQkAYMGKA9e/a41BiGoZSUFIWGhsrb21vdunXTzp07XWrKy8s1ZswYBQYGysfHRwkJCTpw4IBLTVFRkRwOh+x2u+x2uxwOh44dO3a5pwgAAACgAavTgWz9+vUaNWqUsrOzlZmZqZMnTyouLk6lpaVmzYwZMzRr1iylpaVpy5YtCgkJUa9evXT8+HGzJikpSStXrlR6ero2bNigkpISxcfHq6qqyqxJTExUbm6uMjIylJGRodzcXDkcjis6XwAAAAANi7vVDZxPRkaGy+clS5YoKChIOTk5uvvuu2UYhubMmaPJkydr4MCBkqRly5YpODhYK1as0IgRI+R0OrV48WK9/vrr6tmzpyRp+fLlCgsL05o1a9S7d2/t3r1bGRkZys7OVkxMjCRp0aJFio2N1Z49e9SqVasrO3EAAAAADUKdXiE7k9PplCT5+/tLkvbu3auCggLFxcWZNV5eXuratas2btwoScrJyVFlZaVLTWhoqCIjI82arKws2e12M4xJUqdOnWS3282asykvL1dxcbHLBgAAAAAX6qoJZIZhaNy4cbrzzjsVGRkpSSooKJAkBQcHu9QGBweb+woKCuTp6ammTZuetyYoKKjaNYOCgsyas0lNTTWfObPb7QoLC6v5BAEAAAA0OFdNIBs9erS2b9+uN998s9o+m83m8tkwjGpjZzqz5mz1v3aeSZMmyel0mtv+/ft/bRoAAAAAYLoqAtmYMWP07rvvat26dbrhhhvM8ZCQEEmqtopVWFhorpqFhISooqJCRUVF5605dOhQtesePny42urbL3l5ecnPz89lAwAAAIALVacDmWEYGj16tN555x2tXbtW4eHhLvvDw8MVEhKizMxMc6yiokLr169X586dJUnR0dHy8PBwqcnPz9eOHTvMmtjYWDmdTm3evNms2bRpk5xOp1kDAAAAALWtTr9lcdSoUVqxYoX++c9/ytfX11wJs9vt8vb2ls1mU1JSkqZPn66IiAhFRERo+vTpaty4sRITE83aoUOHKjk5WQEBAfL399f48eMVFRVlvnWxdevW6tOnj4YNG6YFCxZIkoYPH674+HjesAgAAADgsqnTgWz+/PmSpG7durmML1myRA8//LAkaeLEiSorK9PIkSNVVFSkmJgYrV69Wr6+vmb97Nmz5e7urkGDBqmsrEw9evTQ0qVL5ebmZta88cYbGjt2rPk2xoSEBKWlpV3eCQIAAABo0Op0IDMM41drbDabUlJSlJKScs6aRo0aae7cuZo7d+45a/z9/bV8+fKatAkAAAAANVKnnyEDAAAAgPqMQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQHaGefPmKTw8XI0aNVJ0dLT+/e9/W90SAAAAgHqKQPYLb731lpKSkjR58mR9/vnnuuuuu9S3b1/l5eVZ3RoAAACAeohA9guzZs3S0KFD9eijj6p169aaM2eOwsLCNH/+fKtbAwAAAFAPuVvdQF1RUVGhnJwcPfXUUy7jcXFx2rhx41mPKS8vV3l5ufnZ6XRKkoqLiy/q2lXlZRfZLa6Ui/3Psqb4DtRdfAfAdwB8ByBdme8B34G67WK/A6frDcM4b53N+LWKBuKHH37Qb37zG3322Wfq3LmzOT59+nQtW7ZMe/bsqXZMSkqKpk6deiXbBAAAAHAV2b9/v2644YZz7meF7Aw2m83ls2EY1cZOmzRpksaNG2d+PnXqlI4ePaqAgIBzHlOfFRcXKywsTPv375efn5/V7cACfAcg8T0A3wHwHQDfAennHHH8+HGFhoaet45A9n8CAwPl5uamgoICl/HCwkIFBwef9RgvLy95eXm5jDVp0uRytXjV8PPza7D/xcPP+A5A4nsAvgPgOwC+A3a7/VdreKnH//H09FR0dLQyMzNdxjMzM11uYQQAAACA2sIK2S+MGzdODodDHTt2VGxsrBYuXKi8vDw99thjVrcGAAAAoB4ikP3CfffdpyNHjugvf/mL8vPzFRkZqQ8++EDNmze3urWrgpeXl5555plqt3Gi4eA7AInvAfgOgO8A+A5cDN6yCAAAAAAW4RkyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMtSaefPmKTw8XI0aNVJ0dLT+/e9/W90SrqBPP/1U/fv3V2hoqGw2m1atWmV1S7iCUlNTdfvtt8vX11dBQUEaMGCA9uzZY3VbuILmz5+vW2+91fwR2NjYWH344YdWtwULpaamymazKSkpyepWcAWlpKTIZrO5bCEhIVa3VacRyFAr3nrrLSUlJWny5Mn6/PPPddddd6lv377Ky8uzujVcIaWlpWrXrp3S0tKsbgUWWL9+vUaNGqXs7GxlZmbq5MmTiouLU2lpqdWt4Qq54YYb9Nxzz2nr1q3aunWr/uu//kv33HOPdu7caXVrsMCWLVu0cOFC3XrrrVa3Agu0bdtW+fn55vbll19a3VKdxmvvUStiYmLUoUMHzZ8/3xxr3bq1BgwYoNTUVAs7gxVsNptWrlypAQMGWN0KLHL48GEFBQVp/fr1uvvuu61uBxbx9/fXzJkzNXToUKtbwRVUUlKiDh06aN68eZo2bZpuu+02zZkzx+q2cIWkpKRo1apVys3NtbqVqwYrZLhkFRUVysnJUVxcnMt4XFycNm7caFFXAKzkdDol/fw/yNHwVFVVKT09XaWlpYqNjbW6HVxho0aNUr9+/dSzZ0+rW4FFvv76a4WGhio8PFz333+/vvvuO6tbqtPcrW4AV78ff/xRVVVVCg4OdhkPDg5WQUGBRV0BsIphGBo3bpzuvPNORUZGWt0OrqAvv/xSsbGxOnHihK699lqtXLlSbdq0sbotXEHp6enatm2btmzZYnUrsEhMTIxee+01tWzZUocOHdK0adPUuXNn7dy5UwEBAVa3VycRyFBrbDaby2fDMKqNAaj/Ro8ere3bt2vDhg1Wt4IrrFWrVsrNzdWxY8f09ttva8iQIVq/fj2hrIHYv3+/Hn/8ca1evVqNGjWyuh1YpG/fvuafo6KiFBsbq5tuuknLli3TuHHjLOys7iKQ4ZIFBgbKzc2t2mpYYWFhtVUzAPXbmDFj9O677+rTTz/VDTfcYHU7uMI8PT118803S5I6duyoLVu26KWXXtKCBQss7gxXQk5OjgoLCxUdHW2OVVVV6dNPP1VaWprKy8vl5uZmYYewgo+Pj6KiovT1119b3UqdxTNkuGSenp6Kjo5WZmamy3hmZqY6d+5sUVcAriTDMDR69Gi98847Wrt2rcLDw61uCXWAYRgqLy+3ug1cIT169NCXX36p3Nxcc+vYsaMefPBB5ebmEsYaqPLycu3evVvXX3+91a3UWayQoVaMGzdODodDHTt2VGxsrBYuXKi8vDw99thjVreGK6SkpETffPON+Xnv3r3Kzc2Vv7+/mjVrZmFnuBJGjRqlFStW6J///Kd8fX3NFXO73S5vb2+Lu8OV8Oc//1l9+/ZVWFiYjh8/rvT0dH3yySfKyMiwujVcIb6+vtWeG/Xx8VFAQADPkzYg48ePV//+/dWsWTMVFhZq2rRpKi4u1pAhQ6xurc4ikKFW3HfffTpy5Ij+8pe/KD8/X5GRkfrggw/UvHlzq1vDFbJ161Z1797d/Hz6PvEhQ4Zo6dKlFnWFK+X0T15069bNZXzJkiV6+OGHr3xDuOIOHTokh8Oh/Px82e123XrrrcrIyFCvXr2sbg3AFXTgwAE98MAD+vHHH3XdddepU6dOys7O5n8Tnge/QwYAAAAAFuEZMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAABqwGazadWqVVa3AQC4yhHIAAA4i4KCAo0ZM0Y33nijvLy8FBYWpv79++vjjz+2ujUAQD3ibnUDAADUNd9//726dOmiJk2aaMaMGbr11ltVWVmpjz76SKNGjdJ//vMfq1sEANQTrJABAHCGkSNHymazafPmzfr973+vli1bqm3btho3bpyys7PPesyTTz6pli1bqnHjxrrxxhs1ZcoUVVZWmvu/+OILde/eXb6+vvLz81N0dLS2bt0qSdq3b5/69++vpk2bysfHR23bttUHH3xwReYKALAWK2QAAPzC0aNHlZGRoWeffVY+Pj7V9jdp0uSsx/n6+mrp0qUKDQ3Vl19+qWHDhsnX11cTJ06UJD344INq37695s+fLzc3N+Xm5srDw0OSNGrUKFVUVOjTTz+Vj4+Pdu3apWuvvfayzREAUHcQyAAA+IVvvvlGhmHolltuuajj/vu//9v8c4sWLZScnKy33nrLDGR5eXmaMGGCed6IiAizPi8vT7/73e8UFRUlSbrxxhsvdRoAgKsEtywCAPALhmFI+vktihfjH//4h+68806FhITo2muv1ZQpU5SXl2fuHzdunB599FH17NlTzz33nL799ltz39ixYzVt2jR16dJFzzzzjLZv3147kwEA1HkEMgAAfiEiIkI2m027d+++4GOys7N1//33q2/fvvrXv/6lzz//XJMnT1ZFRYVZk5KSop07d6pfv35au3at2rRpo5UrV0qSHn30UX333XdyOBz68ssv1bFjR82dO7fW5wYAqHtsxun/KxAAAEiS+vbtqy+//FJ79uyp9hzZsWPH1KRJE9lsNq1cuVIDBgzQiy++qHnz5rmsej366KP6xz/+oWPHjp31Gg888IBKS0v17rvvVts3adIkvf/++6yUAUADwAoZAABnmDdvnqqqqnTHHXfo7bff1tdff63du3fr5ZdfVmxsbLX6m2++WXl5eUpPT9e3336rl19+2Vz9kqSysjKNHj1an3zyifbt26fPPvtMW7ZsUevWrSVJSUlJ+uijj7R3715t27ZNa9euNfcBAOo3XuoBAMAZwsPDtW3bNj377LNKTk5Wfn6+rrvuOkVHR2v+/PnV6u+55x498cQTGj16tMrLy9WvXz9NmTJFKSkpkiQ3NzcdOXJEgwcP1qFDhxQYGKiBAwdq6tSpkqSqqiqNGjVKBw4ckJ+fn/r06aPZs2dfySkDACzCLYsAAAAAYBFuWQQAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwyP8DbNwk+FIWa4oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model1: (array([0, 1]), array([ 7226, 53089]))\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - accuracy: 0.7102 - loss: 0.7843 - val_accuracy: 0.9613 - val_loss: 0.1497 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9147 - loss: 0.2550 - val_accuracy: 0.9700 - val_loss: 0.1014 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9435 - loss: 0.1717 - val_accuracy: 0.9718 - val_loss: 0.0963 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9572 - loss: 0.1393 - val_accuracy: 0.9745 - val_loss: 0.0853 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9650 - loss: 0.1194 - val_accuracy: 0.9749 - val_loss: 0.0802 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9684 - loss: 0.1039 - val_accuracy: 0.9770 - val_loss: 0.0747 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9721 - loss: 0.0931 - val_accuracy: 0.9783 - val_loss: 0.0671 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9746 - loss: 0.0844 - val_accuracy: 0.9801 - val_loss: 0.0628 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9758 - loss: 0.0761 - val_accuracy: 0.9809 - val_loss: 0.0572 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9790 - loss: 0.0667 - val_accuracy: 0.9821 - val_loss: 0.0521 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9804 - loss: 0.0607 - val_accuracy: 0.9834 - val_loss: 0.0503 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9829 - loss: 0.0550 - val_accuracy: 0.9843 - val_loss: 0.0473 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9831 - loss: 0.0515 - val_accuracy: 0.9847 - val_loss: 0.0472 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9845 - loss: 0.0475 - val_accuracy: 0.9855 - val_loss: 0.0468 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9848 - loss: 0.0439 - val_accuracy: 0.9865 - val_loss: 0.0444 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9859 - loss: 0.0418 - val_accuracy: 0.9864 - val_loss: 0.0433 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9861 - loss: 0.0409 - val_accuracy: 0.9864 - val_loss: 0.0443 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9867 - loss: 0.0380 - val_accuracy: 0.9875 - val_loss: 0.0416 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9875 - loss: 0.0374 - val_accuracy: 0.9879 - val_loss: 0.0425 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9868 - loss: 0.0377 - val_accuracy: 0.9875 - val_loss: 0.0438 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9884 - loss: 0.0335 - val_accuracy: 0.9880 - val_loss: 0.0418 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9887 - loss: 0.0317 - val_accuracy: 0.9880 - val_loss: 0.0450 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9884 - loss: 0.0328 - val_accuracy: 0.9878 - val_loss: 0.0419 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9891 - loss: 0.0302 - val_accuracy: 0.9879 - val_loss: 0.0473 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9899 - loss: 0.0287 - val_accuracy: 0.9879 - val_loss: 0.0476 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9892 - loss: 0.0293 - val_accuracy: 0.9882 - val_loss: 0.0470 - learning_rate: 5.0000e-05\n",
      "Epoch 27/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9902 - loss: 0.0261 - val_accuracy: 0.9884 - val_loss: 0.0430 - learning_rate: 5.0000e-05\n",
      "Epoch 28/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9891 - loss: 0.0285 - val_accuracy: 0.9881 - val_loss: 0.0470 - learning_rate: 5.0000e-05\n",
      "Epoch 29/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9905 - loss: 0.0253 - val_accuracy: 0.9885 - val_loss: 0.0462 - learning_rate: 5.0000e-05\n",
      "Epoch 30/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9905 - loss: 0.0261 - val_accuracy: 0.9889 - val_loss: 0.0468 - learning_rate: 5.0000e-05\n",
      "Epoch 31/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9894 - loss: 0.0271 - val_accuracy: 0.9887 - val_loss: 0.0475 - learning_rate: 5.0000e-05\n",
      "Epoch 32/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9899 - loss: 0.0252 - val_accuracy: 0.9887 - val_loss: 0.0494 - learning_rate: 5.0000e-05\n",
      "Epoch 33/150\n",
      "\u001b[1m664/664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9905 - loss: 0.0243 - val_accuracy: 0.9885 - val_loss: 0.0496 - learning_rate: 2.5000e-05\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0418\n",
      "Test Accuracy: 0.9885100722312927\n",
      "\u001b[1m 48/332\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Persister       0.98      0.99      0.99      5255\n",
      "    Persister       0.99      0.98      0.99      5363\n",
      "\n",
      "     accuracy                           0.99     10618\n",
      "    macro avg       0.99      0.99      0.99     10618\n",
      " weighted avg       0.99      0.99      0.99     10618\n",
      "\n",
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.6431 - loss: 0.9372 - val_accuracy: 0.8854 - val_loss: 0.2781 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.8481 - loss: 0.3913 - val_accuracy: 0.9360 - val_loss: 0.1908 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.8961 - loss: 0.2899 - val_accuracy: 0.9438 - val_loss: 0.1791 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9162 - loss: 0.2471 - val_accuracy: 0.9485 - val_loss: 0.1683 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9263 - loss: 0.2254 - val_accuracy: 0.9479 - val_loss: 0.1687 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9332 - loss: 0.2074 - val_accuracy: 0.9501 - val_loss: 0.1647 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9410 - loss: 0.1903 - val_accuracy: 0.9510 - val_loss: 0.1622 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9452 - loss: 0.1770 - val_accuracy: 0.9529 - val_loss: 0.1549 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9476 - loss: 0.1691 - val_accuracy: 0.9529 - val_loss: 0.1532 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9498 - loss: 0.1604 - val_accuracy: 0.9545 - val_loss: 0.1472 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9537 - loss: 0.1514 - val_accuracy: 0.9566 - val_loss: 0.1395 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9558 - loss: 0.1414 - val_accuracy: 0.9572 - val_loss: 0.1358 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9591 - loss: 0.1290 - val_accuracy: 0.9572 - val_loss: 0.1420 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9603 - loss: 0.1237 - val_accuracy: 0.9581 - val_loss: 0.1343 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9621 - loss: 0.1159 - val_accuracy: 0.9598 - val_loss: 0.1310 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9638 - loss: 0.1089 - val_accuracy: 0.9609 - val_loss: 0.1227 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9650 - loss: 0.1021 - val_accuracy: 0.9601 - val_loss: 0.1246 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9669 - loss: 0.0971 - val_accuracy: 0.9610 - val_loss: 0.1235 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9672 - loss: 0.0948 - val_accuracy: 0.9625 - val_loss: 0.1209 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9684 - loss: 0.0893 - val_accuracy: 0.9630 - val_loss: 0.1225 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9709 - loss: 0.0845 - val_accuracy: 0.9638 - val_loss: 0.1101 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9723 - loss: 0.0773 - val_accuracy: 0.9645 - val_loss: 0.1129 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9707 - loss: 0.0811 - val_accuracy: 0.9645 - val_loss: 0.1191 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9730 - loss: 0.0753 - val_accuracy: 0.9635 - val_loss: 0.1326 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9747 - loss: 0.0715 - val_accuracy: 0.9637 - val_loss: 0.1262 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9738 - loss: 0.0737 - val_accuracy: 0.9648 - val_loss: 0.1135 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9762 - loss: 0.0655 - val_accuracy: 0.9656 - val_loss: 0.1083 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9751 - loss: 0.0671 - val_accuracy: 0.9651 - val_loss: 0.1261 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9754 - loss: 0.0675 - val_accuracy: 0.9663 - val_loss: 0.1208 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9766 - loss: 0.0627 - val_accuracy: 0.9663 - val_loss: 0.1170 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9774 - loss: 0.0606 - val_accuracy: 0.9657 - val_loss: 0.1172 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9785 - loss: 0.0588 - val_accuracy: 0.9657 - val_loss: 0.1114 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9786 - loss: 0.0576 - val_accuracy: 0.9677 - val_loss: 0.1128 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9782 - loss: 0.0578 - val_accuracy: 0.9667 - val_loss: 0.1188 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9787 - loss: 0.0553 - val_accuracy: 0.9663 - val_loss: 0.1238 - learning_rate: 5.0000e-05\n",
      "Epoch 36/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9804 - loss: 0.0542 - val_accuracy: 0.9674 - val_loss: 0.1188 - learning_rate: 5.0000e-05\n",
      "Epoch 37/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9801 - loss: 0.0530 - val_accuracy: 0.9667 - val_loss: 0.1334 - learning_rate: 5.0000e-05\n",
      "Epoch 38/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9795 - loss: 0.0513 - val_accuracy: 0.9678 - val_loss: 0.1185 - learning_rate: 5.0000e-05\n",
      "Epoch 39/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9796 - loss: 0.0533 - val_accuracy: 0.9677 - val_loss: 0.1278 - learning_rate: 5.0000e-05\n",
      "Epoch 40/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9809 - loss: 0.0504 - val_accuracy: 0.9676 - val_loss: 0.1177 - learning_rate: 5.0000e-05\n",
      "Epoch 41/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9806 - loss: 0.0489 - val_accuracy: 0.9680 - val_loss: 0.1282 - learning_rate: 5.0000e-05\n",
      "Epoch 42/150\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9813 - loss: 0.0476 - val_accuracy: 0.9680 - val_loss: 0.1224 - learning_rate: 2.5000e-05\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1008\n",
      "Test Accuracy: 0.9668684005737305\n",
      "\u001b[1m 24/305\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2010376/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Group1 (Dint divide)       0.96      0.97      0.97      4919\n",
      "     Group2 (Divide)       0.97      0.96      0.97      4830\n",
      "\n",
      "            accuracy                           0.97      9749\n",
      "           macro avg       0.97      0.97      0.97      9749\n",
      "        weighted avg       0.97      0.97      0.97      9749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LayerNormalization, MultiHeadAttention, Add, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Print initial data information\n",
    "print(\"Unique sample types:\", merged_data['sample_type'].unique())\n",
    "print(\"Sample type distribution:\\n\", merged_data['sample_type'].value_counts())\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "print(\"First few rows of merged data:\\n\", merged_data.head())\n",
    "\n",
    "# Plot class distribution\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=500)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Define the Transformer block\n",
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Add()([x, inputs])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Add()([x, inputs])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x\n",
    "\n",
    "# Custom layer for expanding dimensions\n",
    "class ExpandDimsLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=1)\n",
    "\n",
    "# Custom layer for squeezing dimensions\n",
    "class SqueezeDimsLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=1)\n",
    "\n",
    "# Define the Transformer model\n",
    "def create_transformer_model(input_dim, num_classes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = ExpandDimsLayer()(x)  # Expand dims to create a sequence dimension\n",
    "    x = transformer_block(x, head_size=64, num_heads=8, ff_dim=512, dropout=0.1)\n",
    "    x = SqueezeDimsLayer()(x)  # Remove the sequence dimension\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def model_1():\n",
    "    # Define the mappings based on sample_type\n",
    "    non_persister_samples = [0]\n",
    "    persister_samples = [\"7\",\"3\",\"14_high\",\"14_med\",\"14_low\"]\n",
    "\n",
    "    # Update labels: 0 = non-persister, 1 = persister\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the transformer model\n",
    "    model = create_transformer_model(X_train.shape[1], 2)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_transformer_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=150,\n",
    "              batch_size=128,\n",
    "              validation_data=(X_validation, y_validation),\n",
    "              class_weight=class_weight_dict,\n",
    "              callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    model.save('transformer_model1.keras')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = model.predict(X_test).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Non-Persister', 'Persister']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "def model_2():\n",
    "    # Define the mappings based on sample_type\n",
    "    group1_samples = [\"14_high\"]  # Dint divide\n",
    "    group2_samples = [\"14_med\", \"14_low\"]  # Divide\n",
    "\n",
    "    # Update labels: 0 = Group1 (Dint divide), 1 = Group2 (Divide)\n",
    "    y_model2 = np.where(merged_data['sample_type'].isin(group1_samples), 0, 1)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model2:\", np.unique(y_model2, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model2)) != 2:\n",
    "        print(\"Not enough classes in y_model2 for Model 2\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model2)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the transformer model\n",
    "    model = create_transformer_model(X_train.shape[1], 2)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_transformer_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=150,\n",
    "              batch_size=128,\n",
    "              validation_data=(X_validation, y_validation),\n",
    "              class_weight=class_weight_dict,\n",
    "              callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    model.save('transformer_model2.keras')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = model.predict(X_test).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Group1 (Dint divide)', 'Group2 (Divide)']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "# Execute Model 1\n",
    "model_1()\n",
    "\n",
    "# Execute Model 2\n",
    "model_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff20633-d1d0-4d1f-81f4-b6555bde84b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
