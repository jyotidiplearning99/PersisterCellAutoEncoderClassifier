{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0394ca35-3169-438f-b1f8-d0f36d1da994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 20:17:16.828000: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-28 20:17:21.945875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-28 20:17:49.154082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer  # Correct import for imputation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    return pd.read_csv(file_path, engine='python', encoding='utf-8', chunksize=chunk_size)\n",
    "\n",
    "def preprocess_data(metadata_path, data_path):\n",
    "    metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    scRNA_data = pd.concat(load_data_in_chunks(data_path)).transpose()\n",
    "\n",
    "    metadata_df.index.rename('cell', inplace=True)\n",
    "    metadata_df.reset_index(inplace=True)\n",
    "    metadata_df['cell'] = metadata_df['cell'].astype(str).str.split('-').str[0].str.strip().str.upper()\n",
    "    scRNA_data.reset_index(inplace=True)\n",
    "    scRNA_data.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    scRNA_data['cell'] = scRNA_data['cell'].astype(str).str.split('.').str[0].str.strip().str.upper()\n",
    "\n",
    "    common_cells = set(metadata_df['cell']).intersection(set(scRNA_data['cell']))\n",
    "\n",
    "    filtered_metadata_df = metadata_df[metadata_df['cell'].isin(common_cells)]\n",
    "    filtered_scRNA_data = scRNA_data[scRNA_data['cell'].isin(common_cells)]\n",
    "\n",
    "    merged_data = pd.merge(filtered_scRNA_data, filtered_metadata_df[['cell', 'sample_name', 'sample_type']], on='cell', how='inner')\n",
    "    if merged_data.empty:\n",
    "        print(\"No data was merged. Check cell identifiers and filters.\")\n",
    "        return None\n",
    "\n",
    "    X = merged_data.drop(columns=['cell', 'sample_name', 'sample_type'])\n",
    "    y = merged_data['sample_type']\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X, y, merged_data, label_encoder\n",
    "\n",
    "metadata_path = '/users/barmanjy/Desktop/Persister Cell 2/GSE150949_metaData_with_lineage.txt'\n",
    "data_path = '/scratch/project_2010376/normalized_GSE150949_pc9_count.csv'\n",
    "\n",
    "X, y, merged_data, label_encoder = preprocess_data(metadata_path, data_path)\n",
    "\n",
    "# Proceed with PCA\n",
    "pca = PCA(n_components=500)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "class_counts = np.bincount(y)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=np.arange(len(class_counts)), y=class_counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1400583-b13b-4b62-a579-392177a4eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_model1: (array([0, 1]), array([ 7226, 53089]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 20:54:36.727657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "664/664 - 21s - 32ms/step - loss: 1.0361 - val_loss: 1.0129 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9986 - val_loss: 1.0064 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9943 - val_loss: 1.0010 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9896 - val_loss: 0.9966 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9865 - val_loss: 0.9938 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9838 - val_loss: 0.9910 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9817 - val_loss: 0.9879 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9792 - val_loss: 0.9849 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9767 - val_loss: 0.9822 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9744 - val_loss: 0.9803 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9725 - val_loss: 0.9776 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9708 - val_loss: 0.9763 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9696 - val_loss: 0.9753 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9686 - val_loss: 0.9739 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9672 - val_loss: 0.9723 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9662 - val_loss: 0.9717 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9653 - val_loss: 0.9719 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9647 - val_loss: 0.9691 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9638 - val_loss: 0.9690 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9628 - val_loss: 0.9673 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9621 - val_loss: 0.9667 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9614 - val_loss: 0.9662 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9611 - val_loss: 0.9652 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9604 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9597 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9591 - val_loss: 0.9634 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9586 - val_loss: 0.9631 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9583 - val_loss: 0.9622 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9581 - val_loss: 0.9621 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9576 - val_loss: 0.9625 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9573 - val_loss: 0.9617 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9566 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9565 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9564 - val_loss: 0.9601 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9561 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9559 - val_loss: 0.9599 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9558 - val_loss: 0.9596 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9553 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9549 - val_loss: 0.9586 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9548 - val_loss: 0.9581 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9543 - val_loss: 0.9580 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9541 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9539 - val_loss: 0.9574 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9539 - val_loss: 0.9574 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9534 - val_loss: 0.9570 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9530 - val_loss: 0.9566 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9527 - val_loss: 0.9559 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9530 - val_loss: 0.9562 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9527 - val_loss: 0.9563 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9524 - val_loss: 0.9556 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9524 - val_loss: 0.9557 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9522 - val_loss: 0.9548 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9517 - val_loss: 0.9552 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9518 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9515 - val_loss: 0.9554 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9511 - val_loss: 0.9540 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9513 - val_loss: 0.9547 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9514 - val_loss: 0.9547 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9510 - val_loss: 0.9542 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9510 - val_loss: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9512 - val_loss: 0.9548 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9514 - val_loss: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9509 - val_loss: 0.9540 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9508 - val_loss: 0.9534 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9503 - val_loss: 0.9535 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9500 - val_loss: 0.9532 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9499 - val_loss: 0.9527 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9502 - val_loss: 0.9535 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9499 - val_loss: 0.9535 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9495 - val_loss: 0.9528 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9497 - val_loss: 0.9525 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9495 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9493 - val_loss: 0.9520 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9488 - val_loss: 0.9518 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9487 - val_loss: 0.9515 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9485 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9484 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9486 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9485 - val_loss: 0.9512 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9485 - val_loss: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9482 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9479 - val_loss: 0.9507 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9478 - val_loss: 0.9505 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9477 - val_loss: 0.9504 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9473 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9473 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9474 - val_loss: 0.9501 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9473 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9473 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9470 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9472 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9470 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9469 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9469 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9468 - val_loss: 0.9494 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9469 - val_loss: 0.9491 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9467 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9466 - val_loss: 0.9502 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9465 - val_loss: 0.9493 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9465 - val_loss: 0.9495 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9467 - val_loss: 0.9494 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9463 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9461 - val_loss: 0.9491 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9461 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 105/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9459 - val_loss: 0.9487 - learning_rate: 0.0010\n",
      "Epoch 106/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9458 - val_loss: 0.9491 - learning_rate: 0.0010\n",
      "Epoch 107/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9457 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 108/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9455 - val_loss: 0.9484 - learning_rate: 0.0010\n",
      "Epoch 109/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9454 - val_loss: 0.9487 - learning_rate: 0.0010\n",
      "Epoch 110/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9454 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 111/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9452 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 112/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9455 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 113/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9453 - val_loss: 0.9479 - learning_rate: 0.0010\n",
      "Epoch 114/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9450 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 115/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9450 - val_loss: 0.9475 - learning_rate: 0.0010\n",
      "Epoch 116/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9450 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 117/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9450 - val_loss: 0.9479 - learning_rate: 0.0010\n",
      "Epoch 118/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9451 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 119/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9450 - val_loss: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 120/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9449 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 121/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9447 - val_loss: 0.9477 - learning_rate: 0.0010\n",
      "Epoch 122/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9445 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 123/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9447 - val_loss: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 124/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9447 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 125/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9444 - val_loss: 0.9478 - learning_rate: 0.0010\n",
      "Epoch 126/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9443 - val_loss: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 127/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9439 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 128/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9441 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 129/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9441 - val_loss: 0.9466 - learning_rate: 0.0010\n",
      "Epoch 130/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9440 - val_loss: 0.9472 - learning_rate: 0.0010\n",
      "Epoch 131/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9440 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 132/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9441 - val_loss: 0.9471 - learning_rate: 0.0010\n",
      "Epoch 133/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9443 - val_loss: 0.9480 - learning_rate: 0.0010\n",
      "Epoch 134/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9440 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 135/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9442 - val_loss: 0.9467 - learning_rate: 0.0010\n",
      "Epoch 136/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9440 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 137/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9442 - val_loss: 0.9469 - learning_rate: 0.0010\n",
      "Epoch 138/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9439 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 139/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9435 - val_loss: 0.9463 - learning_rate: 5.0000e-04\n",
      "Epoch 140/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9430 - val_loss: 0.9459 - learning_rate: 5.0000e-04\n",
      "Epoch 141/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9429 - val_loss: 0.9461 - learning_rate: 5.0000e-04\n",
      "Epoch 142/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9429 - val_loss: 0.9461 - learning_rate: 5.0000e-04\n",
      "Epoch 143/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9426 - val_loss: 0.9455 - learning_rate: 5.0000e-04\n",
      "Epoch 144/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9427 - val_loss: 0.9459 - learning_rate: 5.0000e-04\n",
      "Epoch 145/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9425 - val_loss: 0.9460 - learning_rate: 5.0000e-04\n",
      "Epoch 146/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9423 - val_loss: 0.9453 - learning_rate: 5.0000e-04\n",
      "Epoch 147/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9424 - val_loss: 0.9454 - learning_rate: 5.0000e-04\n",
      "Epoch 148/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9423 - val_loss: 0.9460 - learning_rate: 5.0000e-04\n",
      "Epoch 149/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9423 - val_loss: 0.9456 - learning_rate: 5.0000e-04\n",
      "Epoch 150/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9422 - val_loss: 0.9456 - learning_rate: 5.0000e-04\n",
      "Epoch 151/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9420 - val_loss: 0.9452 - learning_rate: 5.0000e-04\n",
      "Epoch 152/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9419 - val_loss: 0.9452 - learning_rate: 5.0000e-04\n",
      "Epoch 153/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9419 - val_loss: 0.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 154/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9417 - val_loss: 0.9450 - learning_rate: 5.0000e-04\n",
      "Epoch 155/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9417 - val_loss: 0.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 156/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9420 - val_loss: 0.9452 - learning_rate: 5.0000e-04\n",
      "Epoch 157/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9418 - val_loss: 0.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 158/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9417 - val_loss: 0.9450 - learning_rate: 5.0000e-04\n",
      "Epoch 159/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9416 - val_loss: 0.9446 - learning_rate: 5.0000e-04\n",
      "Epoch 160/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9413 - val_loss: 0.9452 - learning_rate: 5.0000e-04\n",
      "Epoch 161/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9414 - val_loss: 0.9448 - learning_rate: 5.0000e-04\n",
      "Epoch 162/300\n",
      "664/664 - 18s - 27ms/step - loss: 0.9413 - val_loss: 0.9445 - learning_rate: 5.0000e-04\n",
      "Epoch 163/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9413 - val_loss: 0.9447 - learning_rate: 5.0000e-04\n",
      "Epoch 164/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9415 - val_loss: 0.9448 - learning_rate: 5.0000e-04\n",
      "Epoch 165/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9413 - val_loss: 0.9443 - learning_rate: 5.0000e-04\n",
      "Epoch 166/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9413 - val_loss: 0.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 167/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9412 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 168/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9412 - val_loss: 0.9445 - learning_rate: 5.0000e-04\n",
      "Epoch 169/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9411 - val_loss: 0.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 170/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9412 - val_loss: 0.9448 - learning_rate: 5.0000e-04\n",
      "Epoch 171/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9411 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 172/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9410 - val_loss: 0.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 173/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9409 - val_loss: 0.9437 - learning_rate: 5.0000e-04\n",
      "Epoch 174/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9409 - val_loss: 0.9441 - learning_rate: 5.0000e-04\n",
      "Epoch 175/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9410 - val_loss: 0.9443 - learning_rate: 5.0000e-04\n",
      "Epoch 176/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9408 - val_loss: 0.9437 - learning_rate: 5.0000e-04\n",
      "Epoch 177/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9408 - val_loss: 0.9438 - learning_rate: 5.0000e-04\n",
      "Epoch 178/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9409 - val_loss: 0.9443 - learning_rate: 5.0000e-04\n",
      "Epoch 179/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9409 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 180/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9406 - val_loss: 0.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 181/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9408 - val_loss: 0.9441 - learning_rate: 5.0000e-04\n",
      "Epoch 182/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9407 - val_loss: 0.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 183/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9407 - val_loss: 0.9442 - learning_rate: 5.0000e-04\n",
      "Epoch 184/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9406 - val_loss: 0.9436 - learning_rate: 2.5000e-04\n",
      "Epoch 185/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9403 - val_loss: 0.9434 - learning_rate: 2.5000e-04\n",
      "Epoch 186/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9401 - val_loss: 0.9436 - learning_rate: 2.5000e-04\n",
      "Epoch 187/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9403 - val_loss: 0.9438 - learning_rate: 2.5000e-04\n",
      "Epoch 188/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9400 - val_loss: 0.9435 - learning_rate: 2.5000e-04\n",
      "Epoch 189/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9401 - val_loss: 0.9435 - learning_rate: 2.5000e-04\n",
      "Epoch 190/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9400 - val_loss: 0.9432 - learning_rate: 2.5000e-04\n",
      "Epoch 191/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9400 - val_loss: 0.9432 - learning_rate: 2.5000e-04\n",
      "Epoch 192/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9398 - val_loss: 0.9435 - learning_rate: 2.5000e-04\n",
      "Epoch 193/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9398 - val_loss: 0.9432 - learning_rate: 2.5000e-04\n",
      "Epoch 194/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9400 - val_loss: 0.9430 - learning_rate: 2.5000e-04\n",
      "Epoch 195/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9399 - val_loss: 0.9430 - learning_rate: 2.5000e-04\n",
      "Epoch 196/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9397 - val_loss: 0.9434 - learning_rate: 2.5000e-04\n",
      "Epoch 197/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9399 - val_loss: 0.9432 - learning_rate: 2.5000e-04\n",
      "Epoch 198/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9397 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 199/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9398 - val_loss: 0.9430 - learning_rate: 2.5000e-04\n",
      "Epoch 200/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9397 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 201/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9397 - val_loss: 0.9431 - learning_rate: 2.5000e-04\n",
      "Epoch 202/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9397 - val_loss: 0.9425 - learning_rate: 2.5000e-04\n",
      "Epoch 203/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9397 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 204/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9394 - val_loss: 0.9430 - learning_rate: 2.5000e-04\n",
      "Epoch 205/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9394 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 206/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9395 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 207/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9395 - val_loss: 0.9428 - learning_rate: 2.5000e-04\n",
      "Epoch 208/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9393 - val_loss: 0.9428 - learning_rate: 2.5000e-04\n",
      "Epoch 209/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9393 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 210/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9394 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 211/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9394 - val_loss: 0.9425 - learning_rate: 2.5000e-04\n",
      "Epoch 212/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9394 - val_loss: 0.9429 - learning_rate: 2.5000e-04\n",
      "Epoch 213/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9394 - val_loss: 0.9428 - learning_rate: 1.2500e-04\n",
      "Epoch 214/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9392 - val_loss: 0.9430 - learning_rate: 1.2500e-04\n",
      "Epoch 215/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9392 - val_loss: 0.9428 - learning_rate: 1.2500e-04\n",
      "Epoch 216/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9392 - val_loss: 0.9429 - learning_rate: 1.2500e-04\n",
      "Epoch 217/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9392 - val_loss: 0.9426 - learning_rate: 1.2500e-04\n",
      "Epoch 218/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9391 - val_loss: 0.9425 - learning_rate: 1.2500e-04\n",
      "Epoch 219/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9391 - val_loss: 0.9425 - learning_rate: 1.2500e-04\n",
      "Epoch 220/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9390 - val_loss: 0.9425 - learning_rate: 1.2500e-04\n",
      "Epoch 221/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9389 - val_loss: 0.9425 - learning_rate: 1.2500e-04\n",
      "Epoch 222/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9390 - val_loss: 0.9424 - learning_rate: 1.2500e-04\n",
      "Epoch 223/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9390 - val_loss: 0.9426 - learning_rate: 1.2500e-04\n",
      "Epoch 224/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9390 - val_loss: 0.9424 - learning_rate: 1.2500e-04\n",
      "Epoch 225/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9390 - val_loss: 0.9424 - learning_rate: 1.2500e-04\n",
      "Epoch 226/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9390 - val_loss: 0.9426 - learning_rate: 1.2500e-04\n",
      "Epoch 227/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9389 - val_loss: 0.9425 - learning_rate: 1.2500e-04\n",
      "Epoch 228/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9424 - learning_rate: 1.2500e-04\n",
      "Epoch 229/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9390 - val_loss: 0.9423 - learning_rate: 1.2500e-04\n",
      "Epoch 230/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9424 - learning_rate: 1.2500e-04\n",
      "Epoch 231/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9423 - learning_rate: 1.2500e-04\n",
      "Epoch 232/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9390 - val_loss: 0.9423 - learning_rate: 1.2500e-04\n",
      "Epoch 233/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 234/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9389 - val_loss: 0.9424 - learning_rate: 6.2500e-05\n",
      "Epoch 235/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 236/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9424 - learning_rate: 6.2500e-05\n",
      "Epoch 237/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 238/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 239/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 240/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 241/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9389 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 242/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9423 - learning_rate: 6.2500e-05\n",
      "Epoch 243/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 244/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 245/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 246/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 247/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 248/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 249/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 250/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 6.2500e-05\n",
      "Epoch 251/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9388 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 252/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 253/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9420 - learning_rate: 6.2500e-05\n",
      "Epoch 254/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 255/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 256/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 257/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 258/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 259/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 6.2500e-05\n",
      "Epoch 260/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9384 - val_loss: 0.9422 - learning_rate: 6.2500e-05\n",
      "Epoch 261/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 3.1250e-05\n",
      "Epoch 262/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 263/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9422 - learning_rate: 3.1250e-05\n",
      "Epoch 264/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 265/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 266/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 3.1250e-05\n",
      "Epoch 267/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 268/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9387 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 269/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9383 - val_loss: 0.9420 - learning_rate: 3.1250e-05\n",
      "Epoch 270/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 3.1250e-05\n",
      "Epoch 271/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 272/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 273/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 274/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 275/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 276/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9420 - learning_rate: 1.5625e-05\n",
      "Epoch 277/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9382 - val_loss: 0.9420 - learning_rate: 1.5625e-05\n",
      "Epoch 278/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9421 - learning_rate: 1.5625e-05\n",
      "Epoch 279/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 1.5625e-05\n",
      "Epoch 280/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9420 - learning_rate: 1.5625e-05\n",
      "Epoch 281/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 282/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 283/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 284/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 285/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9421 - learning_rate: 7.8125e-06\n",
      "Epoch 286/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 287/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 7.8125e-06\n",
      "Epoch 288/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 289/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 290/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 7.8125e-06\n",
      "Epoch 291/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 292/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9383 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 293/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 294/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9421 - learning_rate: 3.9063e-06\n",
      "Epoch 295/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 296/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9422 - learning_rate: 3.9063e-06\n",
      "Epoch 297/300\n",
      "664/664 - 17s - 26ms/step - loss: 0.9384 - val_loss: 0.9419 - learning_rate: 3.9063e-06\n",
      "Epoch 298/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9384 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 299/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9385 - val_loss: 0.9420 - learning_rate: 3.9063e-06\n",
      "Epoch 300/300\n",
      "664/664 - 17s - 25ms/step - loss: 0.9386 - val_loss: 0.9421 - learning_rate: 3.9063e-06\n",
      "\u001b[1m2655/2655\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "664/664 - 6s - 8ms/step - accuracy: 0.9052 - loss: 0.2949 - val_accuracy: 0.9652 - val_loss: 0.1210 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9561 - loss: 0.1657 - val_accuracy: 0.9688 - val_loss: 0.1072 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9612 - loss: 0.1438 - val_accuracy: 0.9700 - val_loss: 0.1022 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9663 - loss: 0.1309 - val_accuracy: 0.9714 - val_loss: 0.0948 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9673 - loss: 0.1214 - val_accuracy: 0.9717 - val_loss: 0.0928 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9693 - loss: 0.1143 - val_accuracy: 0.9727 - val_loss: 0.0900 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9702 - loss: 0.1105 - val_accuracy: 0.9735 - val_loss: 0.0878 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9714 - loss: 0.1045 - val_accuracy: 0.9741 - val_loss: 0.0850 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9721 - loss: 0.1014 - val_accuracy: 0.9743 - val_loss: 0.0830 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9724 - loss: 0.0987 - val_accuracy: 0.9746 - val_loss: 0.0826 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9737 - loss: 0.0950 - val_accuracy: 0.9756 - val_loss: 0.0800 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9747 - loss: 0.0923 - val_accuracy: 0.9760 - val_loss: 0.0786 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9749 - loss: 0.0909 - val_accuracy: 0.9759 - val_loss: 0.0790 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9756 - loss: 0.0896 - val_accuracy: 0.9765 - val_loss: 0.0768 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9761 - loss: 0.0861 - val_accuracy: 0.9763 - val_loss: 0.0757 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9757 - loss: 0.0848 - val_accuracy: 0.9772 - val_loss: 0.0735 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9770 - loss: 0.0808 - val_accuracy: 0.9775 - val_loss: 0.0730 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9775 - loss: 0.0799 - val_accuracy: 0.9775 - val_loss: 0.0716 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9778 - loss: 0.0781 - val_accuracy: 0.9778 - val_loss: 0.0706 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9779 - loss: 0.0764 - val_accuracy: 0.9783 - val_loss: 0.0693 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9784 - loss: 0.0748 - val_accuracy: 0.9774 - val_loss: 0.0696 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9783 - loss: 0.0735 - val_accuracy: 0.9784 - val_loss: 0.0684 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9784 - loss: 0.0739 - val_accuracy: 0.9791 - val_loss: 0.0665 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9789 - loss: 0.0705 - val_accuracy: 0.9778 - val_loss: 0.0682 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9789 - loss: 0.0707 - val_accuracy: 0.9784 - val_loss: 0.0668 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9793 - loss: 0.0692 - val_accuracy: 0.9789 - val_loss: 0.0657 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9795 - loss: 0.0680 - val_accuracy: 0.9791 - val_loss: 0.0652 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9796 - loss: 0.0683 - val_accuracy: 0.9800 - val_loss: 0.0665 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9801 - loss: 0.0664 - val_accuracy: 0.9797 - val_loss: 0.0665 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9799 - loss: 0.0664 - val_accuracy: 0.9784 - val_loss: 0.0658 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9800 - loss: 0.0650 - val_accuracy: 0.9795 - val_loss: 0.0642 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9811 - loss: 0.0629 - val_accuracy: 0.9792 - val_loss: 0.0652 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9807 - loss: 0.0624 - val_accuracy: 0.9792 - val_loss: 0.0652 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "664/664 - 5s - 8ms/step - accuracy: 0.9814 - loss: 0.0616 - val_accuracy: 0.9796 - val_loss: 0.0640 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9813 - loss: 0.0615 - val_accuracy: 0.9798 - val_loss: 0.0644 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9814 - loss: 0.0608 - val_accuracy: 0.9795 - val_loss: 0.0654 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9818 - loss: 0.0590 - val_accuracy: 0.9793 - val_loss: 0.0629 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9814 - loss: 0.0598 - val_accuracy: 0.9788 - val_loss: 0.0627 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9817 - loss: 0.0594 - val_accuracy: 0.9795 - val_loss: 0.0633 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9815 - loss: 0.0579 - val_accuracy: 0.9802 - val_loss: 0.0641 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9825 - loss: 0.0569 - val_accuracy: 0.9793 - val_loss: 0.0655 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9815 - loss: 0.0586 - val_accuracy: 0.9802 - val_loss: 0.0626 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9829 - loss: 0.0552 - val_accuracy: 0.9798 - val_loss: 0.0631 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9832 - loss: 0.0541 - val_accuracy: 0.9801 - val_loss: 0.0629 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9830 - loss: 0.0550 - val_accuracy: 0.9793 - val_loss: 0.0637 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9838 - loss: 0.0522 - val_accuracy: 0.9800 - val_loss: 0.0623 - learning_rate: 5.0000e-05\n",
      "Epoch 47/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9838 - loss: 0.0517 - val_accuracy: 0.9798 - val_loss: 0.0635 - learning_rate: 5.0000e-05\n",
      "Epoch 48/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9842 - loss: 0.0505 - val_accuracy: 0.9802 - val_loss: 0.0628 - learning_rate: 5.0000e-05\n",
      "Epoch 49/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9839 - loss: 0.0502 - val_accuracy: 0.9799 - val_loss: 0.0625 - learning_rate: 5.0000e-05\n",
      "Epoch 50/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9841 - loss: 0.0495 - val_accuracy: 0.9803 - val_loss: 0.0635 - learning_rate: 5.0000e-05\n",
      "Epoch 51/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9844 - loss: 0.0493 - val_accuracy: 0.9801 - val_loss: 0.0636 - learning_rate: 5.0000e-05\n",
      "Epoch 52/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9845 - loss: 0.0497 - val_accuracy: 0.9805 - val_loss: 0.0636 - learning_rate: 5.0000e-05\n",
      "Epoch 53/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9845 - loss: 0.0483 - val_accuracy: 0.9805 - val_loss: 0.0646 - learning_rate: 5.0000e-05\n",
      "Epoch 54/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9849 - loss: 0.0484 - val_accuracy: 0.9805 - val_loss: 0.0642 - learning_rate: 2.5000e-05\n",
      "Epoch 55/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9851 - loss: 0.0475 - val_accuracy: 0.9806 - val_loss: 0.0642 - learning_rate: 2.5000e-05\n",
      "Epoch 56/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9849 - loss: 0.0481 - val_accuracy: 0.9805 - val_loss: 0.0641 - learning_rate: 2.5000e-05\n",
      "Epoch 57/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9846 - loss: 0.0472 - val_accuracy: 0.9804 - val_loss: 0.0641 - learning_rate: 2.5000e-05\n",
      "Epoch 58/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9854 - loss: 0.0461 - val_accuracy: 0.9803 - val_loss: 0.0638 - learning_rate: 2.5000e-05\n",
      "Epoch 59/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9847 - loss: 0.0464 - val_accuracy: 0.9812 - val_loss: 0.0643 - learning_rate: 2.5000e-05\n",
      "Epoch 60/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9851 - loss: 0.0459 - val_accuracy: 0.9808 - val_loss: 0.0631 - learning_rate: 2.5000e-05\n",
      "Epoch 61/150\n",
      "664/664 - 4s - 6ms/step - accuracy: 0.9850 - loss: 0.0464 - val_accuracy: 0.9808 - val_loss: 0.0638 - learning_rate: 1.2500e-05\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9787 - loss: 0.0674\n",
      "Test Accuracy: 0.9807873368263245\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Persister       0.98      0.98      0.98      5255\n",
      "    Persister       0.98      0.98      0.98      5363\n",
      "\n",
      "     accuracy                           0.98     10618\n",
      "    macro avg       0.98      0.98      0.98     10618\n",
      " weighted avg       0.98      0.98      0.98     10618\n",
      "\n",
      "Unique values in y_model2: (array([0, 1]), array([11571, 48744]))\n",
      "Epoch 1/300\n",
      "610/610 - 20s - 32ms/step - loss: 1.0371 - val_loss: 1.0082 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "610/610 - 16s - 27ms/step - loss: 1.0008 - val_loss: 1.0059 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9993 - val_loss: 1.0044 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9975 - val_loss: 1.0021 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9953 - val_loss: 0.9990 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9926 - val_loss: 0.9957 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9899 - val_loss: 0.9925 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9874 - val_loss: 0.9898 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9852 - val_loss: 0.9869 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9831 - val_loss: 0.9848 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "610/610 - 18s - 30ms/step - loss: 0.9813 - val_loss: 0.9834 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9800 - val_loss: 0.9819 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9787 - val_loss: 0.9805 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9777 - val_loss: 0.9790 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9765 - val_loss: 0.9781 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9758 - val_loss: 0.9773 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9750 - val_loss: 0.9761 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9742 - val_loss: 0.9753 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9731 - val_loss: 0.9739 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9724 - val_loss: 0.9738 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9719 - val_loss: 0.9732 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9713 - val_loss: 0.9722 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9707 - val_loss: 0.9720 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9702 - val_loss: 0.9707 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9694 - val_loss: 0.9704 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9691 - val_loss: 0.9697 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9687 - val_loss: 0.9693 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9685 - val_loss: 0.9691 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9679 - val_loss: 0.9689 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9675 - val_loss: 0.9682 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9669 - val_loss: 0.9678 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9666 - val_loss: 0.9673 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9664 - val_loss: 0.9668 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9662 - val_loss: 0.9664 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9656 - val_loss: 0.9663 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9654 - val_loss: 0.9660 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9650 - val_loss: 0.9659 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9646 - val_loss: 0.9652 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9642 - val_loss: 0.9649 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9639 - val_loss: 0.9647 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9636 - val_loss: 0.9639 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9633 - val_loss: 0.9641 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9632 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9627 - val_loss: 0.9629 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9631 - val_loss: 0.9638 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9633 - val_loss: 0.9637 - learning_rate: 0.0010\n",
      "Epoch 47/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9626 - val_loss: 0.9631 - learning_rate: 0.0010\n",
      "Epoch 48/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9621 - val_loss: 0.9627 - learning_rate: 0.0010\n",
      "Epoch 49/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9620 - val_loss: 0.9628 - learning_rate: 0.0010\n",
      "Epoch 50/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9620 - val_loss: 0.9624 - learning_rate: 0.0010\n",
      "Epoch 51/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9618 - val_loss: 0.9620 - learning_rate: 0.0010\n",
      "Epoch 52/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9615 - val_loss: 0.9618 - learning_rate: 0.0010\n",
      "Epoch 53/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9612 - val_loss: 0.9611 - learning_rate: 0.0010\n",
      "Epoch 54/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9609 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 55/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9607 - val_loss: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 56/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9607 - val_loss: 0.9611 - learning_rate: 0.0010\n",
      "Epoch 57/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9602 - val_loss: 0.9606 - learning_rate: 0.0010\n",
      "Epoch 58/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9601 - val_loss: 0.9607 - learning_rate: 0.0010\n",
      "Epoch 59/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9600 - val_loss: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 60/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9599 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 61/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9600 - val_loss: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 62/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9596 - val_loss: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 63/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9600 - val_loss: 0.9603 - learning_rate: 0.0010\n",
      "Epoch 64/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9596 - val_loss: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 65/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9592 - val_loss: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 66/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9591 - val_loss: 0.9597 - learning_rate: 0.0010\n",
      "Epoch 67/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9589 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 68/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9587 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 69/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9588 - val_loss: 0.9592 - learning_rate: 0.0010\n",
      "Epoch 70/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9590 - val_loss: 0.9596 - learning_rate: 0.0010\n",
      "Epoch 71/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9586 - val_loss: 0.9597 - learning_rate: 0.0010\n",
      "Epoch 72/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9587 - val_loss: 0.9591 - learning_rate: 0.0010\n",
      "Epoch 73/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9584 - val_loss: 0.9589 - learning_rate: 0.0010\n",
      "Epoch 74/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9584 - val_loss: 0.9591 - learning_rate: 0.0010\n",
      "Epoch 75/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9582 - val_loss: 0.9588 - learning_rate: 0.0010\n",
      "Epoch 76/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9582 - val_loss: 0.9589 - learning_rate: 0.0010\n",
      "Epoch 77/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9581 - val_loss: 0.9583 - learning_rate: 0.0010\n",
      "Epoch 78/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9581 - val_loss: 0.9585 - learning_rate: 0.0010\n",
      "Epoch 79/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9581 - val_loss: 0.9583 - learning_rate: 0.0010\n",
      "Epoch 80/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9578 - val_loss: 0.9584 - learning_rate: 0.0010\n",
      "Epoch 81/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9579 - val_loss: 0.9584 - learning_rate: 0.0010\n",
      "Epoch 82/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9578 - val_loss: 0.9580 - learning_rate: 0.0010\n",
      "Epoch 83/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9575 - val_loss: 0.9581 - learning_rate: 0.0010\n",
      "Epoch 84/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9574 - val_loss: 0.9579 - learning_rate: 0.0010\n",
      "Epoch 85/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9570 - val_loss: 0.9579 - learning_rate: 0.0010\n",
      "Epoch 86/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9570 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 87/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9570 - val_loss: 0.9576 - learning_rate: 0.0010\n",
      "Epoch 88/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9570 - val_loss: 0.9579 - learning_rate: 0.0010\n",
      "Epoch 89/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9569 - val_loss: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 90/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9567 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 91/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9564 - val_loss: 0.9576 - learning_rate: 0.0010\n",
      "Epoch 92/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9566 - val_loss: 0.9567 - learning_rate: 0.0010\n",
      "Epoch 93/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9563 - val_loss: 0.9565 - learning_rate: 0.0010\n",
      "Epoch 94/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9568 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 95/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9571 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 96/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9571 - val_loss: 0.9573 - learning_rate: 0.0010\n",
      "Epoch 97/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9567 - val_loss: 0.9570 - learning_rate: 0.0010\n",
      "Epoch 98/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9565 - val_loss: 0.9569 - learning_rate: 0.0010\n",
      "Epoch 99/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9563 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 100/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9562 - val_loss: 0.9568 - learning_rate: 0.0010\n",
      "Epoch 101/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9561 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 102/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9562 - val_loss: 0.9564 - learning_rate: 0.0010\n",
      "Epoch 103/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9559 - val_loss: 0.9570 - learning_rate: 0.0010\n",
      "Epoch 104/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9557 - val_loss: 0.9565 - learning_rate: 5.0000e-04\n",
      "Epoch 105/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9551 - val_loss: 0.9557 - learning_rate: 5.0000e-04\n",
      "Epoch 106/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9549 - val_loss: 0.9558 - learning_rate: 5.0000e-04\n",
      "Epoch 107/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9550 - val_loss: 0.9566 - learning_rate: 5.0000e-04\n",
      "Epoch 108/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9549 - val_loss: 0.9558 - learning_rate: 5.0000e-04\n",
      "Epoch 109/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9547 - val_loss: 0.9556 - learning_rate: 5.0000e-04\n",
      "Epoch 110/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9546 - val_loss: 0.9559 - learning_rate: 5.0000e-04\n",
      "Epoch 111/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9546 - val_loss: 0.9556 - learning_rate: 5.0000e-04\n",
      "Epoch 112/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9545 - val_loss: 0.9559 - learning_rate: 5.0000e-04\n",
      "Epoch 113/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9541 - val_loss: 0.9554 - learning_rate: 5.0000e-04\n",
      "Epoch 114/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9540 - val_loss: 0.9557 - learning_rate: 5.0000e-04\n",
      "Epoch 115/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9539 - val_loss: 0.9553 - learning_rate: 5.0000e-04\n",
      "Epoch 116/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9540 - val_loss: 0.9553 - learning_rate: 5.0000e-04\n",
      "Epoch 117/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9540 - val_loss: 0.9552 - learning_rate: 5.0000e-04\n",
      "Epoch 118/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9541 - val_loss: 0.9553 - learning_rate: 5.0000e-04\n",
      "Epoch 119/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9541 - val_loss: 0.9553 - learning_rate: 5.0000e-04\n",
      "Epoch 120/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9540 - val_loss: 0.9552 - learning_rate: 5.0000e-04\n",
      "Epoch 121/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9538 - val_loss: 0.9550 - learning_rate: 5.0000e-04\n",
      "Epoch 122/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9536 - val_loss: 0.9552 - learning_rate: 5.0000e-04\n",
      "Epoch 123/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9535 - val_loss: 0.9550 - learning_rate: 5.0000e-04\n",
      "Epoch 124/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9535 - val_loss: 0.9549 - learning_rate: 5.0000e-04\n",
      "Epoch 125/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9535 - val_loss: 0.9550 - learning_rate: 5.0000e-04\n",
      "Epoch 126/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9533 - val_loss: 0.9549 - learning_rate: 5.0000e-04\n",
      "Epoch 127/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9534 - val_loss: 0.9549 - learning_rate: 5.0000e-04\n",
      "Epoch 128/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9532 - val_loss: 0.9545 - learning_rate: 5.0000e-04\n",
      "Epoch 129/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9533 - val_loss: 0.9543 - learning_rate: 5.0000e-04\n",
      "Epoch 130/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9532 - val_loss: 0.9545 - learning_rate: 5.0000e-04\n",
      "Epoch 131/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9533 - val_loss: 0.9549 - learning_rate: 5.0000e-04\n",
      "Epoch 132/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9535 - val_loss: 0.9545 - learning_rate: 5.0000e-04\n",
      "Epoch 133/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9531 - val_loss: 0.9545 - learning_rate: 5.0000e-04\n",
      "Epoch 134/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9530 - val_loss: 0.9548 - learning_rate: 5.0000e-04\n",
      "Epoch 135/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9531 - val_loss: 0.9544 - learning_rate: 5.0000e-04\n",
      "Epoch 136/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9530 - val_loss: 0.9543 - learning_rate: 5.0000e-04\n",
      "Epoch 137/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9530 - val_loss: 0.9546 - learning_rate: 5.0000e-04\n",
      "Epoch 138/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9530 - val_loss: 0.9547 - learning_rate: 5.0000e-04\n",
      "Epoch 139/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9529 - val_loss: 0.9544 - learning_rate: 5.0000e-04\n",
      "Epoch 140/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9528 - val_loss: 0.9542 - learning_rate: 2.5000e-04\n",
      "Epoch 141/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9527 - val_loss: 0.9542 - learning_rate: 2.5000e-04\n",
      "Epoch 142/300\n",
      "610/610 - 16s - 27ms/step - loss: 0.9526 - val_loss: 0.9542 - learning_rate: 2.5000e-04\n",
      "Epoch 143/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9525 - val_loss: 0.9541 - learning_rate: 2.5000e-04\n",
      "Epoch 144/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9524 - val_loss: 0.9539 - learning_rate: 2.5000e-04\n",
      "Epoch 145/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9524 - val_loss: 0.9540 - learning_rate: 2.5000e-04\n",
      "Epoch 146/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9523 - val_loss: 0.9541 - learning_rate: 2.5000e-04\n",
      "Epoch 147/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9524 - val_loss: 0.9539 - learning_rate: 2.5000e-04\n",
      "Epoch 148/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9523 - val_loss: 0.9543 - learning_rate: 2.5000e-04\n",
      "Epoch 149/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9522 - val_loss: 0.9540 - learning_rate: 2.5000e-04\n",
      "Epoch 150/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9523 - val_loss: 0.9537 - learning_rate: 2.5000e-04\n",
      "Epoch 151/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9522 - val_loss: 0.9540 - learning_rate: 2.5000e-04\n",
      "Epoch 152/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9520 - val_loss: 0.9537 - learning_rate: 2.5000e-04\n",
      "Epoch 153/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9520 - val_loss: 0.9537 - learning_rate: 2.5000e-04\n",
      "Epoch 154/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9522 - val_loss: 0.9534 - learning_rate: 2.5000e-04\n",
      "Epoch 155/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9521 - val_loss: 0.9536 - learning_rate: 2.5000e-04\n",
      "Epoch 156/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9520 - val_loss: 0.9536 - learning_rate: 2.5000e-04\n",
      "Epoch 157/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9520 - val_loss: 0.9538 - learning_rate: 2.5000e-04\n",
      "Epoch 158/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9520 - val_loss: 0.9535 - learning_rate: 2.5000e-04\n",
      "Epoch 159/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9519 - val_loss: 0.9536 - learning_rate: 2.5000e-04\n",
      "Epoch 160/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9518 - val_loss: 0.9535 - learning_rate: 2.5000e-04\n",
      "Epoch 161/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9518 - val_loss: 0.9535 - learning_rate: 2.5000e-04\n",
      "Epoch 162/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9519 - val_loss: 0.9536 - learning_rate: 2.5000e-04\n",
      "Epoch 163/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9518 - val_loss: 0.9533 - learning_rate: 2.5000e-04\n",
      "Epoch 164/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9519 - val_loss: 0.9536 - learning_rate: 2.5000e-04\n",
      "Epoch 165/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9515 - val_loss: 0.9537 - learning_rate: 1.2500e-04\n",
      "Epoch 166/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9517 - val_loss: 0.9536 - learning_rate: 1.2500e-04\n",
      "Epoch 167/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9515 - val_loss: 0.9536 - learning_rate: 1.2500e-04\n",
      "Epoch 168/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9516 - val_loss: 0.9535 - learning_rate: 1.2500e-04\n",
      "Epoch 169/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9516 - val_loss: 0.9535 - learning_rate: 1.2500e-04\n",
      "Epoch 170/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9516 - val_loss: 0.9535 - learning_rate: 1.2500e-04\n",
      "Epoch 171/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9514 - val_loss: 0.9536 - learning_rate: 1.2500e-04\n",
      "Epoch 172/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9516 - val_loss: 0.9535 - learning_rate: 1.2500e-04\n",
      "Epoch 173/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9515 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 174/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9514 - val_loss: 0.9532 - learning_rate: 1.2500e-04\n",
      "Epoch 175/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9513 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 176/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9514 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 177/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9512 - val_loss: 0.9532 - learning_rate: 1.2500e-04\n",
      "Epoch 178/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9514 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 179/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9513 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 180/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9513 - val_loss: 0.9531 - learning_rate: 1.2500e-04\n",
      "Epoch 181/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 1.2500e-04\n",
      "Epoch 182/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 1.2500e-04\n",
      "Epoch 183/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9513 - val_loss: 0.9533 - learning_rate: 1.2500e-04\n",
      "Epoch 184/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9514 - val_loss: 0.9534 - learning_rate: 1.2500e-04\n",
      "Epoch 185/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9532 - learning_rate: 6.2500e-05\n",
      "Epoch 186/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 187/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 188/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 189/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 190/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 191/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9530 - learning_rate: 6.2500e-05\n",
      "Epoch 192/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 193/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 6.2500e-05\n",
      "Epoch 194/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9512 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 195/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9512 - val_loss: 0.9530 - learning_rate: 6.2500e-05\n",
      "Epoch 196/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 197/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9532 - learning_rate: 6.2500e-05\n",
      "Epoch 198/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9531 - learning_rate: 6.2500e-05\n",
      "Epoch 199/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 6.2500e-05\n",
      "Epoch 200/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9530 - learning_rate: 6.2500e-05\n",
      "Epoch 201/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9532 - learning_rate: 6.2500e-05\n",
      "Epoch 202/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9529 - learning_rate: 3.1250e-05\n",
      "Epoch 203/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 3.1250e-05\n",
      "Epoch 204/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9531 - learning_rate: 3.1250e-05\n",
      "Epoch 205/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 3.1250e-05\n",
      "Epoch 206/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 3.1250e-05\n",
      "Epoch 207/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.1250e-05\n",
      "Epoch 208/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9511 - val_loss: 0.9530 - learning_rate: 3.1250e-05\n",
      "Epoch 209/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.1250e-05\n",
      "Epoch 210/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 3.1250e-05\n",
      "Epoch 211/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 3.1250e-05\n",
      "Epoch 212/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 213/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 214/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 215/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 216/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 217/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 1.5625e-05\n",
      "Epoch 218/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 219/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 220/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 221/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 1.5625e-05\n",
      "Epoch 222/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 7.8125e-06\n",
      "Epoch 223/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 7.8125e-06\n",
      "Epoch 224/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 7.8125e-06\n",
      "Epoch 225/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 7.8125e-06\n",
      "Epoch 226/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 7.8125e-06\n",
      "Epoch 227/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 7.8125e-06\n",
      "Epoch 228/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9529 - learning_rate: 7.8125e-06\n",
      "Epoch 229/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9506 - val_loss: 0.9529 - learning_rate: 7.8125e-06\n",
      "Epoch 230/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 7.8125e-06\n",
      "Epoch 231/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 7.8125e-06\n",
      "Epoch 232/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 233/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9530 - learning_rate: 3.9063e-06\n",
      "Epoch 234/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 235/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 236/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 3.9063e-06\n",
      "Epoch 237/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 238/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 239/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 240/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 241/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 3.9063e-06\n",
      "Epoch 242/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9528 - learning_rate: 1.9531e-06\n",
      "Epoch 243/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 1.9531e-06\n",
      "Epoch 244/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 245/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 246/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 247/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9506 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 248/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 249/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9506 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 250/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9530 - learning_rate: 1.9531e-06\n",
      "Epoch 251/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 252/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.9531e-06\n",
      "Epoch 253/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9510 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 254/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 255/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 256/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 257/300\n",
      "610/610 - 17s - 27ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 258/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 259/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 260/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 261/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9528 - learning_rate: 1.0000e-06\n",
      "Epoch 262/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 263/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 264/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 265/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9528 - learning_rate: 1.0000e-06\n",
      "Epoch 266/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 267/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9528 - learning_rate: 1.0000e-06\n",
      "Epoch 268/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 269/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 270/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 271/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 272/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 273/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 274/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 1.0000e-06\n",
      "Epoch 275/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9509 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 276/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9530 - learning_rate: 1.0000e-06\n",
      "Epoch 277/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 278/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 279/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9508 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 280/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9506 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "Epoch 281/300\n",
      "610/610 - 16s - 26ms/step - loss: 0.9507 - val_loss: 0.9529 - learning_rate: 1.0000e-06\n",
      "\u001b[1m2438/2438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Epoch 1/150\n",
      "610/610 - 6s - 9ms/step - accuracy: 0.8473 - loss: 0.4899 - val_accuracy: 0.9335 - val_loss: 0.2154 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9036 - loss: 0.3193 - val_accuracy: 0.9361 - val_loss: 0.1968 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9133 - loss: 0.2816 - val_accuracy: 0.9367 - val_loss: 0.1888 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9229 - loss: 0.2556 - val_accuracy: 0.9393 - val_loss: 0.1837 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9260 - loss: 0.2422 - val_accuracy: 0.9411 - val_loss: 0.1800 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9302 - loss: 0.2308 - val_accuracy: 0.9415 - val_loss: 0.1777 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9326 - loss: 0.2234 - val_accuracy: 0.9424 - val_loss: 0.1765 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9332 - loss: 0.2183 - val_accuracy: 0.9427 - val_loss: 0.1744 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9361 - loss: 0.2130 - val_accuracy: 0.9441 - val_loss: 0.1722 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9371 - loss: 0.2074 - val_accuracy: 0.9449 - val_loss: 0.1707 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9383 - loss: 0.2053 - val_accuracy: 0.9444 - val_loss: 0.1715 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9400 - loss: 0.2013 - val_accuracy: 0.9448 - val_loss: 0.1694 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9408 - loss: 0.1984 - val_accuracy: 0.9454 - val_loss: 0.1701 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9415 - loss: 0.1958 - val_accuracy: 0.9465 - val_loss: 0.1680 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9410 - loss: 0.1937 - val_accuracy: 0.9466 - val_loss: 0.1665 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9422 - loss: 0.1908 - val_accuracy: 0.9471 - val_loss: 0.1656 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9433 - loss: 0.1878 - val_accuracy: 0.9477 - val_loss: 0.1643 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9432 - loss: 0.1863 - val_accuracy: 0.9480 - val_loss: 0.1642 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9440 - loss: 0.1851 - val_accuracy: 0.9479 - val_loss: 0.1655 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9444 - loss: 0.1835 - val_accuracy: 0.9484 - val_loss: 0.1617 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9455 - loss: 0.1796 - val_accuracy: 0.9480 - val_loss: 0.1631 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9459 - loss: 0.1785 - val_accuracy: 0.9485 - val_loss: 0.1613 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9459 - loss: 0.1776 - val_accuracy: 0.9485 - val_loss: 0.1602 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9468 - loss: 0.1754 - val_accuracy: 0.9483 - val_loss: 0.1617 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9470 - loss: 0.1738 - val_accuracy: 0.9492 - val_loss: 0.1594 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9466 - loss: 0.1730 - val_accuracy: 0.9487 - val_loss: 0.1601 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9469 - loss: 0.1732 - val_accuracy: 0.9499 - val_loss: 0.1580 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9481 - loss: 0.1691 - val_accuracy: 0.9496 - val_loss: 0.1584 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9481 - loss: 0.1680 - val_accuracy: 0.9503 - val_loss: 0.1568 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9485 - loss: 0.1667 - val_accuracy: 0.9496 - val_loss: 0.1577 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9484 - loss: 0.1669 - val_accuracy: 0.9507 - val_loss: 0.1579 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9494 - loss: 0.1642 - val_accuracy: 0.9504 - val_loss: 0.1563 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9496 - loss: 0.1645 - val_accuracy: 0.9509 - val_loss: 0.1542 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9497 - loss: 0.1631 - val_accuracy: 0.9506 - val_loss: 0.1550 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9501 - loss: 0.1615 - val_accuracy: 0.9513 - val_loss: 0.1543 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9503 - loss: 0.1598 - val_accuracy: 0.9505 - val_loss: 0.1562 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9505 - loss: 0.1593 - val_accuracy: 0.9500 - val_loss: 0.1565 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9509 - loss: 0.1564 - val_accuracy: 0.9510 - val_loss: 0.1545 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9516 - loss: 0.1579 - val_accuracy: 0.9517 - val_loss: 0.1541 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9515 - loss: 0.1554 - val_accuracy: 0.9511 - val_loss: 0.1558 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9515 - loss: 0.1547 - val_accuracy: 0.9516 - val_loss: 0.1526 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9521 - loss: 0.1544 - val_accuracy: 0.9504 - val_loss: 0.1541 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9516 - loss: 0.1535 - val_accuracy: 0.9511 - val_loss: 0.1531 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9520 - loss: 0.1515 - val_accuracy: 0.9516 - val_loss: 0.1522 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9535 - loss: 0.1484 - val_accuracy: 0.9522 - val_loss: 0.1518 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9523 - loss: 0.1501 - val_accuracy: 0.9515 - val_loss: 0.1519 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9525 - loss: 0.1491 - val_accuracy: 0.9527 - val_loss: 0.1511 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9524 - loss: 0.1476 - val_accuracy: 0.9529 - val_loss: 0.1516 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9538 - loss: 0.1447 - val_accuracy: 0.9522 - val_loss: 0.1514 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9532 - loss: 0.1454 - val_accuracy: 0.9518 - val_loss: 0.1527 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9534 - loss: 0.1445 - val_accuracy: 0.9526 - val_loss: 0.1529 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9544 - loss: 0.1425 - val_accuracy: 0.9531 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9542 - loss: 0.1431 - val_accuracy: 0.9528 - val_loss: 0.1524 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9542 - loss: 0.1418 - val_accuracy: 0.9535 - val_loss: 0.1504 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9547 - loss: 0.1426 - val_accuracy: 0.9535 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9542 - loss: 0.1409 - val_accuracy: 0.9521 - val_loss: 0.1506 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9558 - loss: 0.1362 - val_accuracy: 0.9528 - val_loss: 0.1513 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9555 - loss: 0.1369 - val_accuracy: 0.9528 - val_loss: 0.1496 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9557 - loss: 0.1390 - val_accuracy: 0.9530 - val_loss: 0.1486 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9564 - loss: 0.1346 - val_accuracy: 0.9537 - val_loss: 0.1514 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9552 - loss: 0.1375 - val_accuracy: 0.9534 - val_loss: 0.1504 - learning_rate: 1.0000e-04\n",
      "Epoch 62/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9569 - loss: 0.1346 - val_accuracy: 0.9538 - val_loss: 0.1506 - learning_rate: 1.0000e-04\n",
      "Epoch 63/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9563 - loss: 0.1353 - val_accuracy: 0.9547 - val_loss: 0.1497 - learning_rate: 1.0000e-04\n",
      "Epoch 64/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9563 - loss: 0.1350 - val_accuracy: 0.9528 - val_loss: 0.1512 - learning_rate: 1.0000e-04\n",
      "Epoch 65/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9565 - loss: 0.1332 - val_accuracy: 0.9520 - val_loss: 0.1502 - learning_rate: 1.0000e-04\n",
      "Epoch 66/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9565 - loss: 0.1350 - val_accuracy: 0.9528 - val_loss: 0.1493 - learning_rate: 1.0000e-04\n",
      "Epoch 67/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9588 - loss: 0.1291 - val_accuracy: 0.9530 - val_loss: 0.1492 - learning_rate: 5.0000e-05\n",
      "Epoch 68/150\n",
      "610/610 - 3s - 6ms/step - accuracy: 0.9586 - loss: 0.1262 - val_accuracy: 0.9533 - val_loss: 0.1509 - learning_rate: 5.0000e-05\n",
      "Epoch 69/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9589 - loss: 0.1282 - val_accuracy: 0.9530 - val_loss: 0.1496 - learning_rate: 5.0000e-05\n",
      "Epoch 70/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9589 - loss: 0.1261 - val_accuracy: 0.9536 - val_loss: 0.1495 - learning_rate: 5.0000e-05\n",
      "Epoch 71/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9591 - loss: 0.1254 - val_accuracy: 0.9536 - val_loss: 0.1504 - learning_rate: 5.0000e-05\n",
      "Epoch 72/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9601 - loss: 0.1238 - val_accuracy: 0.9539 - val_loss: 0.1513 - learning_rate: 5.0000e-05\n",
      "Epoch 73/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9593 - loss: 0.1240 - val_accuracy: 0.9534 - val_loss: 0.1499 - learning_rate: 5.0000e-05\n",
      "Epoch 74/150\n",
      "610/610 - 4s - 6ms/step - accuracy: 0.9604 - loss: 0.1205 - val_accuracy: 0.9534 - val_loss: 0.1506 - learning_rate: 2.5000e-05\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9518 - loss: 0.1496\n",
      "Test Accuracy: 0.9516873359680176\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Group1 (Dint divide)       0.95      0.95      0.95      4919\n",
      "     Group2 (Divide)       0.95      0.95      0.95      4830\n",
      "\n",
      "            accuracy                           0.95      9749\n",
      "           macro avg       0.95      0.95      0.95      9749\n",
      "        weighted avg       0.95      0.95      0.95      9749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_complex_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    x = Dense(2048, activation='relu')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(512, activation='relu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_ensemble_classifier(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def model_1():\n",
    "    # Define the mappings based on sample_type\n",
    "    non_persister_samples = [0]\n",
    "    persister_samples = [\"7\",\"3\",\"14_high\",\"14_med\",\"14_low\"]\n",
    "\n",
    "    # Update labels: 0 = non-persister, 1 = persister\n",
    "    y_model1 = np.where(merged_data['sample_type'].isin(persister_samples), 1, 0)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model1:\", np.unique(y_model1, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model1)) != 2:\n",
    "        print(\"Not enough classes in y_model1 for Model 1\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model1)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model1.keras')\n",
    "    encoder.save('complex_encoder_model1.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model1.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model1.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Non-Persister', 'Persister']))\n",
    "\n",
    "def model_2():\n",
    "    # Define the mappings based on sample_type\n",
    "    group1_samples = ['14_high']  # Dint divide\n",
    "    group2_samples = ['14_med', '14_low']  # Divide\n",
    "\n",
    "    # Update labels: 0 = Group1 (Dint divide), 1 = Group2 (Divide)\n",
    "    y_model2 = np.where(merged_data['sample_type'].isin(group1_samples), 0, 1)\n",
    "\n",
    "    # Debug statements to check the unique values and their counts\n",
    "    print(\"Unique values in y_model2:\", np.unique(y_model2, return_counts=True))\n",
    "\n",
    "    # Ensure there are samples for both classes\n",
    "    if len(np.unique(y_model2)) != 2:\n",
    "        print(\"Not enough classes in y_model2 for Model 2\")\n",
    "        return\n",
    "\n",
    "    # Oversample the minority class using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_reduced, y_model2)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder, encoder = create_complex_autoencoder(X_train.shape[1], 256)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_complex_autoencoder_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=300,\n",
    "                    batch_size=128,  # reduced batch size\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_validation, X_validation),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    verbose=2)  # increased verbosity\n",
    "\n",
    "    autoencoder.save('complex_autoencoder_model2.keras')\n",
    "    encoder.save('complex_encoder_model2.keras')\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_validation_encoded = encoder.predict(X_validation)\n",
    "    X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "    # Create and train the classifier\n",
    "    classifier = create_ensemble_classifier((256,))\n",
    "\n",
    "    classifier.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_classifier_model2.keras', save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "    classifier.fit(X_train_encoded, y_train,\n",
    "                   epochs=150,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(X_validation_encoded, y_validation),\n",
    "                   class_weight=class_weight_dict,\n",
    "                   callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                   verbose=2)  # increased verbosity\n",
    "\n",
    "    classifier.save('classifier_model2.keras')\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    test_loss, test_accuracy = classifier.evaluate(X_test_encoded, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Predict the test set\n",
    "    y_test_pred = classifier.predict(X_test_encoded).argmax(axis=-1)\n",
    "\n",
    "    # Ensure there are two classes in the target names\n",
    "    if len(np.unique(y_test_pred)) == 2:\n",
    "        print(classification_report(y_test, y_test_pred, target_names=['Group1 (Dint divide)', 'Group2 (Divide)']))\n",
    "    else:\n",
    "        print(f\"Only one class {np.unique(y_test_pred)[0]} predicted, classification report is not generated.\")\n",
    "\n",
    "# Execute Model 1\n",
    "model_1()\n",
    "\n",
    "# Execute Model 2\n",
    "model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef21fd-4a3f-43e0-bdc2-586345c6c364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
