{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce070d-b4d5-4eb8-9452-aea40cd45e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset 1  : GSM4869652_xOsi.dge.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606a5e6b-0c00-4e93-8e5f-d25f06a6a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister):\n",
      "Persister        3150\n",
      "Non-Persister     521\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide):\n",
      "Group2 (Divide)         3373\n",
      "Group1 (Dint divide)     298\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved models\n",
    "autoencoder_model1 = load_model('complex_autoencoder_model1.keras')\n",
    "encoder_model1 = load_model('complex_encoder_model1.keras')\n",
    "classifier_model1 = load_model('classifier_model1.keras')\n",
    "\n",
    "autoencoder_model2 = load_model('complex_autoencoder_model2.keras')\n",
    "encoder_model2 = load_model('complex_encoder_model2.keras')\n",
    "classifier_model2 = load_model('classifier_model2.keras')\n",
    "\n",
    "# Preprocess new data function\n",
    "def preprocess_new_data(new_data_path):\n",
    "    new_data_df = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Transpose the dataframe\n",
    "    new_data_df = new_data_df.transpose()\n",
    "    \n",
    "    # Rename the first row as columns\n",
    "    new_data_df.columns = new_data_df.iloc[0]\n",
    "    new_data_df = new_data_df[1:]\n",
    "    \n",
    "    # Reset index and rename it to 'cell'\n",
    "    new_data_df.reset_index(inplace=True)\n",
    "    new_data_df.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_new = new_data_df.drop(columns=['cell'])\n",
    "    X_new = imputer.fit_transform(X_new)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X_new, new_data_df['cell']\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data_path = '/scratch/project_2010376/GSM4869652_xOsi.dge.csv'\n",
    "X_new, cell_ids = preprocess_new_data(new_data_path)\n",
    "\n",
    "# Apply PCA transformation\n",
    "pca = PCA(n_components=500)\n",
    "X_new_reduced = pca.fit_transform(X_new)\n",
    "\n",
    "# Encode the new data using the encoder\n",
    "X_new_encoded_model1 = encoder_model1.predict(X_new_reduced)\n",
    "X_new_encoded_model2 = encoder_model2.predict(X_new_reduced)\n",
    "\n",
    "# Predict the classes using the classifier\n",
    "y_new_pred_model1 = classifier_model1.predict(X_new_encoded_model1).argmax(axis=-1)\n",
    "y_new_pred_model2 = classifier_model2.predict(X_new_encoded_model2).argmax(axis=-1)\n",
    "\n",
    "# Assuming the label_encoder was fitted with the actual classes used during training\n",
    "label_encoder_model1 = LabelEncoder()\n",
    "label_encoder_model1.fit(['Non-Persister', 'Persister'])  # Example classes, fit with your actual classes\n",
    "\n",
    "label_encoder_model2 = LabelEncoder()\n",
    "label_encoder_model2.fit(['Group1 (Dint divide)', 'Group2 (Divide)'])  # Example classes, fit with your actual classes\n",
    "\n",
    "# Decode the predictions\n",
    "y_new_pred_model1_decoded = label_encoder_model1.inverse_transform(y_new_pred_model1)\n",
    "y_new_pred_model2_decoded = label_encoder_model2.inverse_transform(y_new_pred_model2)\n",
    "\n",
    "# Print the predictions and counts for Model 1\n",
    "print(\"Predictions for Model 1 (Persister vs Non-Persister):\")\n",
    "print(pd.Series(y_new_pred_model1_decoded).value_counts())\n",
    "\n",
    "# Print the predictions and counts for Model 2\n",
    "print(\"Predictions for Model 2 (Divide vs Dint divide):\")\n",
    "print(pd.Series(y_new_pred_model2_decoded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4fc180f-1df6-4cff-b932-86c93141a0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister):\n",
      "Persister        3141\n",
      "Non-Persister     530\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide):\n",
      "Group2 (Divide)         3377\n",
      "Group1 (Dint divide)     294\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved models\n",
    "autoencoder_model1 = load_model('complex_autoencoder_model1.keras')\n",
    "encoder_model1 = load_model('complex_encoder_model1.keras')\n",
    "classifier_model1 = load_model('classifier_model1.keras')\n",
    "\n",
    "autoencoder_model2 = load_model('complex_autoencoder_model2.keras')\n",
    "encoder_model2 = load_model('complex_encoder_model2.keras')\n",
    "classifier_model2 = load_model('classifier_model2.keras')\n",
    "\n",
    "# Preprocess new data function\n",
    "def preprocess_new_data(new_data_path):\n",
    "    new_data_df = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Transpose the dataframe\n",
    "    new_data_df = new_data_df.transpose()\n",
    "    \n",
    "    # Rename the first row as columns\n",
    "    new_data_df.columns = new_data_df.iloc[0]\n",
    "    new_data_df = new_data_df[1:]\n",
    "    \n",
    "    # Reset index and rename it to 'cell'\n",
    "    new_data_df.reset_index(inplace=True)\n",
    "    new_data_df.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_new = new_data_df.drop(columns=['cell'])\n",
    "    X_new = imputer.fit_transform(X_new)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X_new, new_data_df['cell']\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data_path = '/scratch/project_2010376/GSM4869652_xOsi.dge.csv'\n",
    "X_new, cell_ids = preprocess_new_data(new_data_path)\n",
    "\n",
    "# Apply PCA transformation\n",
    "pca = PCA(n_components=500)\n",
    "X_new_reduced = pca.fit_transform(X_new)\n",
    "\n",
    "# Encode the new data using the encoder\n",
    "X_new_encoded_model1 = encoder_model1.predict(X_new_reduced)\n",
    "X_new_encoded_model2 = encoder_model2.predict(X_new_reduced)\n",
    "\n",
    "# Predict the classes using the classifier\n",
    "y_new_pred_model1 = classifier_model1.predict(X_new_encoded_model1).argmax(axis=-1)\n",
    "y_new_pred_model2 = classifier_model2.predict(X_new_encoded_model2).argmax(axis=-1)\n",
    "\n",
    "# Assuming the label_encoder was fitted with the actual classes used during training\n",
    "label_encoder_model1 = LabelEncoder()\n",
    "label_encoder_model1.fit(['Non-Persister', 'Persister'])  # Example classes, fit with your actual classes\n",
    "\n",
    "label_encoder_model2 = LabelEncoder()\n",
    "label_encoder_model2.fit(['Group1 (Dint divide)', 'Group2 (Divide)'])  # Example classes, fit with your actual classes\n",
    "\n",
    "# Decode the predictions\n",
    "y_new_pred_model1_decoded = label_encoder_model1.inverse_transform(y_new_pred_model1)\n",
    "y_new_pred_model2_decoded = label_encoder_model2.inverse_transform(y_new_pred_model2)\n",
    "\n",
    "# Print the predictions and counts for Model 1\n",
    "print(\"Predictions for Model 1 (Persister vs Non-Persister):\")\n",
    "print(pd.Series(y_new_pred_model1_decoded).value_counts())\n",
    "\n",
    "# Print the predictions and counts for Model 2\n",
    "print(\"Predictions for Model 2 (Divide vs Dint divide):\")\n",
    "print(pd.Series(y_new_pred_model2_decoded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e04f30a-01c3-4490-a1e2-f3445d3ba95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the dataset:\n",
      "  Unnamed: 0  AAAAATCTACTC  AAAACACTCAAA  AAAACCCTGTGA  AAAACGTACCGT  \\\n",
      "0       A1BG     -0.016505     -0.016505     -0.016505     -0.016505   \n",
      "1        A2M     -0.045910     -0.045910     -0.045910     -0.045910   \n",
      "2    A2M-AS1     -0.016505     -0.016505     -0.016505     -0.016505   \n",
      "3      A2ML1     -0.132679     -0.132679     -0.132679     -0.132679   \n",
      "4     A4GALT     -0.192582     -0.192582     -0.192582     -0.192582   \n",
      "5       AAAS     -0.174389     -0.174389     -0.174389     -0.174389   \n",
      "6       AACS     -0.184125     -0.184125     -0.184125     -0.184125   \n",
      "7      AADAC     -0.098700     -0.098700     -0.098700     -0.098700   \n",
      "8      AADAT     -0.070681     -0.070681     -0.070681     -0.070681   \n",
      "9      AAED1     -0.116073     -0.116073     -0.116073     -0.116073   \n",
      "\n",
      "   AAAACTGGGGAT  AAAAGAAAAATT  AAAAGCTTACAA  AAAATAACCATC  AAAATCCGTCTT  ...  \\\n",
      "0     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505  ...   \n",
      "1     -0.045910     -0.045910     -0.045910     -0.045910     -0.045910  ...   \n",
      "2     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505  ...   \n",
      "3     -0.132679     -0.132679     -0.132679     -0.132679     -0.132679  ...   \n",
      "4     -0.192582     -0.192582     -0.192582     -0.192582     -0.192582  ...   \n",
      "5     -0.174389     -0.174389     -0.174389     -0.174389     -0.174389  ...   \n",
      "6     -0.184125     -0.184125     -0.184125     -0.184125     -0.184125  ...   \n",
      "7     -0.098700     -0.098700     -0.098700     -0.098700     -0.098700  ...   \n",
      "8     -0.070681     -0.070681     -0.070681     -0.070681     -0.070681  ...   \n",
      "9     -0.116073     -0.116073     -0.116073     -0.116073     -0.116073  ...   \n",
      "\n",
      "   TTTTCTTGATTC  TTTTGAGGTAAG  TTTTGATATTGC  TTTTGGACGCCG  TTTTGTCGTTCC  \\\n",
      "0     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505   \n",
      "1     -0.045910     -0.045910     -0.045910     -0.045910     -0.045910   \n",
      "2     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505   \n",
      "3     -0.132679     -0.132679     -0.132679     -0.132679     -0.132679   \n",
      "4     -0.192582     -0.192582     -0.192582     -0.192582     -0.192582   \n",
      "5     -0.174389     -0.174389     -0.174389     -0.174389     -0.174389   \n",
      "6     -0.184125     -0.184125     -0.184125      6.099012     -0.184125   \n",
      "7     -0.098700     -0.098700     -0.098700     -0.098700     -0.098700   \n",
      "8     -0.070681     -0.070681     -0.070681     -0.070681     -0.070681   \n",
      "9     -0.116073     -0.116073     -0.116073     -0.116073     -0.116073   \n",
      "\n",
      "   TTTTTAAACCGT  TTTTTACGCTCA  TTTTTATGAAAA  TTTTTGCGGAGC  TTTTTTTTTTTT  \n",
      "0     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505  \n",
      "1     -0.045910     -0.045910     -0.045910     -0.045910     -0.045910  \n",
      "2     -0.016505     -0.016505     -0.016505     -0.016505     -0.016505  \n",
      "3     -0.132679     -0.132679     -0.132679     -0.132679     -0.132679  \n",
      "4     -0.192582     -0.192582     -0.192582     -0.192582     -0.192582  \n",
      "5     -0.174389     -0.174389     -0.174389     -0.174389     -0.174389  \n",
      "6     -0.184125     -0.184125     -0.184125     -0.184125     -0.184125  \n",
      "7     -0.098700     -0.098700     -0.098700     -0.098700     -0.098700  \n",
      "8     -0.070681     -0.070681     -0.070681     -0.070681     -0.070681  \n",
      "9     -0.116073     -0.116073     -0.116073     -0.116073     -0.116073  \n",
      "\n",
      "[10 rows x 3672 columns]\n",
      "\n",
      "Columns in the dataset:\n",
      "Index(['Unnamed: 0', 'AAAAATCTACTC', 'AAAACACTCAAA', 'AAAACCCTGTGA',\n",
      "       'AAAACGTACCGT', 'AAAACTGGGGAT', 'AAAAGAAAAATT', 'AAAAGCTTACAA',\n",
      "       'AAAATAACCATC', 'AAAATCCGTCTT',\n",
      "       ...\n",
      "       'TTTTCTTGATTC', 'TTTTGAGGTAAG', 'TTTTGATATTGC', 'TTTTGGACGCCG',\n",
      "       'TTTTGTCGTTCC', 'TTTTTAAACCGT', 'TTTTTACGCTCA', 'TTTTTATGAAAA',\n",
      "       'TTTTTGCGGAGC', 'TTTTTTTTTTTT'],\n",
      "      dtype='object', length=3672)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/scratch/project_2010376/GSM4869652_xOsi.dge.csv'\n",
    "\n",
    "# Load the first 10 rows of the dataset\n",
    "first_10_rows = pd.read_csv(file_path, nrows=10)\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "print(first_10_rows)\n",
    "\n",
    "# Display the columns\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(first_10_rows.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e58df3-f03b-4fb8-9bd0-f1f399947f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset 2 : GSM4869650_xCtrl.dge.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77cc0a2f-94a0-4138-b05e-62e52fa332ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the dataset:\n",
      "  Unnamed: 0  AAAAAGCCCCTG  AAAAAGGGCTAC  AAAAAGTGGACA  AAAAAGTTAGTA  \\\n",
      "0       A1BG     -0.014638     -0.014638     -0.014638     -0.014638   \n",
      "1        A2M     -0.020678     -0.020678     -0.020678     -0.020678   \n",
      "2    A2M-AS1     -0.020136     -0.020136     -0.020136     -0.020136   \n",
      "3      A2ML1     -0.107807     -0.107807     -0.107807     -0.107807   \n",
      "4     A4GALT     -0.062042     -0.062042     -0.062042     -0.062042   \n",
      "5       AAAS     -0.116893     -0.116893     -0.116893     -0.116893   \n",
      "6       AACS     -0.185283     -0.185283     -0.185283     -0.185283   \n",
      "7      AADAC     -0.045646     -0.045646     -0.045646     -0.045646   \n",
      "8    AADACL4     -0.014638     -0.014638     -0.014638     -0.014638   \n",
      "9      AADAT     -0.061207     -0.061207     -0.061207     -0.061207   \n",
      "\n",
      "   AAAAATCTCACG  AAAACCATTGAG  AAAACGATTTAT  AAAACGTGTATG  AAAACTACGATT  ...  \\\n",
      "0     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638  ...   \n",
      "1     -0.020678     -0.020678     -0.020678     -0.020678     -0.020678  ...   \n",
      "2     -0.020136     -0.020136     -0.020136     -0.020136     -0.020136  ...   \n",
      "3     -0.107807     -0.107807     -0.107807     -0.107807     -0.107807  ...   \n",
      "4     -0.062042     -0.062042     -0.062042     -0.062042     -0.062042  ...   \n",
      "5     -0.116893     -0.116893     -0.116893     -0.116893     -0.116893  ...   \n",
      "6     -0.185283     -0.185283      5.506888     -0.185283     -0.185283  ...   \n",
      "7     -0.045646     -0.045646     -0.045646     -0.045646     -0.045646  ...   \n",
      "8     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638  ...   \n",
      "9     -0.061207     -0.061207     -0.061207     -0.061207     -0.061207  ...   \n",
      "\n",
      "   TTTTGCGGTGGG  TTTTGCGTGGAT  TTTTGCTGACGT  TTTTGCTTTGTA  TTTTGGTCGTCT  \\\n",
      "0     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638   \n",
      "1     -0.020678     -0.020678     -0.020678     -0.020678     -0.020678   \n",
      "2     -0.020136     -0.020136     -0.020136     -0.020136     -0.020136   \n",
      "3     -0.107807     -0.107807     -0.107807     -0.107807     -0.107807   \n",
      "4     -0.062042     -0.062042     -0.062042     -0.062042     -0.062042   \n",
      "5     -0.116893     -0.116893     -0.116893     -0.116893     -0.116893   \n",
      "6      7.015079     -0.185283     -0.185283     -0.185283     -0.185283   \n",
      "7     -0.045646     -0.045646     -0.045646     -0.045646     -0.045646   \n",
      "8     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638   \n",
      "9     -0.061207     -0.061207     -0.061207     -0.061207     -0.061207   \n",
      "\n",
      "   TTTTGGTTATGG  TTTTGTAGACCG  TTTTTAGTACGA  TTTTTTCCATCA  TTTTTTTTTTTT  \n",
      "0     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638  \n",
      "1     -0.020678     -0.020678     -0.020678     -0.020678     -0.020678  \n",
      "2     -0.020136     -0.020136     -0.020136     -0.020136     -0.020136  \n",
      "3     -0.107807     -0.107807     -0.107807     -0.107807     -0.107807  \n",
      "4     -0.062042     -0.062042     -0.062042     -0.062042     -0.062042  \n",
      "5     -0.116893     -0.116893     -0.116893     -0.116893     -0.116893  \n",
      "6     -0.185283     -0.185283     -0.185283     -0.185283     -0.185283  \n",
      "7     -0.045646     -0.045646     -0.045646     -0.045646     -0.045646  \n",
      "8     -0.014638     -0.014638     -0.014638     -0.014638     -0.014638  \n",
      "9     -0.061207     -0.061207     -0.061207     -0.061207     -0.061207  \n",
      "\n",
      "[10 rows x 4668 columns]\n",
      "\n",
      "Columns in the dataset:\n",
      "Index(['Unnamed: 0', 'AAAAAGCCCCTG', 'AAAAAGGGCTAC', 'AAAAAGTGGACA',\n",
      "       'AAAAAGTTAGTA', 'AAAAATCTCACG', 'AAAACCATTGAG', 'AAAACGATTTAT',\n",
      "       'AAAACGTGTATG', 'AAAACTACGATT',\n",
      "       ...\n",
      "       'TTTTGCGGTGGG', 'TTTTGCGTGGAT', 'TTTTGCTGACGT', 'TTTTGCTTTGTA',\n",
      "       'TTTTGGTCGTCT', 'TTTTGGTTATGG', 'TTTTGTAGACCG', 'TTTTTAGTACGA',\n",
      "       'TTTTTTCCATCA', 'TTTTTTTTTTTT'],\n",
      "      dtype='object', length=4668)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/scratch/project_2010376/GSM4869650_xCtrl.dge.csv'\n",
    "\n",
    "# Load the first 10 rows of the dataset\n",
    "first_10_rows = pd.read_csv(file_path, nrows=10)\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "print(first_10_rows)\n",
    "\n",
    "# Display the columns\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(first_10_rows.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753a1099-65bb-4b84-9af5-fad72b15118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister):\n",
      "Persister        4060\n",
      "Non-Persister     607\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide):\n",
      "Group2 (Divide)         4065\n",
      "Group1 (Dint divide)     602\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved models\n",
    "autoencoder_model1 = load_model('complex_autoencoder_model1.keras')\n",
    "encoder_model1 = load_model('complex_encoder_model1.keras')\n",
    "classifier_model1 = load_model('classifier_model1.keras')\n",
    "\n",
    "autoencoder_model2 = load_model('complex_autoencoder_model2.keras')\n",
    "encoder_model2 = load_model('complex_encoder_model2.keras')\n",
    "classifier_model2 = load_model('classifier_model2.keras')\n",
    "\n",
    "# Preprocess new data function\n",
    "def preprocess_new_data(new_data_path):\n",
    "    new_data_df = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Transpose the dataframe\n",
    "    new_data_df = new_data_df.transpose()\n",
    "    \n",
    "    # Rename the first row as columns\n",
    "    new_data_df.columns = new_data_df.iloc[0]\n",
    "    new_data_df = new_data_df[1:]\n",
    "    \n",
    "    # Reset index and rename it to 'cell'\n",
    "    new_data_df.reset_index(inplace=True)\n",
    "    new_data_df.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_new = new_data_df.drop(columns=['cell'])\n",
    "    X_new = imputer.fit_transform(X_new)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X_new, new_data_df['cell']\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data_path = '/scratch/project_2010376/GSM4869650_xCtrl.dge.csv'\n",
    "X_new, cell_ids = preprocess_new_data(new_data_path)\n",
    "\n",
    "# Apply PCA transformation\n",
    "pca = PCA(n_components=500)\n",
    "X_new_reduced = pca.fit_transform(X_new)\n",
    "\n",
    "# Encode the new data using the encoder\n",
    "X_new_encoded_model1 = encoder_model1.predict(X_new_reduced)\n",
    "X_new_encoded_model2 = encoder_model2.predict(X_new_reduced)\n",
    "\n",
    "# Predict the classes using the classifier\n",
    "y_new_pred_model1 = classifier_model1.predict(X_new_encoded_model1).argmax(axis=-1)\n",
    "y_new_pred_model2 = classifier_model2.predict(X_new_encoded_model2).argmax(axis=-1)\n",
    "\n",
    "# Assuming the label_encoder was fitted with the actual classes used during training\n",
    "label_encoder_model1 = LabelEncoder()\n",
    "label_encoder_model1.fit(['Non-Persister', 'Persister'])  # Example classes, fit with your actual classes\n",
    "\n",
    "label_encoder_model2 = LabelEncoder()\n",
    "label_encoder_model2.fit(['Group1 (Dint divide)', 'Group2 (Divide)'])  # Example classes, fit with your actual classes\n",
    "\n",
    "# Decode the predictions\n",
    "y_new_pred_model1_decoded = label_encoder_model1.inverse_transform(y_new_pred_model1)\n",
    "y_new_pred_model2_decoded = label_encoder_model2.inverse_transform(y_new_pred_model2)\n",
    "\n",
    "# Print the predictions and counts for Model 1\n",
    "print(\"Predictions for Model 1 (Persister vs Non-Persister):\")\n",
    "print(pd.Series(y_new_pred_model1_decoded).value_counts())\n",
    "\n",
    "# Print the predictions and counts for Model 2\n",
    "print(\"Predictions for Model 2 (Divide vs Dint divide):\")\n",
    "print(pd.Series(y_new_pred_model2_decoded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c02971-e2e5-4605-b3ec-7d341bd31515",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset 3: GSM4869653_xOsiCriz.dge.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bba15634-2500-493e-9fad-a2673321b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister):\n",
      "Persister        4003\n",
      "Non-Persister     664\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide):\n",
      "Group2 (Divide)         4008\n",
      "Group1 (Dint divide)     659\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved models\n",
    "autoencoder_model1 = load_model('complex_autoencoder_model1.keras')\n",
    "encoder_model1 = load_model('complex_encoder_model1.keras')\n",
    "classifier_model1 = load_model('classifier_model1.keras')\n",
    "\n",
    "autoencoder_model2 = load_model('complex_autoencoder_model2.keras')\n",
    "encoder_model2 = load_model('complex_encoder_model2.keras')\n",
    "classifier_model2 = load_model('classifier_model2.keras')\n",
    "\n",
    "# Preprocess new data function\n",
    "def preprocess_new_data(new_data_path):\n",
    "    new_data_df = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Transpose the dataframe\n",
    "    new_data_df = new_data_df.transpose()\n",
    "    \n",
    "    # Rename the first row as columns\n",
    "    new_data_df.columns = new_data_df.iloc[0]\n",
    "    new_data_df = new_data_df[1:]\n",
    "    \n",
    "    # Reset index and rename it to 'cell'\n",
    "    new_data_df.reset_index(inplace=True)\n",
    "    new_data_df.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_new = new_data_df.drop(columns=['cell'])\n",
    "    X_new = imputer.fit_transform(X_new)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X_new, new_data_df['cell']\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data_path = '/scratch/project_2010376/GSM4869653_xOsiCriz.dge.csv'\n",
    "X_new, cell_ids = preprocess_new_data(new_data_path)\n",
    "\n",
    "# Apply PCA transformation\n",
    "pca = PCA(n_components=500)\n",
    "X_new_reduced = pca.fit_transform(X_new)\n",
    "\n",
    "# Encode the new data using the encoder\n",
    "X_new_encoded_model1 = encoder_model1.predict(X_new_reduced)\n",
    "X_new_encoded_model2 = encoder_model2.predict(X_new_reduced)\n",
    "\n",
    "# Predict the classes using the classifier\n",
    "y_new_pred_model1 = classifier_model1.predict(X_new_encoded_model1).argmax(axis=-1)\n",
    "y_new_pred_model2 = classifier_model2.predict(X_new_encoded_model2).argmax(axis=-1)\n",
    "\n",
    "# Assuming the label_encoder was fitted with the actual classes used during training\n",
    "label_encoder_model1 = LabelEncoder()\n",
    "label_encoder_model1.fit(['Non-Persister', 'Persister'])  # Example classes, fit with your actual classes\n",
    "\n",
    "label_encoder_model2 = LabelEncoder()\n",
    "label_encoder_model2.fit(['Group1 (Dint divide)', 'Group2 (Divide)'])  # Example classes, fit with your actual classes\n",
    "\n",
    "# Decode the predictions\n",
    "y_new_pred_model1_decoded = label_encoder_model1.inverse_transform(y_new_pred_model1)\n",
    "y_new_pred_model2_decoded = label_encoder_model2.inverse_transform(y_new_pred_model2)\n",
    "\n",
    "# Print the predictions and counts for Model 1\n",
    "print(\"Predictions for Model 1 (Persister vs Non-Persister):\")\n",
    "print(pd.Series(y_new_pred_model1_decoded).value_counts())\n",
    "\n",
    "# Print the predictions and counts for Model 2\n",
    "print(\"Predictions for Model 2 (Divide vs Dint divide):\")\n",
    "print(pd.Series(y_new_pred_model2_decoded).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182981d4-472a-491b-b26b-5428bedc53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab17b362-feda-4b08-b481-309aebaf42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: GSM4869653_xOsiCriz\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister) for GSM4869653_xOsiCriz:\n",
      "Persister        4040\n",
      "Non-Persister     627\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide) for GSM4869653_xOsiCriz:\n",
      "Group2 (Divide)         3449\n",
      "Group1 (Dint divide)    1218\n",
      "Name: count, dtype: int64\n",
      "Processing dataset: GSM4869650_xCtrl\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister) for GSM4869650_xCtrl:\n",
      "Persister        4049\n",
      "Non-Persister     618\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide) for GSM4869650_xCtrl:\n",
      "Group2 (Divide)         3437\n",
      "Group1 (Dint divide)    1230\n",
      "Name: count, dtype: int64\n",
      "Processing dataset: GSM4869652_xOsi\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Predictions for Model 1 (Persister vs Non-Persister) for GSM4869652_xOsi:\n",
      "Persister        3164\n",
      "Non-Persister     507\n",
      "Name: count, dtype: int64\n",
      "Predictions for Model 2 (Divide vs Dint divide) for GSM4869652_xOsi:\n",
      "Group2 (Divide)         2903\n",
      "Group1 (Dint divide)     768\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tabular view of all performances:\n",
      "               Dataset  Model 1 - Non-Persister  Model 1 - Persister  \\\n",
      "0  GSM4869653_xOsiCriz                      627                 4040   \n",
      "1     GSM4869650_xCtrl                      618                 4049   \n",
      "2      GSM4869652_xOsi                      507                 3164   \n",
      "\n",
      "   Model 1 - Non-Persister (%)  Model 1 - Persister (%)  \\\n",
      "0                    13.434755                86.565245   \n",
      "1                    13.241911                86.758089   \n",
      "2                    13.810951                86.189049   \n",
      "\n",
      "   Model 2 - Group1 (Dint divide)  Model 2 - Group2 (Divide)  \\\n",
      "0                            1218                       3449   \n",
      "1                            1230                       3437   \n",
      "2                             768                       2903   \n",
      "\n",
      "   Model 2 - Group1 (Dint divide) (%)  Model 2 - Group2 (Divide) (%)  \n",
      "0                           26.098136                      73.901864  \n",
      "1                           26.355260                      73.644740  \n",
      "2                           20.920730                      79.079270  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved models\n",
    "autoencoder_model1 = load_model('complex_autoencoder_model1.keras')\n",
    "encoder_model1 = load_model('complex_encoder_model1.keras')\n",
    "classifier_model1 = load_model('classifier_model1.keras')\n",
    "\n",
    "autoencoder_model2 = load_model('complex_autoencoder_model2.keras')\n",
    "encoder_model2 = load_model('complex_encoder_model2.keras')\n",
    "classifier_model2 = load_model('classifier_model2.keras')\n",
    "\n",
    "# Preprocess new data function\n",
    "def preprocess_new_data(new_data_path):\n",
    "    new_data_df = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Transpose the dataframe\n",
    "    new_data_df = new_data_df.transpose()\n",
    "    \n",
    "    # Rename the first row as columns\n",
    "    new_data_df.columns = new_data_df.iloc[0]\n",
    "    new_data_df = new_data_df[1:]\n",
    "    \n",
    "    # Reset index and rename it to 'cell'\n",
    "    new_data_df.reset_index(inplace=True)\n",
    "    new_data_df.rename(columns={'index': 'cell'}, inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_new = new_data_df.drop(columns=['cell'])\n",
    "    X_new = imputer.fit_transform(X_new)  # Apply imputation to handle NaNs\n",
    "\n",
    "    return X_new, new_data_df['cell']\n",
    "\n",
    "# Define the dataset paths\n",
    "datasets = {\n",
    "    \"GSM4869653_xOsiCriz\": \"/scratch/project_2010376/GSM4869653_xOsiCriz.dge.csv\",\n",
    "    \"GSM4869650_xCtrl\": \"/scratch/project_2010376/GSM4869650_xCtrl.dge.csv\",\n",
    "    \"GSM4869652_xOsi\": \"/scratch/project_2010376/GSM4869652_xOsi.dge.csv\"\n",
    "}\n",
    "\n",
    "# Initialize the results dictionary\n",
    "results = []\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "    # Load and preprocess the new data\n",
    "    X_new, cell_ids = preprocess_new_data(dataset_path)\n",
    "\n",
    "    # Apply PCA transformation\n",
    "    pca = PCA(n_components=500)\n",
    "    X_new_reduced = pca.fit_transform(X_new)\n",
    "\n",
    "    # Encode the new data using the encoder\n",
    "    X_new_encoded_model1 = encoder_model1.predict(X_new_reduced)\n",
    "    X_new_encoded_model2 = encoder_model2.predict(X_new_reduced)\n",
    "\n",
    "    # Predict the classes using the classifier\n",
    "    y_new_pred_model1 = classifier_model1.predict(X_new_encoded_model1).argmax(axis=-1)\n",
    "    y_new_pred_model2 = classifier_model2.predict(X_new_encoded_model2).argmax(axis=-1)\n",
    "\n",
    "    # Assuming the label_encoder was fitted with the actual classes used during training\n",
    "    label_encoder_model1 = LabelEncoder()\n",
    "    label_encoder_model1.fit(['Non-Persister', 'Persister'])  # Example classes, fit with your actual classes\n",
    "\n",
    "    label_encoder_model2 = LabelEncoder()\n",
    "    label_encoder_model2.fit(['Group1 (Dint divide)', 'Group2 (Divide)'])  # Example classes, fit with your actual classes\n",
    "\n",
    "    # Decode the predictions\n",
    "    y_new_pred_model1_decoded = label_encoder_model1.inverse_transform(y_new_pred_model1)\n",
    "    y_new_pred_model2_decoded = label_encoder_model2.inverse_transform(y_new_pred_model2)\n",
    "\n",
    "    # Print the predictions and counts for each model\n",
    "    print(f\"Predictions for Model 1 (Persister vs Non-Persister) for {dataset_name}:\")\n",
    "    count_model1 = pd.Series(y_new_pred_model1_decoded).value_counts()\n",
    "    print(count_model1)\n",
    "    \n",
    "    print(f\"Predictions for Model 2 (Divide vs Dint divide) for {dataset_name}:\")\n",
    "    count_model2 = pd.Series(y_new_pred_model2_decoded).value_counts()\n",
    "    print(count_model2)\n",
    "\n",
    "    # Calculate the percentage of each class\n",
    "    percentage_model1 = (count_model1 / count_model1.sum()) * 100\n",
    "    percentage_model2 = (count_model2 / count_model2.sum()) * 100\n",
    "\n",
    "    # Append results to the dictionary\n",
    "    results.append({\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Model 1 - Non-Persister\": count_model1.get(\"Non-Persister\", 0),\n",
    "        \"Model 1 - Persister\": count_model1.get(\"Persister\", 0),\n",
    "        \"Model 1 - Non-Persister (%)\": percentage_model1.get(\"Non-Persister\", 0),\n",
    "        \"Model 1 - Persister (%)\": percentage_model1.get(\"Persister\", 0),\n",
    "        \"Model 2 - Group1 (Dint divide)\": count_model2.get(\"Group1 (Dint divide)\", 0),\n",
    "        \"Model 2 - Group2 (Divide)\": count_model2.get(\"Group2 (Divide)\", 0),\n",
    "        \"Model 2 - Group1 (Dint divide) (%)\": percentage_model2.get(\"Group1 (Dint divide)\", 0),\n",
    "        \"Model 2 - Group2 (Divide) (%)\": percentage_model2.get(\"Group2 (Divide)\", 0)\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for tabular view\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nTabular view of all performances:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e9062-c4fd-49c1-bffd-ee3e9fd98f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
